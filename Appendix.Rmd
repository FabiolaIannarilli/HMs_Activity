# Appendix {#app}

## The *sim_activity* function {#simfunction}

Throughout the tutorial, we use the `sim_activity` function for simulating activity patterns. This function simulates realized activity patterns based on a 'true' activity pattern distribution density build as specified in equation 1 reported in Tutorial \@ref(var) and introduced in @HMsActivity2024. The arguments of the function specify the following:

*  `M`: Numeric. Number of sampling units (e.g. sites).
*  `J`: Numeric. Number of cycles (e.g. days).
*  `wavelength`: Numeric. Length of a period (i.e. distance between identical points in a trigonometric function)
*  `b0`: Numeric. Intercept.
*  `b0_constant`: Logic. If TRUE (default), b0 is held constant; otherwise time-varying.
*  `tau_constant`: Logic. If TRUE (default), random intercept is equal to 0 and there is no variability in use among sampling units
*  `sdtau`: Numeric. Only if `tau_constant` = FALSE. Random intercept describing variability in use among sampling units coded as a Normal distribution N(0, `sdtau`).
*  `b1`: Numeric. Wave amplitude of first cosinusoidal curve.
*  `b2`: Numeric. Wave amplitude of second cosinusoidal curve.
*  `theta0`: Numeric. Curve-specific phaseshift for the first cosinusoidal curve.
*  `theta1`: Numeric. Curve-specific phaseshift for the second cosinusoidal curve.
*  `n_peak`: Numeric. Number of peak in activity during a period - a day - (e.g. unimodal = 1, bimodal = 2). Must be a positive integer.
*  `phaseshift_constant`: Logic. Should all the sites have a peak at the same time? (Default = TRUE).
*  `sd_phaseshift`: Numeric. Variability in timing of peak activity among sites  (i.e. standard deviation of the phaseshift in the sinusoidal curve that determines an horizontal shift).
*  `n_sim`: Numeric. Number of datasets of realized detections to create.
*  `plot_true_act`: Logic. If TRUE (default), a plot of the conditional and marginal means probability of activity by occasion is returned.

When the goal is simulating diel activity patterns, `wavelength` can be specified either as the number of hours (i.e. `wavelength` = 24) or number of minutes (i.e. `wavelength` = 1440) in the diel cycle. When `b2` = 0, the function returns a curve governed only by the first cosine term, and thus a curve with a unimodal pattern (the frequency of the first cosine term is set to 24).

```{r app1}
# Load function
source("source_functions/sim_activity.R")

# Set equations' parameters
M = 100
J = 30
wavelength = 24
n_peak = 2
b0 = -3
b1 = 1 
b2 = 0
theta0 = 3
theta1 = 2 
sd_tau = 1
sd_gamma = 0.3
time <- seq(0, 23, length.out = 100)

# simulate data
dat <- sim_activity(M = M, 
                     J = J, 
                     wavelength = wavelength, 
                     n_peak = n_peak, 
                     n_sim = 1, 
                     b0 = b0, 
                     b0_constant = TRUE, # common intercept
                     tau_constant = FALSE, 
                     sdtau = sd_tau, # ~site-specific intercept
                     b1 = b1, 
                     b2 = b2, # amplitude of the cosine terms 
                     theta0 = theta0, 
                     theta1 = theta1, # common phaseshifts for the cosine terms
                     phaseshift_constant = FALSE, 
                     sd_phaseshift = sd_gamma, # site-specific phaseshift (equal for both cosine terms)
                     plot_true_act = TRUE)

```


When `b2` differs from zero, the function returns a bimodal pattern. The presence of two cosine terms allows us to simulate curves with the two peaks having different heights and, thus, resembling real activity patterns.

```{r app2}
# Set equations' parameters (all others as used before)
n_peak = 2
b1 = 0.2
b2 = 0.6
theta0 = 1
theta1 = 2 

# simulate data
dat <- sim_activity(M = M, 
                     J = J, 
                     wavelength = wavelength, 
                     n_peak = n_peak, 
                     n_sim = 1, 
                     b0 = b0, 
                     b0_constant = TRUE, # common intercept
                     tau_constant = FALSE, 
                     sdtau = sd_tau, # ~site-specific intercept
                     b1 = b1, 
                     b2 = b2, # amplitude of the cosine terms 
                     theta0 = theta0, 
                     theta1 = theta1, # common phaseshifts for the cosine terms
                     phaseshift_constant = FALSE, 
                     sd_phaseshift = sd_gamma, # site-specific phaseshift (equal for both cosine terms)
                     plot_true_act = TRUE)

```

Activity patterns having  two peaks that are the same height can be simulated setting `b1` = 0, `n_peak` = 2, and `b2` to a value other than zero. These choices silence the first cosine term (with frequency of 24) and impose single cosine with a frequency of 12. 

```{r app3}
# Set equations' parameters (all others as used before)
n_peak = 2
b1 = 0 
b2 = 3
theta0 = 2
theta1 = 2 

# simulate data
dat <- sim_activity(M = M, 
                     J = J, 
                     wavelength = wavelength, 
                     n_peak = n_peak, 
                     n_sim = 1, 
                     b0 = b0, 
                     b0_constant = TRUE, # common intercept
                     tau_constant = FALSE, 
                     sdtau = sd_tau, # ~site-specific intercept
                     b1 = b1, 
                     b2 = b2, # amplitude of the cosine terms 
                     theta0 = theta0, 
                     theta1 = theta1, # common phaseshifts for the cosine terms
                     phaseshift_constant = FALSE, 
                     sd_phaseshift = sd_gamma, # site-specific phaseshift (equal for both cosine terms)
                     plot_true_act = TRUE)

```

## Binomial versus other distributions {#poisson}

How we aggregate observations of activity determines how we model our data. @Rees2024 modeled animal diel activity using a generalized additive modeling approach with a negative binomial distribution after determining there was overdispersion using a Poisson distribution. The Poisson probability density function (PDF) is a special case of the Negative Binomial. Throughout, we have used the binomial probability mass function to model diel activity in both trigonometric GLMMs and cyclic cubic splines HGAMs. The Poisson distribution is in fact a special case of the binomial when the number of 'trials' is very large. As such, we can gain similar inference using these different distributions, but we need to think about how sampling effort is accounted for. 

In the binomial application, the zeros indicate that a camera was active on a given day, but there was no detection of the species. When using the Poisson or negative binomial distribution to model the counts of detections, we need to include sampling effort (e.g., @Rees2024). We can do this as an offset within the model, which changes our inference from the expected number of counts to the expected number of counts per unit of effort (i.e. thus modeling a rate). If setup this way, results using these different probability distributions should be quite similar (depending on sample size). There however may be computational benefits modeling counts at very large sample sizes. 

Below, we demonstrate the equivalence between using the binomial and Poisson probability distributions when fitting animal activity data using trigonometric and cyclic spline hierarchical models.


### Data preparation

We use camera-trap records collected between 2016 and 2018 at 100 locations in Northern Minnesota, USA [@Iannarilliea2021]. We use the same data records of American black bear (*Ursus americanus*) as done in the Tutorial \@ref(cat). However, we ignore the season covariate for simplicity and just focus on modeling the Fall data. We first load the `occasions_cbind` object that we saved in the Tutorial \@ref(cat), then subset the data to only include records related to the Fall season.


```{r pdf1, message = TRUE, warnings = TRUE}
# Load Libraries
rm(list = ls())
set.seed(129)
library(dplyr)
library(lubridate)
library(GLMMadaptive)
library(mgcv)
library(ggpubr)

# Load data
occasions_cbind <- read.csv("data_output/occasions_cbind_Ursus_americanus_seasons.csv") %>% 
  mutate(Site = as.factor(Site))

# Subset the data to the Fall data only, thus removing Spring data.
y_fall <-  occasions_cbind[which(occasions_cbind$Season == "F"),]
head(y_fall)

# Create a column of the total number of possible detections (i.e. trials) to be used
# as the offset when controlling for sampling effort when using the Poisson PDF.
y_fall$total <-  y_fall$success + y_fall$failure

```

### Trigonometric PDF comparison

Fit the model using  a binomial distribution and then fit the equivalent count-based model with an offset to control for sampling effort.

```{r pdf2}
# Fit the Binomial model
trig_rand_int <- mixed_model(fixed = cbind(success, failure) ~ 
                                              cos(2 * pi * Time/24) +
                                              sin(2 * pi * Time/24) +
                                              sin(2 * pi * Time/12) +
                                              cos(2 * pi * Time/12) ,
            random = ~  1  |   Site,
            data = y_fall, family = binomial(), iter_EM = 0)
summary(trig_rand_int)


# Fit Poisson Model with log offset using the variable 'total'
trig_rand_int_P <- mixed_model(fixed = success ~ 
                                              cos(2 * pi * Time/24) +
                                              sin(2 * pi * Time/24) +
                                              sin(2 * pi * Time/12) +
                                              cos(2 * pi * Time/12) + offset(log(total)),
            random = ~  1  |   Site,
            data = y_fall, family = poisson(), iter_EM = 0)
summary(trig_rand_int_P)

```

First, note the similarity in the coefficient estimates. Next, we can predict the diel activity for each model. Note that to be equivalent, we need to consider the same predictions for 1 unit of sampling effort, thus we use 'total = 1'. 

```{r pdf3}
newdat <- with(occasions_cbind, expand.grid(Time = seq(0, 24, length.out = 24), total=1))
pred_rand_int <- effectPlotData(trig_rand_int, newdat, marginal = FALSE) 
pred_rand_int_P <- effectPlotData(trig_rand_int_P, newdat, marginal = FALSE) 
```

Now, we can plot the activity predictions from both models. 

```{r pdf4, class.source = 'fold-hide'}
# plot 1
pl_trig1 <- ggplot(pred_rand_int, aes(Time, plogis(pred))) +
  geom_ribbon(aes(ymin = plogis(low), ymax = plogis(upp)), alpha = 0.1, linewidth = 0.25) +
  geom_line(aes(), linewidth = 1) +
  labs(x = "Time of Day (Hour)", y = "Predicted Activity Pattern \n (probability)", title = "B: Trigonometric Binomial Model")+
  theme_minimal()+
  theme(legend.position = "bottom",
        legend.title = element_blank(),
        legend.text = element_text(size=10,face="bold"),
        legend.margin=margin(0,0,0,0),
        legend.box.margin=margin(-5,-10,-10,-10),
        plot.title = element_text(size=10,face="bold"),
        axis.line = element_line(colour = 'black', linetype = 'solid'),
        axis.ticks = element_line(colour = 'black', linetype = 'solid'),
        axis.title = element_text(size=9,face="bold"),
        panel.grid.minor.y = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.grid.major.x = element_line(colour = 'lightgrey', linetype = 'dashed', linewidth=0.5),
        panel.grid.minor.x = element_blank(),
        strip.text = element_text(size = 9, colour = "black", face = "bold", hjust = 0))+
  scale_x_continuous(breaks=seq(0,23,length.out=7), labels=seq(0,24,4))

# plot 2
pl_trig1_P <- ggplot(pred_rand_int_P, aes(Time, plogis(pred))) +
  geom_ribbon(aes(ymin = plogis(low), ymax = plogis(upp)), alpha = 0.1, linewidth = 0.25) +
  geom_line(aes(), linewidth = 1) +
  labs(x = "Time of Day (Hour)", y = "Predicted Activity Pattern \n (probability)", title = "A: Trigonometric Poisson Model ")+
  theme_minimal()+
  theme(legend.position = "bottom",
        legend.title = element_blank(),
        legend.text = element_text(size=10,face="bold"),
        legend.margin=margin(0,0,0,0),
        legend.box.margin=margin(-5,-10,-10,-10),
        plot.title = element_text(size=10,face="bold"),
        axis.line = element_line(colour = 'black', linetype = 'solid'),
        axis.ticks = element_line(colour = 'black', linetype = 'solid'),
        axis.title = element_text(size=9,face="bold"),
        panel.grid.minor.y = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.grid.major.x = element_line(colour = 'lightgrey', linetype = 'dashed', linewidth=0.5),
        panel.grid.minor.x = element_blank(),
        strip.text = element_text(size = 9, colour = "black", face = "bold", hjust = 0))+
  scale_x_continuous(breaks=seq(0,23,length.out=7), labels=seq(0,24,4)) 
ggarrange(pl_trig1_P, pl_trig1)
  
```

We can see that the predictions are equivalent. 

### Cyclic cubic PDF comparison

Now, let's do the same comparison when fitting models in a generalized additive model framework.

```{r pdf5}
# Fit binomial model
mod_cycl <- bam(cbind(success, failure) ~ 
                   s(Time, bs = "cc", k = 12) + 
                   s(Site, bs = "re"), 
                 knots = list(Time = c(0,23)),
                 family = "binomial", 
                 data = y_fall)

# Predict activity patterns
newdat <- with(occasions_cbind, expand.grid(Time = seq(0, 24, length.out=24),total=1, 
                                          Site = "7B")) # Station doesn't matter

# Create predictions and confidence intervals
cycl_pred <- predict.bam(mod_cycl, newdata = newdat,  exclude = "s(Site)", se.fit = TRUE, type = "link") 
cycl_pred$low <- cycl_pred$fit - (1.96 * cycl_pred$se.fit)
cycl_pred$upp <- cycl_pred$fit + (1.96 * cycl_pred$se.fit)
cycl_pred$Time <- newdat$Time
cycl_pred <- data.frame(cycl_pred)

# Fit Poisson model
mod_cycl_P <- bam(success ~ 
                   s(Time, bs = "cc", k = 12) + 
                   s(Site, bs = "re") + offset(log(total)), 
                 knots = list(Time = c(0,23)),
                 family = "poisson", 
                 data = y_fall)

# Create predictions and confidence intervals
cycl_pred_P <- predict.bam(mod_cycl_P, newdata = newdat,  exclude = "s(Site)", se.fit = TRUE, type = "link") 
cycl_pred_P$low <- cycl_pred_P$fit - (1.96 * cycl_pred_P$se.fit)
cycl_pred_P$upp <- cycl_pred_P$fit + (1.96 * cycl_pred_P$se.fit)
cycl_pred_P$Time <- newdat$Time
cycl_pred_P <- data.frame(cycl_pred_P)
```

Now, we can plot the activity predictions from both models. 

```{r pdf6, class.source = 'fold-hide'}
# plot 1
pl_cycle1 <- ggplot(cycl_pred, aes(Time, plogis(fit))) +
  geom_ribbon(aes(ymin = plogis(low), ymax = plogis(upp)), alpha = 0.1, linewidth = 0.25) +
  geom_line(aes(), linewidth = 1) +
  labs(x = "Time of Day (Hour)", y = "Predicted Activity Pattern \n (probability)", title = "A: Hierarhical GAM - Binomial")+
  theme_minimal()+
  theme(legend.position = "bottom",
        legend.title = element_blank(),
        legend.text = element_text(size=10,face="bold"),
        legend.margin=margin(0,0,0,0),
        legend.box.margin=margin(-5,-10,-10,-10),
        plot.title = element_text(size=10,face="bold"),
        axis.line = element_line(colour = 'black', linetype = 'solid'),
        axis.ticks = element_line(colour = 'black', linetype = 'solid'),
        axis.title = element_text(size=9,face="bold"),
        panel.grid.minor.y = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.grid.major.x = element_line(colour = 'lightgrey', linetype = 'dashed', linewidth=0.5),
        panel.grid.minor.x = element_blank(),
        strip.text = element_text(size = 9, colour = "black", face = "bold", hjust = 0))+
  scale_x_continuous(breaks=seq(0,23,length.out=7), labels=seq(0,24,4))

#plot 2
pl_cycle1_P <- ggplot(cycl_pred, aes(Time, plogis(fit))) +
  geom_ribbon(aes(ymin = plogis(low), ymax = plogis(upp)), alpha = 0.1, linewidth = 0.25) +
  geom_line(aes(), linewidth = 1) +
  labs(x = "Time of Day (Hour)", y = "Predicted Activity Pattern \n (probability)", title = "B: Hierarhical GAM - Poisson")+
  theme_minimal()+
  theme(legend.position = "bottom",
        legend.title = element_blank(),
        legend.text = element_text(size=10,face="bold"),
        legend.margin=margin(0,0,0,0),
        legend.box.margin=margin(-5,-10,-10,-10),
        plot.title = element_text(size=10,face="bold"),
        axis.line = element_line(colour = 'black', linetype = 'solid'),
        axis.ticks = element_line(colour = 'black', linetype = 'solid'),
        axis.title = element_text(size=9,face="bold"),
        panel.grid.minor.y = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.grid.major.x = element_line(colour = 'lightgrey', linetype = 'dashed', linewidth=0.5),
        panel.grid.minor.x = element_blank(),
        strip.text = element_text(size = 9, colour = "black", face = "bold", hjust = 0))+
  scale_x_continuous(breaks=seq(0,23,length.out=7), labels=seq(0,24,4)) 
ggarrange(pl_cycle1_P, pl_cycle1)

```

Again, we can see that the predictions are equivalent. 

### Not controlling for sampling effort

Let's see what happens when we don't control for sampling effort using the Poisson PDF.

```{r pdf7}
# Fit Poisson model
mod_cycl_P2 <- bam(success ~ 
                     s(Time, bs = "cc", k = 12) + 
                     s(Site, bs="re"), 
                   knots = list(Time=c(0,23)),
                   family = "poisson", 
                   data = y_fall)

# Create predictions and confidence intervals
newdat <- with(occasions_cbind, expand.grid(Time = seq(0, 24, length.out=24),
                                            Site = "7B")) # Station doesn't matter
cycl_pred_P2 <- predict.bam(mod_cycl_P2, newdata = newdat,  exclude = "s(Site)", se.fit = TRUE, type = "link") 
cycl_pred_P2$low <- cycl_pred_P2$fit - (1.96 * cycl_pred_P2$se.fit)
cycl_pred_P2$upp <- cycl_pred_P2$fit + (1.96 * cycl_pred_P2$se.fit)
cycl_pred_P2$Time <- newdat$Time
cycl_pred_P2 <- data.frame(cycl_pred_P2)

```

Let's plot this model output compared to the above Poisson model where we do control for sampling effort.

```{r pdf8, class.source = 'fold-hide'}
pl_cycle1_P2 <- ggplot(cycl_pred_P2, aes(Time, plogis(fit))) +
  geom_ribbon(aes(ymin = plogis(low), ymax = plogis(upp)), alpha = 0.1, linewidth = 0.25) +
  geom_line(aes(), linewidth = 1) +
  labs(x = "Time of Day (Hour)", y = "Predicted Activity Pattern \n (probability)", title = "C: Hierarhical GAM - Poisson NO OFFSET")+
  theme_minimal()+
  theme(legend.position = "bottom",
        legend.title = element_blank(),
        legend.text = element_text(size=10,face="bold"),
        legend.margin=margin(0,0,0,0),
        legend.box.margin=margin(-5,-10,-10,-10),
        plot.title = element_text(size=10,face="bold"),
        axis.line = element_line(colour = 'black', linetype = 'solid'),
        axis.ticks = element_line(colour = 'black', linetype = 'solid'),
        axis.title = element_text(size=9,face="bold"),
        panel.grid.minor.y = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.grid.major.x = element_line(colour = 'lightgrey', linetype = 'dashed', linewidth=0.5),
        panel.grid.minor.x = element_blank(),
        strip.text = element_text(size = 9, colour = "black", face = "bold", hjust = 0))+
  scale_x_continuous(breaks=seq(0,23,length.out=7), labels=seq(0,24,4)) 

ggarrange(pl_cycle1_P, pl_cycle1_P2)
```

We can see that the activity curves have the same shape, but the magnitude on the y-axis is different. Using the offset allows us to better represent animal activity in terms of when they are active and not active. 

## Activity patterns in the Bayesian framework {#bay}

Here, we use simulated data to illustrate how to fit Bayesian versions of the cyclic cubic spline hierarchical models. In particular, we briefly demonstrate how the `brms` package [@brms] can be used to fit an HGAM with random-intercepts.

### Data Preparation

We simulate data using the same approach and the same parameters presented in Tutorial \@ref(est). As such, the simulated data are generated from the same 'true' activity patterns. 

``` {r bay1, message = FALSE, warnings = FALSE}
# Load libraries and function to simulate activity patterns
rm(list = ls())
set.seed(129)
library(dplyr)
library(nimble)
library(tidyr)
library(brms)
source("source_functions/sim_activity.R")
source("source_functions/sim_to_minute.R")

# Set equations' parameters
M = 100 # The number of sites
J = 30 # the number of binomial trials
wavelength = 24 # 24 hour period
n_peak = 2 # number of activity peaks
b0 = -3
b1 = 1 
b2 = 0.7
theta0 = 3
theta1 = 2 
sd_tau = 1
sd_gamma = 0.3
time <- seq(0, wavelength, wavelength/512)

dat <- sim_activity(M = M, 
                   J = J,  
                   wavelength = wavelength, 
                   n_peak = n_peak, 
                   n_sim = 1,  #number of simulated datasets
                   b0 = -3, 
                   b0_constant = TRUE, # common intercept
                   tau_constant = FALSE, 
                   sdtau = sd_tau, # ~site-specific intercept
                   b1 = b1, 
                   b2 = b2, # amplitude of the cosine terms 
                   theta0 = theta0, 
                   theta1 = theta1, # common phaseshifts for the cosine terms
                   phaseshift_constant = FALSE, 
                   sd_phaseshift = sd_gamma, # site-specific phaseshift (equal for both cosine terms)
                   plot_true_act = TRUE
                   )

# The data
sim_data <- as.data.frame(dat$sim_data)
dim(sim_data)

```

The first step is to rearrange the specific format of the data created by the `sim_activity` function (see also Tutorial \@ref(est)). <!-- We access the encounter data and store them in an object called `y`, which has a number of rows equal to the number of simulated sites and a number of columns equal to 24 (hours) times the length (in days) of the simulated sampling period (e.g. 24 hour * 30 days = 720 encounter occasions). We add a column which contains a unique identifier for each site (e.g. `id = A1`) and assign a name to each column that represents the time occasion (e.g. from 1 to 720), then convert the data in long format using the `id` column as reference. -->

```{r data.prep.bayes.cc.1}
# The data
y <- as.data.frame(dat$sim_data)
dim(y)

# summarize aggregated data
y <- data.frame(id=paste("A", seq(1,M,1), sep=""), 
                y=as.data.frame(y)
                )
colnames(y) <- c("id", 
                 paste0("S", seq(1, ncol(y)-1, 1), sep="")
                 )

# Format data in long format
y_long <- pivot_longer(data = y, cols = -id, names_to = "time", values_to = "y") %>%
                mutate(time=as.numeric(substr(time,2,10)),
                       id = factor(id)
                       )

# create variable to describe time as hour (between 0 and 23)
y_long$hour <- sim_to_minute(y_long$time, group = wavelength)
```

<!-- We obtain a data set in which each row specifies the outcome (i.e. encounter or non-encounter of the target species) at a certain site during a specific hour of a certain day of sampling.

To optimize computational time, we can reduce the number of rows in the dataset by counting the number of hour-long occasions in which the species was observed (i.e. successes) and the number of occasion in which the species was not observed (i.e. failures) for each hourly time interval and camera site (see also \@ref(est)). -->

```{r data.prep.bayes.cc.2, message = FALSE, warnings = FALSE}
# count successes and failures at each site-occasion
  occasions_cbind <- y_long %>% 
    group_by(id, hour) %>% 
    summarise(success = sum(y),
              n_events = n(),
              failure = n_events - success
              ) %>% 
    dplyr::rename(Site = id,
                 Time = hour
                 )

# show first few rows of the dataset created
  knitr::kable(head(occasions_cbind))
```

### Model Setup and Fitting

We are now ready to setup the Bayesian cyclic cubic spline model using  code similar to that shown in Tutorial \@ref(est). However, here we wrap our code in the `brm` function from the package `brms` [@brms]. We also need to change our notation slightly to specify the number of successes and the number of trials using `success| trials(n_events)`. <!---From the `brms` R package vignette, "cbind() syntax does not work in `brms` in the expected way because this syntax is reserved for other purposes."---> Note that below, we only consider random intercepts, but we could also include random slopes by adding `s(Time, Site, bs="re")`.

The random intercept can be specified in one of two ways, `s(Site, bs="re")` or `(1 | Site)`. Note that `brms` functions do not allow excluding smooth terms, such as `s(Site, bs="re")` when making predictions ([see here](https://discourse.mc-stan.org/t/excluding-a-smooth-term-in-brms-using-posterior-linpred/10960/4)), so the latter approach is necessary if we want to estimate the conditional mean for a typical site. We compare these two approaches, below.


```{r fit.bayes.cc.3, cache=TRUE, cache.path="cache/", eval=TRUE, echo=TRUE}
# Setup the model but do not fit it (i.e., chains = 0)
cycl_rand_int_setup <- brms::brm(bf(success| trials(n_events)  ~ s(Time, bs = "cc", k = wavelength) +
                                          s(Site, bs="re")
                                   ),
                                   family = "binomial", 
                                   data = occasions_cbind, 
                                   knots = list(Time = c(0,23)),
                                   cores = 1, 
                                   seed = 17,
                                   chains=0
                                 )


cycl_rand_int_setup_alt <- brms::brm(bf(success| trials(n_events)  ~ s(Time, bs = "cc", k = wavelength) +
                                          (1|Site)
                                       ),
                                       family = "binomial", 
                                       data = occasions_cbind, 
                                       knots = list(Time = c(0,23)),
                                       cores = 1, 
                                       seed = 17,
                                       chains=0
                                       )


# Now fit the model
cycl_rand_int <- update(cycl_rand_int_setup, 
                        recompile = FALSE, 
                        newdata = occasions_cbind,
                        chains = 3
                        )
  
cycl_rand_int_alt <- update(cycl_rand_int_setup_alt, 
                            recompile = FALSE, 
                            newdata = occasions_cbind,
                            chains = 3
                            )
```


### Model Assessment 

For Bayesian models, we need to investigate parameter posterior distributions to evaluate whether there is evidence they have converged and thus are reliable to be used for inference. There are plenty of ways to conduct MCMC diagnostics (e.g., @roy2020convergence). A good first step, is to simply visualize the posterior distributions as 'traceplots' and 'density plots' for each chain (independent model fit) to evaluate whether 1) different chains for each parameter are largely the same (plotted on top of each other), and 2) there is good 'mixing' within chain for each parameter. Mixing refers to the process of how the MCMC explores the parameter space to 'fill out' the full posterior distribution; good mixing can generally be assessed visually where the traceplot looks like a 'hairy caterpillar' and density plots look smooth. For a simple introduction see Denis Valle's [How do I tell if my MCMC has converged?](https://drvalle1.github.io/20_MCMC_convergence.html) and for additional details, see [MCMC diagnostics](https://sbfnk.github.io/mfiidd/mcmc_diagnostics.html).

```{r fit.bayes.cc.4}
#Traceplots to check convergence
mcmc_plot(cycl_rand_int, type = "trace")
mcmc_plot(cycl_rand_int_alt, type = "trace")

# Look at the coefficients for the two models and note their equivalence (parameters displayed in different orders)
mcmc_plot(cycl_rand_int, 
         type = "areas",
         prob = 0.95
         )
mcmc_plot(cycl_rand_int_alt, 
         type = "areas",
         prob = 0.95
         )
  
  
# Summary of outputs - coefficients are the same
summary(cycl_rand_int)
summary(cycl_rand_int_alt)

```

### Model Predictions

We use the first parameterization (with `s(Site, bs="re")`) to explore site-level variability in the activity patterns:

```{r fit.bayes.cc.pred1}
# Predict the mean site-level activity pattern
newdata <- data.frame(Time=0:23,
                      Site = occasions_cbind$Site, 
                      n_events=1
                      )
preds <- fitted(cycl_rand_int,scale="response",newdata=newdata)
preds <- data.frame(newdata,preds)
head(preds)

# Plot site level mean activity
ggplot(aes(x=Time, y=Estimate, group=Site, color=Site), data=preds) +
      geom_line() + 
      theme_minimal() + 
      theme(legend.position = "none")
```


We can then approximate the marginal mean activity curve by averaging across the site-level variation.

```{r fit.bayes.cc.pred2, class.source = 'fold-hide', fig.cap="Predicted probability of the marginal mean activity."}
#Average across sites to get the marginal mean
marginal.mean = aggregate(preds$Estimate, list(preds$Time), FUN=mean)
marginal.lower = aggregate(preds$Q2.5, list(preds$Time), FUN=mean)
marginal.upper = aggregate(preds$Q97.5, list(preds$Time), FUN=mean) 

# Plot the marginal mean and compare it to the simulated true mean
plot(marginal.mean$Group.1,marginal.mean$x,type="l",lwd=3,col=1,
       ylim=c(0,0.25),xlab="Time",ylab="Probability of Activity"
     )
lines(marginal.lower$Group.1,marginal.lower$x)
lines(marginal.upper$Group.1,marginal.upper$x)
lines(dat$Marginal[,2],dat$Marginal[,1],lwd=3,col=2)
legend("topright",lwd=c(3,1,1,3),col=c(1,1,1,2),
         legend=c("Marginal Mean", "Lower Marginal Mean","Upper Marginal Mean", "True Marginal Mean")
       )
```

Next, we predict the conditional mean activity using the second parameterization (with `(1 | Site)`).  <!---Note, the first model structure with a random effect spline has no random effects according to `brms`. However, the second model structure does..

```{r fit.bayes.cc.note, error=TRUE}
ranef(cycl_rand_int)
ranef(cycl_rand_int_alt)$Site[1:5,,]
```
 
Because `brms` does not allow smooth terms, such as `s(Site, bs="re")`, to be omitted when predicting, we use the second model to predict the conditional mean activity curve. --->

```{r fit.bayes.cc.predict3, class.source = 'fold-hide', fig.cap="Predicted probability of the conditional mean activity."}
# Predict the conditional mean activity
newdata2 = data.frame(Time=0:23,
                      n_events=1
                      )
preds2=fitted(cycl_rand_int_alt,scale="response", 
              newdata=newdata2,
              re_formula = NA
              )
  
preds2=data.frame(newdata2,preds2)
head(preds2)

# Plot the conditional mean and compare it to the simulated true mean
plot(preds2$Time, preds2$Estimate,type="l",lwd=3,col=1,ylim=c(0,0.25),
       xlab="Time",ylab="Probability of Activity"
     )
lines(preds2$Time,preds2$Q2.5)
lines(preds2$Time,preds2$Q97.5)
lines(dat$Conditional[,2],dat$Conditional[,1],lwd=3,col=2)
legend("topright",lwd=c(3,1,1,3),col=c(1,1,1,2),
         legend=c("Conditional Mean", "Lower Conditional Mean","Upper Conditional Mean", "True Conditional Mean")
       )
```
 


## Link to the Diel.Niche package {#DielNiche}

In the ecology and evolution literature, animal activity is often discussed in terms of diel classifications or phenotypes, such as diurnal, nocturnal, crepuscular, or cathemeral. But, how much activity in what time period does it mean to be diurnal, nocturnal, etc.? To make comparable inference across studies, we should define these diel phenotypes mathematically so that we can make inference based on consistent definitions and thus make appropriate comparisons across species or projects. We can use the [`Diel.Niche` R package](https://github.com/diel-project/Diel-Niche-Modeling/) to do exactly this. First, we use our data to estimate animal diel activity across the 24-hour period and then second, we can classify predicted activity patterns into the corresponding diel phenotype. Here, we demonstrate this approach using a hierarchical cyclic cubic spline model, but the process is the same for any model predicting 24-hour temporal activity.


### Data

We use camera-trap records collected between 2016 and 2018 at 100 locations in Northern Minnesota, USA [@Iannarilliea2021]. We use the same data records of American black bear (*Ursus americanus*) as done in other tutorials and in section \@ref(poisson). However, we ignore examining a season covariate for simplicity and focus on modeling the Spring data. As before, we first load the `occasions_cbind` object that we saved in section \@ref(cat), then subset the data to only include records related to the season of interest, which is now Spring. 

``` {r data1, warnings = TRUE, message = TRUE, warnings = TRUE}
# load packages
library(mgcv)
library(brms)
library(suncalc)
library(lubridate)
library(Diel.Niche)
library(ggplot2)

# Load data
occasions_cbind <- read.csv("data_output/occasions_cbind_Ursus_americanus_seasons.csv")%>% 
  mutate(Site = as.factor(Site))

# Subset the data to the Spring data only, thus removing Fall data.
y_spring <-  occasions_cbind[which(occasions_cbind$Season == "S"),]
head(y_spring)

```

### Model Fitting
Using the Spring data for black bears, we can now fit a hierarchical cyclic cubic spline model.

```{r fit..cc.3, eval=TRUE, echo=TRUE}
cycl_rand_int_ML <- bam(cbind(success, failure) ~ 
                                                 s(Time, bs = "cc", k = 12) + 
                                                 s(Site, bs="re"), 
                                                 knots = list(Time=c(0,23)),
                                                 family = "binomial", 
                                                 data = y_spring
                          )
```

### Model Predictions
Let's now use our model to predict the conditional mean activity curve across all sites. We then use those predictions to define the diel phenotype. Below, we classify each predicted time period into twilight, daytime, and nighttime. To make sure we capture enough predictions to summarize each of these periods, we predict the probability of activity at a smaller interval than how we modeled the data (e.g., 100 intervals instead of 24). 

```{r fit.ml.pred1}
newdat <- with(occasions_cbind, expand.grid(Time = seq(0, 24, length.out=100),total=1, 
                                            Site = "7B")) # Station doesn't matter, as 'site' is excluded below

# Get predictions and of the conditional mean
cycl_pred <- predict.bam(cycl_rand_int_ML, newdata = newdat,  exclude = "s(Site)", type = "response") 
cycl_pred <- data.frame(cycl_pred)
cycl_pred$Time <- newdat$Time
head(cycl_pred)
```

These predictions are absolute probabilities of activity, which incorporates sampling effort. We need to turn these into relative activity probabilities; therefore, assuming the species is active, these are the relative probabilities of activity during each time period. 
 
```{r fit.ml.pred1b}
cycl_pred$cycl_pred.st = cycl_pred$cycl_pred/sum(cycl_pred$cycl_pred)
```

### Twilight, daytime, and nighttime

Next, we need to determine the time periods that correspond to twilight, daytime, and nighttime. To do so, we need to get the relevant astronomical periods that define these periods.  

```{r fit.ml.classify}
# Extract sunrise and sunset times for the general study area and period. 
sun.times <- getSunlightTimes( date = as.Date("2016-04-15"), 
                    lat = 47.73 , lon = -94.55, data = NULL, 
                    keep = c("dawn", "dusk", "nightEnd", "night"), 
                    tz = "America/Chicago")

# Use the extracted sun times to define dawn and dusk
dawn <- c(hour(sun.times$nightEnd)+minute(sun.times$nightEnd)/60,
         hour(sun.times$dawn)+minute(sun.times$dawn)/60)
  
dusk <- c(hour(sun.times$dusk)+minute(sun.times$dusk)/60,
         hour(sun.times$night)+minute(sun.times$night)/60)
  
# Create a vector that defines each time period of cycl_pred$Time into twilight (combination of dawn and dusk)
# daytime, and nighttime  
diel.class <- rep(NA,nrow(cycl_pred))
diel.class[cycl_pred$Time>=dawn[1] & cycl_pred$Time<=dawn[2]] <- "Twilight"
diel.class[cycl_pred$Time>=dusk[1] & cycl_pred$Time<=dusk[2]] <- "Twilight"
diel.class[cycl_pred$Time>=dawn[2] & cycl_pred$Time<=dusk[1]] <- "Day"
# the undefined time periods are then the night  
diel.class[is.na(diel.class)] <- "Night"
cycl_pred$diel.class <- diel.class

# Sum the standardized predictions of activity by diel period
# These are the relative probabilities of activity  
diel.summary <- aggregate(cycl_pred$cycl_pred.st, by=list(diel.class=cycl_pred$diel.class), FUN=sum)
diel.summary
```

### Diel.Niche

Next, we reorganize the probabilities of activity in the order used by the `Diel.Niche` package: twilight, daytime, nighttime. For details about the `Diel.Niche` package, see the [Github repository](https://github.com/diel-project/Diel-Niche-Modeling/) and the associated [manuscript](https://doi.org/10.1111/1365-2656.14035).


```{r fit.ml.pred2}  
y = t(matrix(c(diel.summary$x[3],
                 diel.summary$x[1], 
                 diel.summary$x[2]
                 )
               )
        )
colnames(y)=c("P_twilight","P_daytime","P_nightime")
  
# Use the diel niche package to extract the associated diel phenotype within the Traditional hypothesis set  
diel.out <- Diel.Niche::posthoc.niche(y, hyp= hyp.sets("Traditional"))
diel.out
```
The corresponding diel phenotype for the conditional mean activity is the **Cathemeral** phenotype.

Lastly, lets plot the conditional mean activity with the supported diel phenotype for these results. 
```{r fit.ml.pred3}  
plot(cycl_pred$Time,cycl_pred$cycl_pred,type="l",col=2,lwd=3, 
       main=paste("Spring Black Bear Activity \n",diel.out$Hypothesis),
       ylab="Probability of Activity",xlab="Time")
  abline(v=c(dawn[1],dawn[2]),lty=3)
  abline(v=c(dusk[1],dusk[2]),lty=3)
```

The vertical dotted lines indicate the periods of dawn and dusk for this sampling period. As we can see, black bears are mostly active during the day with a relative probability activity of `r round(diel.out$p.day,digits=2)`. They are also have a high relative activity during twilight with a probability of `r round(diel.out$p.twi,digits=2)`. 

Interestingly, if we use a set of diel phenotype hypotheses that are more specific in separating bimodal and trimodal activity (i.e. General hypothesis set; see definitions at [Github repository](https://github.com/diel-project/Diel-Niche-Modeling/)) we would find the correspondent diel phenotype to be better described as **Diurnal-Crepuscular**.
```{r fit.ml.pred4}  
Diel.Niche::posthoc.niche(y, hyp= hyp.sets("General"))
```
This difference in classification is because of the low amount of activity during the nighttime. It is hard to describe this activity pattern as Cathemeral, given that all three time periods (twilight, daytime, and nighttime) are not used. More specifically, black bears are concentrating their activity during twilight and daytime, thus the more accurate description would be **Diurnal-Crepuscular**.

### Conclusion

Modeling animal activity patterns continuously throughout the 24-hour period provides important fine scale inference that is not achieved when aggregating data into blocks of time (e.g., daytime vs nighttime). However, it is important to then still be able to translate findings in relevant terminology based on diel phenotypes. The [`Diel.Niche` R package](https://github.com/diel-project/Diel-Niche-Modeling/) provides an efficient way of summarizing findings from hierarchical trigonometric and cyclic cubic spline models.

Here, we provided a simple example to demonstrate the process of using the [`Diel.Niche` R package](https://github.com/diel-project/Diel-Niche-Modeling/). However, there are a number of options we could also consider. For example, we could predict the diel phenotype for each site. This is most useful when we have site-to-site variation in terms of a random intercept and slope. We could also use covariates in our model and predict the supported diel phenotype across covariate values. Furthermore, we only predicted the corresponding diel phenotype using the estimated conditional mean activity. There is of course uncertainty associated with this mean activity level. To explore how supported the diel phenotype is when considering uncertainty from the conditional mean, we could predict activity curves at different levels of uncertainty or at certain confidence intervals to investigate uncertainty in the associated diel phenotype.

## Info session

To facilitate reproducibility, we report information on packages and package versions used to create these tutorials.

```{r infosession}
library(grateful)
pkgs <- cite_packages(output = "table", out.dir = ".")
knitr::kable(pkgs)

sessionInfo()
```
