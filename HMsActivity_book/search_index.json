[["index.html", "A ‘How-to’ Guide for Estimating Animal Diel Activity Using Hierarchical Models Tutorial 1 Preface 1.1 Requirements", " A ‘How-to’ Guide for Estimating Animal Diel Activity Using Hierarchical Models Fabiola Iannarilli Department of Fisheries, Wildlife and Conservation Biology, St. Paul, Minnesota, USA; Max Planck Institute of Animal Behavior, Konstanz, Germany fabiola.iannarilli@gmail.com Brian D. Gerber USGS, Colorado Cooperative Fish and Wildlife Research Unit, Colorado State University, Fort Collins, Colorado, USA John Erb Minnesota Department of Natural Resources, Grand Rapids, Minnesota, USA John R. Fieberg Department of Fisheries, Wildlife and Conservation Biology, St. Paul, Minnesota, USA Tutorial 1 Preface This tutorial accompanies the manuscript Iannarilli et al. (2024) and contains step-by-step examples on how to use trigonometric and cyclic cubic spline hierarchical models to analyze activity patterns from time-stamped data. We present case studies based on camera-trap data, but these approaches can be applied to data collected via other static sensors (e.g., acoustic recorders). We start the tutorial by introducing the concept of site-to-site variability in diel activity patterns. Using simulated data, we explore two components of this variability, variability in frequency of site-use and variability in timing of activity (Tutorial 2). We then introduce trigonometric generalized linear mixed models and cyclic cubic spline hierarchical generalized additive models as approaches to estimate animals’ diel activity patterns (Tutorial 3). From Tutorial 4 to 7, we use real-case studies to illustrate how to organize the data (Tutorial 4) and apply these hierarchical model approaches. In particular, we present three applications typical of ecological studies focused on activity patterns. We explore: whether a certain species of interest concentrates its activity at one or two specific times of the day (i.e. unimodal and bimodal patterns), or whether its activity is constant throughout the 24-hour cycle (i.e. cathemeral pattern; Tutorial 5); whether changes in diel activity are driven by seasonal or other biotic and abiotic sources of variation (Tutorial 6). We present examples using both categorical (section 6.1) and continuous (section 6.2) variables; whether changes in diel activity are linked to the co-occurrence of another species, providing support for the existence of time-partitioning mechanisms that facilitate species’ coexistence (Tutorial 7). The tutorials also include a description of the difference between conditional and marginal mean activity patterns, how we can obtain both means and how this difference is relevant depending on the ecological questions at hand (Tutorial 8). We also provide a Tutorial focused on Kernel Density Estimators (KDEs), currently the most used approach for the estimate of activity patterns (Tutorial 9). We compare the three R-packages currently available to estimate diel activity using KDEs (section 9.2), illustrate the consequences of non-aggregating the data in independent events when estimating diel activity (section 9.3), and compare estimates of activity based on KDEs and hierarchical models (section 9.4). In the Appendix, we provide several additional resources, including: 1. a description of the function sim_activity, a custom-built function used in Tutorials 2 and 3 and throughout the tutorial to simulate activity patterns under different conditions (section 10.1); 2. a formulation of the trigonometric and cyclic cubic hierarchical models based on the Poisson distribution, an alternative to the formulation based on the binomial distribution used throughout the tutorial (section 10.2); 3. an example of how to apply these models in a Bayesian framework (section 10.3); 4. a link to the Diel.Niche R package, a framework recently introduced to robustly characterize animal activity patterns as diel phenotypes (section 10.4). All analyses were performed in R Core Team (2024) and the tutorial was compiled using bookdown (Xie 2024a, 2016a). The camera-trap data used in the real-case examples were collected at 100 locations sampled between 2016 and 2018 in Northern Minnesota, USA (Iannarilli et al. 2021). Further details on these data can be found in Iannarilli et al. (2021), and data and code to reproduce this tutorial can be accessed at https://github.com/FabiolaIannarilli/HMs_Activity. 1.1 Requirements Throughout this tutorial, we use several packages. Users interested in compiling the tutorial from source or in reproducing the analysis on their own machine should install them. The code below provides a list of all the libraries needed, checks if they are already installed in the user’s local machine, and installs those packages that are not. list.of.packages &lt;- c(&quot;dplyr&quot;, &quot;grid&quot;, &quot;gridExtra&quot;, &quot;GLMMadaptive&quot;, &quot;ggpubr&quot;, &quot;mgcv&quot;, &quot;tidyr&quot;, &quot;lubridate&quot;, &quot;lmtest&quot;, &quot;activity&quot;, &quot;overlap&quot;, &quot;circular&quot;, &quot;nimble&quot;, &quot;brms&quot;, &quot;forcats&quot;, &quot;MESS&quot;, &quot;suncalc&quot;, &quot;grateful&quot; ) new.packages &lt;- list.of.packages[!(list.of.packages %in% installed.packages()[,&quot;Package&quot;])] if(length(new.packages)&gt;0) install.packages(new.packages) if(!(&quot;Diel.Niche&quot; %in% installed.packages()[,&quot;Package&quot;])){ # Install package from GitHub via the devtools package. devtools::install_github(&quot;diel-project/Diel-Niche-Modeling&quot;, ref=&quot;main&quot;, dependencies = TRUE, build_vignettes = FALSE ) } "],["var.html", "Tutorial 2 Variability in activity patterns 2.1 Variability in frequency of site-use 2.2 Variability in timing of activity", " Tutorial 2 Variability in activity patterns We begin the tutorial by illustrating how site-to-site variability can affect animals’ diel activity patterns. Site-to-site variability can be decomposed into two components: 1. variability in frequency of site-use and 2. variability in timing of activity. Here, we illustrate these two components separately in the context of a bimodal activity pattern (i.e. a pattern in which an animal’s activity is concentrated at two times of the 24-hour cycle) first, and then in the context of a unimodal activity pattern (i.e. a pattern in which an animal’s activity is concentrated at one specific time within the 24-hour cycle). Real activity patterns are likely to exhibit both sources of variability, so, in Tutorial 3, we present an example in which we simulate activity patterns by simultaneously varying both the frequency of site-use and variability in timing of activity. We simulated data under equation 1 explained in the main text (Materials and Results in Iannarilli et al. (2024)) and reported here as: \\[ y_{it} \\sim Bernoulli(p_{it}) \\] \\[ \\text{logit}(p_{it}) = \\beta_0 + \\beta_1*\\text{cos}(\\frac{2\\pi t}{\\omega_1} + (\\theta_0 + \\gamma_i)) + \\beta_2*\\text{cos}(\\frac{2\\pi t}{omega_1} + (\\theta_1 + \\gamma_i)) + \\tau_i \\] \\[ \\tau_i \\sim N(0,\\sigma_\\tau) \\] \\[ \\gamma_i \\sim N(0,\\sigma_\\gamma) \\] We load the necessary libraries and set the intercept \\(\\beta_0\\), and the two amplitudes \\(\\beta_1\\) and \\(\\beta_2\\) equal to -3, 1, and 0.7, respectively. We also assign the values 3 and 2 to the common phaseshifts terms, \\(\\theta_0\\) and \\(\\theta_1\\), respectively, and set \\(\\omega_1\\) and \\(\\omega_2\\) equal to 24 and 12, respectively. The parameters \\(\\tau_i\\) and \\(\\gamma_i\\) (and their associated distributions) control the level of variability in site-use (i.e. vertical shift among the activity curves) and timing of activity (i.e. horizontal shift among activity curves), respectively. For illustrative purposes, here we assign predefined values to \\(\\tau_i\\) and \\(\\gamma_i\\), and vary only one of them at a time. # Load libraries rm(list = ls()) set.seed(129) library(dplyr) ## Warning: package &#39;dplyr&#39; was built under R version 4.2.3 library(grid) library(gridExtra) library(ggplot2) ## Warning: package &#39;ggplot2&#39; was built under R version 4.2.3 #Set equation parameters wavelength = 24 b0 = -3 b1 = 1 b2 = 0.7 theta0 = 3 theta1 = 2 tau_i = gamma_i = seq(-1.5, 1.5, by = 0.5) time &lt;- seq(0, 23, length.out = 100) 2.1 Variability in frequency of site-use To create examples of variability in frequency of site-use, we set \\(\\gamma_i = 0\\), keep all the other parameters constant, and vary \\(\\tau_i\\) from -1.5 to 1.5 in increments of 0.5. We create the curves on the logit scale, based on equation 1, but then transform them back to the probability scale, for ease of interpretation. # create the curves p_df&lt;-data.frame() for(i in 1:7){ p_df&lt;-rbind(p_df, data.frame(p = plogis(b0 + b1*cos(2*pi*time/(24)+theta0 + gamma_i[4]) + b2*cos(2*pi*time/(12)+ theta1 + gamma_i[4]) + tau_i[i] ), time = time, curvesID = rep(as.factor(i), length(time)) ) ) } p_df$curvesLeg = as.factor(rep(c(&quot;+/- 0.5 SD&quot;, &quot;+/- 1 SD&quot;, &quot;+/- 1.5 SD&quot;, &quot;Conditional&quot;, &quot;+/- 0.5 SD&quot;, &quot;+/- 1 SD&quot;, &quot;+/- 1.5 SD&quot;), each = length(time) ) ) p_df$mean &lt;- c(rep(rep(&quot;off&quot;, length(time)),3), rep(&quot;on&quot;, length(time)), rep(rep(&quot;off&quot;, length(time)),3) ) We can then calculate the marginal mean by integrating over the distribution of the random effects. p_marg &lt;- matrix(NA,length(time), 1) for(i in 1:length(time)){ intfun&lt;-function(tau_x){ plogis(b0 + b1*cos(2*pi*time[i]/(24) + theta0 + gamma_i[4]) + b2*cos(2*pi*time[i]/(12) + theta1 + gamma_i[4]) + tau_x ) * dnorm(tau_x, mean = 0, sd = 1) } p_marg[i]&lt;-integrate(intfun,-Inf, Inf)[1] } We discuss differences between conditional and marginal means in detail in Tutorial 8 and in Box 1 in the main text (Iannarilli et al. 2024). Although we can consider conditional means for any site, it is often instructive to consider the conditional mean curve for a “typical site”, which is obtained by setting all the random effects equal to zero; it is this conditional mean we are referring to if we do not mention a specific site. We can plot the conditional and marginal curves using the code below: # Combine dataframes for plotting p_df2 &lt;- data.frame(p = unlist(p_marg), time = time, mean = &quot;on&quot;, curvesID = as.factor(8), curvesLeg = as.factor(&quot;Marginal&quot;) ) p_df &lt;- rbind(p_df, p_df2) # plot activity curves (pl_vert &lt;- ggplot(p_df, aes(x = time, y = p, group = curvesID)) + geom_line(aes(color = curvesLeg, linewidth = mean, linetype = mean)) + scale_color_manual(values = c(&quot;grey40&quot;, &quot;grey60&quot;, &quot;grey80&quot;, &quot;red&quot;, &quot;black&quot;)) + scale_linewidth_manual(values = c(0.5, 1.5)) + scale_linetype_manual(values = c(2, 1, 2)) + coord_cartesian(ylim = c(-0.05, 0.45)) + labs(x = &quot;Time of Day&quot;, y = &quot;Probability of Activity&quot;, title = &quot;A) Variability in \\n frequency of site-use&quot;) + theme_minimal()+ theme(legend.position = &quot;none&quot;, axis.line = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.ticks.x = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), #element_blank(), # axis.text.y = element_blank(), axis.title=element_text(size = 8,face = &quot;bold&quot;), plot.title=element_text(size = 9,face = &quot;bold&quot;), panel.grid.minor = element_blank(), panel.grid.major = element_blank() ) + geom_segment(aes(x=12, y=min(p), xend=12, yend=min(p)-0.05), arrow=arrow(length= unit(0.5, &quot;cm&quot;)), linewidth = 1, color = &quot;orange&quot;, linejoin = &quot;mitre&quot;) + geom_segment(aes(x=12, y=0.35+0.003, xend=12, yend=0.35+0.053), arrow=arrow(length= unit(0.5, &quot;cm&quot;)), linewidth = 1, color = &quot;orange&quot;, linejoin = &quot;mitre&quot;)+ scale_x_continuous(breaks=seq(0,24,length.out=7), labels=seq(0,24,4)) ) Figure 2.1: Probability activity curves across sites (dashed lines) along with the marginal mean curve (black line) and conditional mean curve (red line). In this first plot, the dashed grey curves represent the curves generated using the different values assigned to \\(\\tau_i\\) (darker to lighter colors from lower to higher in absolute values). All other things equal, these different values of \\(\\tau_i\\) lead to a vertical shift in the activity curves, which corresponds to a change in the frequency of site-use. The red curve gives the conditional mean for a “typical site” (estimated by setting \\(\\tau_i = 0\\) and \\(\\gamma_i = 0\\) in equation 1), and the black curve gives the marginal mean. 2.2 Variability in timing of activity To create examples of variability in timing of activity, we set the random phaseshift \\(\\gamma_i\\) as equal to 0, \\(\\pm\\) 0.5, \\(\\pm\\) 1.0, and \\(\\pm\\) 1.5, while keeping \\(\\tau_i = 0\\) and the remaining parameters with the same values used before. We create and plot the curves, then join the two plots to create the first portion of Figure 1, reported in the main text (Iannarilli et al. 2024). # create the curves p_df&lt;-data.frame() for(i in 1:7){ p_df&lt;-rbind(p_df, data.frame(p = plogis(b0 + b1*cos(2*pi*time/(24)+theta0 + gamma_i[i]) + b2*cos(2*pi*time/(12)+ theta1 + gamma_i[i]) + tau_i[4] ), time = time, curvesID = rep(as.factor(i), length(time)) ) ) } p_df$curvesLeg = as.factor(rep(c(&quot;+/- 0.5 SD&quot;, &quot;+/- 1 SD&quot;, &quot;+/- 1.5 SD&quot;, &quot;Conditional&quot;, &quot;+/- 0.5 SD&quot;, &quot;+/- 1 SD&quot;, &quot;+/- 1.5 SD&quot;), each = length(time) ) ) p_df$mean &lt;- c(rep(rep(&quot;off&quot;, length(time)),3), rep(&quot;on&quot;, length(time)), rep(rep(&quot;off&quot;, length(time)),3) ) # calculate the marginal mean activity curve by integrating over the distribution of the random effects p_marg &lt;- matrix(NA,length(time), 1) for(i in 1:length(time)){ intfun&lt;-function(gamma_x){ plogis(b0 + b1*cos(2*pi*time[i]/(24)+ theta0 + gamma_x) + b2*cos(2*pi*time[i]/(12)+theta1 + gamma_x) + tau_i[4] ) *dnorm(gamma_x, mean = 0, sd = 1) } p_marg[i]&lt;-integrate(intfun,-Inf, Inf)[1] } p_df2 &lt;- data.frame(p = unlist(p_marg), time = time, mean = &quot;on&quot;, curvesID = as.factor(8), curvesLeg = as.factor(&quot;Marginal&quot;) ) p_df &lt;- rbind(p_df, p_df2) # plot pl_hor &lt;- ggplot(p_df, aes(x = time, y = p, group = curvesID)) + geom_line(aes(color = curvesLeg, linewidth = mean, linetype = mean)) + scale_color_manual(values = c(&quot;grey40&quot;, &quot;grey60&quot;, &quot;grey80&quot;, &quot;red&quot;, &quot;black&quot;)) + scale_linewidth_manual(values = c(0.5, 1.5)) + scale_linetype_manual(values = c(2, 1)) + labs(x = &quot;Time of Day&quot;, y = &quot;Probability of Activity&quot;, title = &quot;B) Variability in \\n timing of activity&quot;) + theme_minimal()+ theme(legend.position = &quot;none&quot;, axis.line = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.ticks.x = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.text.y = element_blank(), axis.title=element_text(size=8,face=&quot;bold&quot;), plot.title=element_text(size=9,face=&quot;bold&quot;), panel.grid.minor = element_blank(), panel.grid.major = element_blank() ) + geom_segment(aes(x=12 - 0.75, y=max(p) + 0.02, xend=12 - 7, yend=max(p) + 0.02), arrow=arrow(length= unit(0.5, &quot;cm&quot;)), linewidth = 1, color = &quot;orange&quot;, linejoin = &quot;mitre&quot;) + geom_segment(aes(x=12 + 0.75, y=max(p) + 0.02, xend=12 + 7, yend=max(p) + 0.02), arrow=arrow(length= unit(0.5, &quot;cm&quot;)), linewidth = 1, color = &quot;orange&quot;, linejoin = &quot;mitre&quot;) + scale_x_continuous(breaks=seq(0,24,length.out=7), labels=seq(0,24,4))+ guides(linetype=element_blank()) # Join plots pl &lt;- grid.arrange(pl_vert, pl_hor, ncol = 2) Figure 2.2: Bimodal probability activity curves showing variability in frequency of site-use (A) and variability in timing of activity (B); displayed are activity curves across sites (dashed lines) along with the marginal mean curves (black lines) and conditional mean curves (red lines). The dashed lines in Figure 2.2 panel B represent curves simulated using different values of \\(\\gamma_i\\) (darker to lighter colors from lower to higher in absolute values). Varying the random phaseshift \\(\\gamma_i\\) results in activity curves that are shifted horizontally. Because we are simulating data using two cosine terms, the cumulative effects of the two random phaseshifts (one per cosine term) also affect the intensity of site-use. Site-to-site variability in a unimodal pattern. For completeness, we also present how variability affects a unimodal activity pattern. We can simulate this activity pattern by re-running the code above without the second cosine term, that is, by setting \\(\\beta_2 = 0\\). #Setup data p_df&lt;-data.frame() for(i in 1:7){ p_df&lt;-rbind(p_df, data.frame(p = plogis(b0 + b1*cos(2*pi*time/(24)+theta0 + gamma_i[4]) + tau_i[i]), time = time, curvesID = rep(as.factor(i), length(time)) ) ) } p_df$curvesLeg = as.factor(rep(c(&quot;+/- 0.5 SD&quot;, &quot;+/- 1 SD&quot;, &quot;+/- 1.5 SD&quot;, &quot;Conditional&quot;, &quot;+/- 0.5 SD&quot;, &quot;+/- 1 SD&quot;, &quot;+/- 1.5 SD&quot;), each = length(time) ) ) # change in frequency of site-use p_df$mean &lt;- c(rep(rep(&quot;off&quot;, length(time)),3), rep(&quot;on&quot;, length(time)), rep(rep(&quot;off&quot;, length(time)),3) ) # calculate the marginal mean activity curve p_marg &lt;- matrix(NA,length(time), 1) for(i in 1:length(time)){ intfun&lt;-function(tau_x){ plogis(b0 + b1*cos(2*pi*time[i]/(24)+ theta0 + gamma_i[4]) + tau_x ) * dnorm(tau_x, mean = 0, sd = 1) } p_marg[i]&lt;-integrate(intfun,-Inf, Inf)[1] } p_df2 &lt;- data.frame(p = unlist(p_marg), time = time, mean = &quot;on&quot;, curvesID = as.factor(8), curvesLeg = as.factor(&quot;Marginal&quot;) ) p_df &lt;- rbind(p_df, p_df2) # plot pl_vert_uni &lt;- ggplot(p_df, aes(x = time, y = p, group = curvesID)) + geom_line(aes(color = curvesLeg, linewidth = mean, linetype = mean)) + scale_color_manual(values = c(&quot;grey40&quot;, &quot;grey60&quot;, &quot;grey80&quot;, &quot;red&quot;, &quot;black&quot;)) + scale_linewidth_manual(values = c(0.5, 1.5)) + scale_linetype_manual(values = c(2, 1, 2)) + coord_cartesian(ylim = c(-0.05, max(p_df$p)+0.053)) + labs(x = &quot;Time of Day&quot;, y = &quot;Activity Patterns&quot;, title = &quot;A) Variability in frequency of site-use&quot;) + theme_minimal()+ theme(legend.position = &quot;none&quot;, axis.line = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.ticks.x = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.text.y = element_blank(), #element_text(size=8), axis.title=element_text(size=10,face=&quot;bold&quot;), panel.grid.minor = element_blank(), panel.grid.major = element_blank() ) + geom_segment(aes(x=12, y=min(p), xend=12, yend=min(p)-0.05), arrow=arrow(length= unit(0.5, &quot;cm&quot;)), linewidth = 1, color = &quot;orange&quot;, linejoin = &quot;mitre&quot;) + geom_segment(aes(x=12, y=max(p)+0.003, xend=12, yend=max(p)+0.053), arrow=arrow(length= unit(0.5, &quot;cm&quot;)), linewidth = 1, color = &quot;orange&quot;, linejoin = &quot;mitre&quot;)+ scale_x_continuous(breaks=seq(0,24,length.out=7), labels=seq(0,24,4)) # change in timing of activity # creating the curves p_df&lt;-data.frame() for(i in 1:7){ p_df&lt;-rbind(p_df, data.frame(p = plogis(b0 + b1*cos(2*pi*time/(24)+theta0 + gamma_i[i]) + tau_i[4]), time = time, curvesID = rep(as.factor(i), length(time)) ) ) } p_df$curvesLeg = as.factor(rep(c(&quot;+/- 0.5 SD&quot;, &quot;+/- 1 SD&quot;, &quot;+/- 1.5 SD&quot;, &quot;Conditional&quot;, &quot;+/- 0.5 SD&quot;, &quot;+/- 1 SD&quot;, &quot;+/- 1.5 SD&quot;), each = length(time) ) ) p_df$mean &lt;- c(rep(rep(&quot;off&quot;, length(time)),3), rep(&quot;on&quot;, length(time)), rep(rep(&quot;off&quot;, length(time)),3)) # calculate the marginal mean activity curve by integrating over the distribution of the random effects p_marg &lt;- matrix(NA,length(time), 1) for(i in 1:length(time)){ intfun&lt;-function(gamma_x){ plogis(b0 + b1*cos(2*pi*time[i]/(24)+ theta0 + gamma_x) +tau_i[4] ) * dnorm(gamma_x, mean = 0, sd = 1) } p_marg[i]&lt;-integrate(intfun,-Inf, Inf)[1] } p_df2 &lt;- data.frame(p = unlist(p_marg), time = time, mean = &quot;on&quot;, curvesID = as.factor(8), curvesLeg = as.factor(&quot;Marginal&quot;) ) p_df &lt;- rbind(p_df, p_df2) # plot pl_hor_uni &lt;- ggplot(p_df, aes(x = time, y = p, group = curvesID)) + geom_line(aes(color = curvesLeg, linewidth = mean, linetype = mean)) + scale_color_manual(values = c(&quot;grey40&quot;, &quot;grey60&quot;, &quot;grey80&quot;, &quot;red&quot;, &quot;black&quot;)) + scale_linewidth_manual(values = c(0.5, 1.5)) + scale_linetype_manual(values = c(2, 1)) + labs(x = &quot;Time of Day&quot;, y = &quot;Activity Patterns&quot;, title = &quot;B) Variability in timing of activity&quot;) + theme_minimal()+ theme(legend.position = &quot;none&quot;, axis.line = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.ticks.x = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.text.y = element_blank(), #element_text(size=8), axis.title=element_text(size=10,face=&quot;bold&quot;), panel.grid.minor = element_blank(), panel.grid.major = element_blank() ) + geom_segment(aes(x=13.5 - 0.75, y=max(p) + 0.02, xend=13.5 - 7, yend=max(p) + 0.02), arrow=arrow(length= unit(0.5, &quot;cm&quot;)), linewidth = 1, color = &quot;orange&quot;, linejoin = &quot;mitre&quot;) + geom_segment(aes(x=13.5 + 0.75, y=max(p) + 0.02, xend=13.5 + 7, yend=max(p) + 0.02), arrow=arrow(length= unit(0.5, &quot;cm&quot;)), linewidth = 1, color = &quot;orange&quot;, linejoin = &quot;mitre&quot;) + scale_x_continuous(breaks=seq(0,24,length.out=7), labels=seq(0,24,4)) + guides(linetype=element_blank()) # Join plots pl_uni &lt;- grid.arrange(pl_vert_uni, pl_hor_uni, ncol = 2) Figure 2.3: Unimodal probability activity curves showing variability in frequency of site-use (A) and variability in timing of activity (B); displayed are activity curves across sites (dashed lines) along with the marginal mean curves (black lines) and conditional mean curves (red lines). This second example (Figure 2.3) clearly illustrates how varying \\(\\gamma_i\\) (while keeping all the other parameters constant) leads to the horizontal shift of the curves, with peaks in activity equal across time but occurring at different times of the diel cycle. In the next Tutorial, we simulate data drawing values of \\(\\tau_i\\) and \\(\\gamma_i\\) from the normal distributions specified at the beginning of this Tutorial. We use these simulated data to illustrate how trigonometric and cyclic cubic regression spline hierarchical models can be used to estimate activity patterns. We will further elaborate on differences between conditional and marginal mean activity patterns in Tutorial 8. "],["est.html", "Tutorial 3 Estimating activity patterns from time-stamped data 3.1 Simulating activity patterns 3.2 Data preparation 3.3 Trigonometric GLMMs 3.4 Cyclic cubic spline HGAMs", " Tutorial 3 Estimating activity patterns from time-stamped data Activity patterns are currently estimated primarily using Kernel Density Estimators (KDEs). However, this approach presents several limitations that we highlight in Iannarilli et al. (2024). In this Tutorial, we introduce two alternative, model-based approaches to estimate activity patterns from time-stamped data: trigonometric generalized linear mixed models (trigonometric GLMMs) and cyclic cubic spline generalized linear mixed models (cyclic cubic spline HGAMs). Both types of hierarchical models can accommodate the periodic nature of activity data and also address site-to-site and other forms of variability. Although our examples are all focused on the analysis of camera-trap data, the same framework can be applied to data collected using other static devices (e.g., acoustic recorders) or site-based sampling strategies. 3.1 Simulating activity patterns To show how trigonometric and cyclic cubic spline hierarchical models can successfully recover the true pattern of activity, we apply these approaches to simulated data. As such, we can compare the estimated patterns to the known, ‘true’, simulated patterns. To keep the focus on estimating activity patterns, we encapsulate the code to simulate the data in the function sim_activity, that we source after loading the necessary libraries. This function simulates data based on equation 1 (Tutorial 2 and Materials and Results in Iannarilli et al. (2024)). The function is available in the source_functions folder available at https://github.com/FabiolaIannarilli/HMs_Activity; a description of its arguments and outputs is provided in section 10.1. # Load libraries and function to simulate activity patterns set.seed(129) suppressWarnings({ library(dplyr) library(GLMMadaptive) library(ggpubr) library(mgcv) library(tidyr) }) source(&quot;source_functions/sim_activity.R&quot;) source(&quot;source_functions/sim_to_minute.R&quot;) We simulate site-specific bimodal activity patterns for 100 sites (M) and 30 days (J) using the parameters defined in the code below. We include both variability in frequency of site-use and variability in timing of activity by drawing values of \\(\\tau_i\\) and \\(\\gamma_i\\) from the distributions \\(\\tau_i \\sim N(0,\\sigma_\\tau)\\) and \\(\\gamma_i \\sim N(0,\\sigma_\\gamma)\\), respectively, with \\(\\sigma_\\tau = 1\\) and \\(\\sigma_\\gamma = 0.3\\). # Set equations&#39; parameters M = 100 J = 30 wavelength = 24 n_peak = 2 b0 = -3 b1 = 1 b2 = 0.7 theta0 = 3 theta1 = 2 sd_tau = 1 sd_gamma = 0.3 time &lt;- seq(0, 23, length.out = 100) # simulate data dat &lt;- sim_activity(M = M, J = J, wavelength = wavelength, n_peak = n_peak, n_sim = 1, b0 = b0, b0_constant = TRUE, # common intercept tau_constant = FALSE, sdtau = sd_tau, # ~site-specific intercept b1 = b1, b2 = b2, # amplitude of the cosine terms theta0 = theta0, theta1 = theta1, # common phaseshifts for the cosine terms phaseshift_constant = FALSE, sd_phaseshift = sd_gamma, # site-specific phaseshift (equal for both cosine terms) plot_true_act = FALSE ) #Observe the structure of the new object str(dat) ## List of 5 ## $ true_activity_prob: num [1:100, 1:720] 0.00354 0.00474 0.00263 0.00297 0.07437 ... ## $ Conditional :&#39;data.frame&#39;: 513 obs. of 3 variables: ## ..$ p : num [1:513] 0.0138 0.0135 0.0133 0.0131 0.0129 ... ## ..$ time : num [1:513] 0 0.0469 0.0938 0.1406 0.1875 ... ## ..$ y_dens: num [1:513] 0.00931 0.00915 0.009 0.00885 0.00871 ... ## $ Marginal :&#39;data.frame&#39;: 513 obs. of 3 variables: ## ..$ p : num [1:513] 0.0233 0.0229 0.0226 0.0222 0.0219 ... ## ..$ time : num [1:513] 0 0.0469 0.0938 0.1406 0.1875 ... ## ..$ y_dens: num [1:513] 0.0115 0.0113 0.0111 0.0109 0.0107 ... ## $ sim_data :List of 1 ## ..$ : int [1:100, 1:720] 0 0 0 0 0 1 0 0 0 0 ... ## $ phaseshift : num [1:100] 0.4761 -0.3666 -0.0015 -0.2245 -0.1194 ... The sim_activity function returns a list containing several objects, including the true activity patterns simulated for each of the 100 sites (object: dat$true_activity_prob) and the associated realized patterns (i.e. the encounter data; object: dat$sim_data). The list also contains the values of the conditional and marginal mean probability of activity (objects: dat$Conditional and dat$Marginal, respectively) and site-specific phaseshift values (object: dat$phaseshift). We access the information for a randomly selected set of these sites and plot their true activity patterns, along with the conditional and marginal mean activity patterns (the red and black curves, respectively, in the following plot). # Organize data for plotting sites_i &lt;- sample(x = 1:M, size = 10, replace = FALSE) sample_sites &lt;- cbind(sites_i, dat$true_activity_prob[sites_i, 0:25]) colnames(sample_sites) &lt;- c(&quot;sites&quot;, 00:24) sample_sites &lt;- as.data.frame(sample_sites) %&gt;% pivot_longer(cols = -sites, names_to = &quot;time&quot;, values_to = &quot;prob&quot;) #plot true conditional and marginal with 10 simulated sites (pl_sites &lt;- ggplot() + geom_line(data = sample_sites, aes(x = as.numeric(time), y = prob, group = sites, color = as.factor(sites)), linewidth = 0.5, alpha = 0.7, linetype = 2) + geom_line(data = dat$Conditional, aes(x = time, y = p), linewidth = 1, color = &quot;red&quot;, inherit.aes = TRUE) + geom_line(data = dat$Marginal, aes(x = time, y = p), linewidth = 1, color = &quot;black&quot;, inherit.aes = TRUE) + labs(x = &quot;Time of Day&quot;, y = &quot;Probability of Activity&quot;, title = &quot;C) Variability in frequency of site-use and \\n timing of activity&quot;, color = &quot;Site_ID&quot;)+ theme_minimal()+ theme(legend.position = &quot;none&quot;, legend.title = element_text(size=10,face=&quot;bold&quot;), legend.text = element_text(size=10,face=&quot;bold&quot;), axis.line = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.ticks.x = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.title = element_text(size=8,face=&quot;bold&quot;), axis.text.y = element_blank(), plot.title = element_text(size=9,face=&quot;bold&quot;), panel.grid.minor.y = element_blank(), panel.grid.major.y = element_blank(), panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank(), strip.text = element_text(size = 9, colour = &quot;black&quot;, face = &quot;bold&quot;, hjust = 0))+ geom_segment(aes(x=12, y=min(sample_sites$prob), xend=12, yend=min(sample_sites$prob)-0.05), arrow=arrow(length= unit(0.5, &quot;cm&quot;)), linewidth = 1, color = &quot;orange&quot;, linejoin = &quot;mitre&quot;) + geom_segment(aes(x=12, y=0.45+0.003, xend=12, yend=0.45+0.053), arrow=arrow(length= unit(0.5, &quot;cm&quot;)), linewidth = 1, color = &quot;orange&quot;, linejoin = &quot;mitre&quot;) + geom_segment(aes(x=12 - 0.75, y=max(sample_sites$prob) + 0.02, xend=12 - 7, yend=max(sample_sites$prob) + 0.02), arrow=arrow(length= unit(0.5, &quot;cm&quot;)), linewidth = 1, color = &quot;orange&quot;, linejoin = &quot;mitre&quot;) + geom_segment(aes(x=12 + 0.75, y=max(sample_sites$prob) + 0.02, xend=12 + 7, yend=max(sample_sites$prob) + 0.02), arrow=arrow(length= unit(0.5, &quot;cm&quot;)), linewidth = 1, color = &quot;orange&quot;, linejoin = &quot;mitre&quot;) + scale_x_continuous(breaks=seq(0,23,length.out=7), labels=seq(0,24,4))) Figure 3.1: Bimodal probability activity curves showing variability in both the frequency of site-use and variability in timing of activity; displayed are activity curves across sites (dashed lines) along with the marginal mean curve (black line) and conditional mean curve (red line). We see there is considerable variability in the frequency of use among the different sites and relatively smaller variation in timing of activity within the 24-hour cycle. To create Figure 1 in Iannarilli et al. (2024), we bring together this plot and the two produced in the previous Tutorial. ggarrange(pl, pl_sites, widths = c(66, 34)) %&gt;% ggsave(filename = &quot;ms_figures/Fig1_Site_Variability_in_Activity_bimodal.jpg&quot;, device = &quot;jpeg&quot;, units = &quot;cm&quot;, width = 24, height = 10, dpi = 600 ) 3.2 Data preparation The first step to analyze the simulated activity patterns is to arrange the simulated encounter data in a specific format. These data are stored in the object sim_data in the list created by the sim_activity function. This object has a number of rows equal to the number of simulated sites (100) and a number of columns equal to 24 (hours) times the length (in days) of the simulated sampling period, which we set to 30 days. This corresponds to 720 columns (\\(24 \\text{hours} \\times 30 \\text{days}\\)), that is, 720 hourly encounter occasions. We access the encounter data and store them in an object called y. We add a column that specifies a unique identifier for each site (e.g., id = A1) and assign a name to each of the columns to represent the consecutive time occasions (e.g., from 1 to 720). Then, we reorganize the data from the wide to the long format using the id column as the reference and convert the consecutive time occasions in the relative time of day (e.g., time = 35 is converted to Hour = 11 of the second day of sampling). # The data y &lt;- as.data.frame(dat$sim_data) dim(y) ## [1] 100 720 # summarize aggregated data y &lt;- data.frame(id=paste(&quot;A&quot;, seq(1,M,1), sep=&quot;&quot;), y=as.data.frame(y) ) colnames(y) &lt;- c(&quot;id&quot;, paste0(&quot;S&quot;, seq(1, ncol(y)-1, 1), sep=&quot;&quot;) ) # Format data in long format y_long &lt;- pivot_longer(data = y, cols = -id, names_to = &quot;time&quot;, values_to = &quot;y&quot;) %&gt;% mutate(time=as.numeric(substr(time,2,10)), id = factor(id) ) # create variable to describe time as hour (between 0 and 23) y_long$hour &lt;- sim_to_minute(y_long$time, group = wavelength) # show first few rows of the dataset created knitr::kable(head(y_long)) id time y hour A1 1 0 1 A1 2 0 2 A1 3 0 3 A1 4 0 4 A1 5 0 5 A1 6 0 6 We obtain a dataset in which each row specifies the outcome (i.e. encounter or non-encounter of the target species) at a certain site during a specific hour of a certain day of sampling. for our analysis, we can use this dataset as it is, considering y as our response variable. Alternatively, we can reduce model fitting computational time by counting the number of hourly-long occasions in which the species was observed (i.e. successes) and the number of occasions in which the species was not observed (i.e. failures) for each combination of hourly time interval (e.g., time = 1) and camera site (e.g. id = A1). # count successes and failures at each site-occasion occasions_cbind &lt;- y_long %&gt;% group_by(id, hour) %&gt;% summarise(success = sum(y), n_events = n(), failure = n_events - success) %&gt;% dplyr::rename(Site = id, Time = hour) ## `summarise()` has grouped output by &#39;id&#39;. You can override using the `.groups` argument. # show first few rows of the dataset created knitr::kable(head(occasions_cbind)) Site Time success n_events failure A1 0 0 30 30 A1 1 1 30 29 A1 2 0 30 30 A1 3 1 30 29 A1 4 1 30 29 A1 5 0 30 30 In this new version of the dataset, each row specifies how many times a species was (i.e. success: 0 days) and was not (i.e. failure: 30 days) detected at a certain site (Site = A1) during a certain time interval (Time = 0, that is from 00:00 to 00:59). We can now use this dataframe to estimate the diel activity patterns using hierarchical models. 3.3 Trigonometric GLMMs Equation 1 in Tutorial 2 is non-linear due the presence of the phaseshift parameters. Currently, we are unaware of any out-of-the-box options in the R programming language for fitting trigonometric non-linear mixed models using a frequentist approach. To overcome this challenge, we rewrite equation 1 using compound angle formulas (see also main text Iannarilli et al. (2024)): \\[ \\text{logit}(p_t) = \\beta_0 + \\alpha_1 \\times \\text{cos}(\\frac{2\\pi t}{\\omega_1}) + \\alpha_2 \\times \\text{sin}(\\frac{2\\pi t}{\\omega_1}) + \\alpha_3 \\times \\text{cos}(\\frac{2\\pi t}{\\omega_2}) + \\alpha_4 \\times \\text{sin}(\\frac{2\\pi t}{\\omega_2}) + \\tau_i \\] where \\(\\alpha_1 = \\beta_1 \\times \\text{cos}(\\theta_0 + \\gamma_i)\\), \\(\\alpha_2= -\\beta_1 \\times \\text{sin}(\\theta_0 + \\gamma_i)\\), \\(\\alpha_3= \\beta_2 \\times \\text{cos}(\\theta_1 + \\gamma_i)\\), and \\(\\alpha_4= -\\beta_2 \\times \\text{sin}(\\theta_1 + \\gamma_i)\\). In this version (labeled as equation 2 in Iannarilli et al. (2024)), we can model activity patterns using any R package available for fitting GLMMs; we list some of these options in Tables 1 and 2 in Iannarilli et al. (2024). Here, we choose the GLMMadaptive library (Rizopoulos 2022) because it facilitates estimation of both conditional and marginal mean activity patterns (see Tutorial 8). The simplest trigonometric GLMM we can run is one that includes only a random intercept. In the context of estimating activity patterns, a random intercept-only model accounts for variability in the frequency of site-use (i.e. vertical shifts among the curves), but not variability in the timing of activity (i.e. horizontal shift). # run model trig_rand_int &lt;- mixed_model(fixed = cbind(success, failure) ~ cos(2 * pi * Time/24) + sin(2 * pi * Time/24) + cos(2 * pi * Time/12) + sin(2 * pi * Time/12), random = ~ 1 | Site, data = occasions_cbind, family = binomial(), iter_EM = 0 ) summary(trig_rand_int) ## ## Call: ## mixed_model(fixed = cbind(success, failure) ~ cos(2 * pi * Time/24) + ## sin(2 * pi * Time/24) + cos(2 * pi * Time/12) + sin(2 * pi * ## Time/12), random = ~1 | Site, data = occasions_cbind, family = binomial(), ## iter_EM = 0) ## ## Data Descriptives: ## Number of Observations: 2400 ## Number of Groups: 100 ## ## Model: ## family: binomial ## link: logit ## ## Fit statistics: ## log.Lik AIC BIC ## -3742.866 7497.732 7513.363 ## ## Random effects covariance matrix: ## StdDev ## (Intercept) 0.9725664 ## ## Fixed effects: ## Estimate Std.Err z-value p-value ## (Intercept) -3.0670 0.0998 -30.7257 &lt; 1e-04 ## cos(2 * pi * Time/24) -0.9339 0.0263 -35.5111 &lt; 1e-04 ## sin(2 * pi * Time/24) -0.1321 0.0220 -6.0164 &lt; 1e-04 ## cos(2 * pi * Time/12) -0.2802 0.0225 -12.4511 &lt; 1e-04 ## sin(2 * pi * Time/12) -0.5891 0.0229 -25.7027 &lt; 1e-04 ## ## Integration: ## method: adaptive Gauss-Hermite quadrature rule ## quadrature points: 11 ## ## Optimization: ## method: quasi-Newton ## converged: TRUE For trigonometric models, it is difficult to interpret the coefficients. An easier and effective way to explore these results is to visually check the activity patterns predicted based on the model itself. Thus, we predict the estimated (conditional) activity pattern, backtrasform the results from logit to the probability scale, and plot the estimated activity pattern along with the true simulated (conditional mean) activity pattern. # estimated activity pattern newdat &lt;- with(occasions_cbind, expand.grid(Time = seq(min(Time), 24, length.out = 48))) cond_eff0 &lt;- effectPlotData(trig_rand_int, newdat, marginal = FALSE) %&gt;% mutate(pred = plogis(pred), low = plogis(low), upp = plogis(upp), Mod = &quot;Estimated: Random Intercept-only&quot;) # simulated conditional activity pattern cond_true &lt;- dat$Conditional %&gt;% mutate(low = NA, upp = NA, Mod = &quot;Simulated Conditional&quot;) %&gt;% dplyr::rename(Time = time, pred = p) %&gt;% select(Time, pred, low, upp, Mod) # combine the two for visualization purposes cond_eff &lt;- rbind(cond_true, cond_eff0) # plot (pl_trig1 &lt;- ggplot(cond_eff, aes(Time, pred)) + geom_ribbon(aes(ymin = low, ymax = upp, color = Mod, fill = Mod), alpha = 0.3, linewidth = 0.25) + geom_line(aes(color = Mod), linewidth = 1) + # scale_color_manual(values = c(&quot;blue&quot;, &quot;red&quot;)) + scale_fill_manual(values = c(&quot;blue&quot;, &quot;red&quot;)) + labs(x = &quot;Time of Day (Hour)&quot;, y = &quot;Predicted Activity Pattern \\n (probability)&quot;, title = &quot;Estimated vs Simulated Activity Patterns&quot;)+ theme_minimal()+ theme(legend.position = &quot;bottom&quot;, legend.title = element_blank(), legend.text = element_text(size=10,face=&quot;bold&quot;), legend.margin=margin(0,0,0,0), legend.box.margin=margin(-5,-10,-10,-10), plot.title = element_text(size=10,face=&quot;bold&quot;), axis.line = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.ticks = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.title = element_text(size=9,face=&quot;bold&quot;), panel.grid.minor.y = element_blank(), panel.grid.major.y = element_blank(), panel.grid.major.x = element_line(colour = &#39;lightgrey&#39;, linetype = &#39;dashed&#39;, linewidth=0.5), panel.grid.minor.x = element_blank(), strip.text = element_text(size = 9, colour = &quot;black&quot;, face = &quot;bold&quot;, hjust = 0), plot.margin = margin(0.1,0.1,0.5,0.1, &quot;cm&quot;))+ scale_x_continuous(breaks=seq(0,23,length.out=7), labels=seq(0,24,4))) ## Warning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning -Inf Figure 3.2: Comparison of simulated and estimated probability of activity conditional mean curves with variability in the frequency of site-use from a trigonometric hierarchical model; shading corresponds to 95% confidence intervals. The estimated activity pattern from the random intercept-only trigonometric GLMM (in blue) closely resembles the simulated conditional probability (in red) even though we ignored site-to-site variability in the timing of activity. To allow site-to-site variability in the timing of activity, we can add random slopes to the model. The choice of the structure of the random slope effect is not trivial; we can include only parameters linked to the first cosine term in equation 1 (i.e. first two terms in equation 2), only terms for the second cosine term in equation 2 (i.e. third and forth terms in equation 2) or both. When possible, based on computational time and the information contained in the data, we recommend to include all the terms and allow for the random intercept to vary independently of the random slope1. In GLMMadaptive, we ensure this independence by using the syntax || when specifying the random effect component of the model. In Table 2 in Iannarilli et al. (2024), we provide guidance on the code syntax needed to run the equivalent model and the other models presented throughout the tutorial with other R packages. # run model trig_rand_slope &lt;- mixed_model(fixed = cbind(success, failure) ~ cos(2 * pi * Time/24) + sin(2 * pi * Time/24) + cos(2 * pi * Time/12) + sin(2 * pi * Time/12), random = ~ cos(2 * pi * Time/24) + sin(2 * pi * Time/24) + cos(2 * pi * Time/12) + sin(2 * pi * Time/12) || Site, data = occasions_cbind, family = binomial(), iter_EM = 0 ) summary(trig_rand_slope) ## ## Call: ## mixed_model(fixed = cbind(success, failure) ~ cos(2 * pi * Time/24) + ## sin(2 * pi * Time/24) + cos(2 * pi * Time/12) + sin(2 * pi * ## Time/12), random = ~cos(2 * pi * Time/24) + sin(2 * pi * ## Time/24) + cos(2 * pi * Time/12) + sin(2 * pi * Time/12) || ## Site, data = occasions_cbind, family = binomial(), iter_EM = 0) ## ## Data Descriptives: ## Number of Observations: 2400 ## Number of Groups: 100 ## ## Model: ## family: binomial ## link: logit ## ## Fit statistics: ## log.Lik AIC BIC ## -3704.187 7428.374 7454.425 ## ## Random effects covariance matrix: ## StdDev ## (Intercept) 0.9742 ## cos(2 * pi * Time/24) 0.0307 ## sin(2 * pi * Time/24) 0.2343 ## cos(2 * pi * Time/12) 0.1913 ## sin(2 * pi * Time/12) 0.0430 ## ## Fixed effects: ## Estimate Std.Err z-value p-value ## (Intercept) -3.0888 0.1000 -30.8945 &lt; 1e-04 ## cos(2 * pi * Time/24) -0.9410 0.0268 -35.1529 &lt; 1e-04 ## sin(2 * pi * Time/24) -0.1584 0.0345 -4.5927 &lt; 1e-04 ## cos(2 * pi * Time/12) -0.2659 0.0315 -8.4323 &lt; 1e-04 ## sin(2 * pi * Time/12) -0.5988 0.0237 -25.2223 &lt; 1e-04 ## ## Integration: ## method: adaptive Gauss-Hermite quadrature rule ## quadrature points: 7 ## ## Optimization: ## method: quasi-Newton ## converged: TRUE As before, we visually inspect the estimated conditional mean activity pattern based on this model and compare it with the simulated conditional mean activity pattern. # estimated activity pattern newdat &lt;- with(occasions_cbind, expand.grid(Time = seq(min(Time), 24, length.out = 48))) cond_eff1 &lt;- effectPlotData(trig_rand_slope, newdat, marginal = FALSE) %&gt;% mutate(pred = plogis(pred), low = plogis(low), upp = plogis(upp), Mod = &quot;Estimated: Random Intercept and Slope&quot;) # simulated conditional activity pattern cond_true &lt;- dat$Conditional %&gt;% mutate(low = NA, upp = NA, Mod = &quot;Simulated Conditional&quot;) %&gt;% dplyr::rename(Time = time, pred = p) %&gt;% select(Time, pred, low, upp, Mod) # combine the two for visualization purposes cond_eff &lt;- rbind(cond_true, cond_eff1) # plot (pl_trig2 &lt;- ggplot(cond_eff, aes(Time, pred)) + geom_ribbon(aes(ymin = low, ymax = upp, color = Mod, fill = Mod), alpha = 0.3, linewidth = 0.25) + # geom_line(aes(color = Mod), linewidth = 1) + # scale_color_manual(values = c(&quot;blue&quot;, &quot;red&quot;)) + scale_fill_manual(values = c(&quot;blue&quot;, &quot;red&quot;)) + labs(x = &quot;Time of Day (Hour)&quot;, y = &quot;Predicted Activity Pattern \\n (probability)&quot;, title = &quot;Estimated vs Simulated Activity Patterns&quot; )+ theme_minimal()+ theme(legend.position = &quot;bottom&quot;, legend.title = element_blank(), legend.text = element_text(size=10,face=&quot;bold&quot;), legend.margin=margin(0,0,0,0), legend.box.margin=margin(-5,-10,-10,-10), plot.title = element_text(size=10,face=&quot;bold&quot;), axis.line = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.ticks = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.title = element_text(size=9,face=&quot;bold&quot;), panel.grid.minor.y = element_blank(), panel.grid.major.y = element_blank(), panel.grid.major.x = element_line(colour = &#39;lightgrey&#39;, linetype = &#39;dashed&#39;, linewidth=0.5), panel.grid.minor.x = element_blank(), strip.text = element_text(size = 9, colour = &quot;black&quot;, face = &quot;bold&quot;, hjust = 0), plot.margin = margin(0.1,0.1,0.5,0.1, &quot;cm&quot;) )+ scale_x_continuous(breaks=seq(0,23,length.out=7), labels=seq(0,24,4)) ) ## Warning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning -Inf Figure 3.3: Comparison of simulated and estimated probability of activity conditional mean curve with variability in the frequency of site-use and timing of activity from a trigonometric hierarchical model; shading corresponds to 95% confidence intervals. Again, the estimated activity curve closely matches the true activity that was used to generate the data. In this case, adding a random slope improved the estimates only marginally. This will often be the case when there is only a low level of variation in the timing of activity across sites (Iannarilli 2020), as occurs in the data we simulated. In section 6.1, we consider an example in which modelling variability in the timing of activity reveals interesting site-to-site differences in activity patterns. 3.4 Cyclic cubic spline HGAMs In this section, we illustrate how cyclic cubic spline HGAMs can be used to explore activity patterns. HGAMs, of which cyclic cubic spline hierarchical models are a special case, are complex functions that allow analysts to model non-linear relationships. These models are highly flexible, often computationally intensive for large datasets, and can be used to model a wide array of model structures and related hypotheses. This high flexibility can make these models intimidating at first. A complete review of the different proprieties of these models is beyond the scope of this work (but see Pedersen et al. (2019) and N. (2017)). Here, we present two model structures that, in our opinion, are useful when addressing the most common ecological questions related to activity patterns. We fit a cyclic cubic spline hierarchical model using the bam function in package mgcv (N. 2017). We start with a model that resembles a trigonometric random intercept-only model. We use the argument bs=\"re\" in the smoother s(Site, bs=\"re\") to specify Site as a random intercept. We also have a cyclic cubic smoother for Time that accommodates the periodicity in the data. As before, we run the model, predict the estimated activity pattern using the predict.bam function available in the mgcv package (N. 2017), and visually compare it with the simulated conditional mean activity pattern. # run model cycl_rand_int &lt;- bam(cbind(success, failure) ~ s(Time, bs = &quot;cc&quot;, k = 12) + s(Site, bs=&quot;re&quot;), family = &quot;binomial&quot;, data = occasions_cbind, knots = list(Time=c(0,23)) ) summary(cycl_rand_int) ## ## Family: binomial ## Link function: logit ## ## Formula: ## cbind(success, failure) ~ s(Time, bs = &quot;cc&quot;, k = 12) + s(Site, ## bs = &quot;re&quot;) ## ## Parametric coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.04745 0.09894 -30.8 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df Chi.sq p-value ## s(Time) 9.066 10 3094 &lt;2e-16 *** ## s(Site) 95.262 99 3687 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.76 Deviance explained = 70.6% ## fREML = 3722 Scale est. = 1 n = 2400 # build estimated activity curves newdat &lt;- with(occasions_cbind, expand.grid(Time = seq(min(Time), max(Time), 1), Site = &quot;A1&quot;) #Site doesn&#39;t matter ) temp &lt;- predict.bam(cycl_rand_int, newdata = newdat, exclude = &quot;s(Site)&quot;, se.fit = TRUE, type = &quot;response&quot;) cycl_pred &lt;- newdat %&gt;% mutate(pred = temp$fit, low = pred - 1.96*temp$se.fit, upp = pred + 1.96*temp$se.fit, Mod = &quot;Estimated: Random Intercept-only&quot;) %&gt;% select(-Site) # combine true and estimated curves for visualization purposes cond_eff &lt;- rbind(cond_true, cycl_pred) # plot (pl_cycl1 &lt;- ggplot(cond_eff, aes(Time, pred)) + geom_ribbon(aes(ymin = low, ymax = upp, color = Mod, fill = Mod), alpha = 0.3, linewidth = 0.25) + geom_line(aes(color = Mod), linewidth = 1) + scale_color_manual(values = c(&quot;blue&quot;, &quot;red&quot;)) + scale_fill_manual(values = c(&quot;blue&quot;, &quot;red&quot;)) + labs(x = &quot;Time of Day (Hour)&quot;, y = &quot;Predicted Activity Pattern \\n (probability)&quot;, title = &quot;Estimated vs Simulated Activity Patterns&quot; )+ theme_minimal()+ theme(legend.position = &quot;bottom&quot;, legend.title = element_blank(), legend.text = element_text(size=10,face=&quot;bold&quot;), legend.margin=margin(0,0,0,0), legend.box.margin=margin(-5,-10,-10,-10), plot.title = element_text(size=10,face=&quot;bold&quot;), axis.line = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.ticks = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.title = element_text(size=9,face=&quot;bold&quot;), panel.grid.minor.y = element_blank(), panel.grid.major.y = element_blank(), panel.grid.major.x = element_line(colour = &#39;lightgrey&#39;, linetype = &#39;dashed&#39;, linewidth=0.5), panel.grid.minor.x = element_blank(), strip.text = element_text(size = 9, colour = &quot;black&quot;, face = &quot;bold&quot;, hjust = 0), plot.margin = margin(0.1,0.1,0.5,0.1, &quot;cm&quot;) )+ scale_x_continuous(breaks=seq(0,23,length.out=7), labels=seq(0,24,4)) ) ## Warning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning -Inf Figure 3.4: Comparison of simulated and HGAM estimated probability of activity conditional mean curves with variability in the frequency of site-use; shading corresponds to 95% confidence intervals. This model corresponds, in general terms, to the trigonometric random intercept-only model we ran earlier. Thus, it only accommodates variability in the frequency of site-use and not in the timing of activity. The model again captures the general shape of the simulated conditional mean activity curve. We can add complexity and include a smoother that shrinks the site-specific estimates toward a common smooth and one that allows curves to vary among sites. This model structure resembles the random slope component in trigonometric random intercept and random slope models and allows the site-specific estimates to vary also in the timing of activity. In mgcv, there is no need to explicitly force the independence between the random intercept and the random slope (as done before for the trigonometric hierarchical models) because there is no covariance term between two random effects. # Fit model with general smoother for Time cycl_rand_slope &lt;- bam(cbind(success, failure) ~ s(Time, bs = &quot;cc&quot;, k = 12) + # general smoother s(Time, bs = &quot;cc&quot;, k = 12, by = Site, m = 1) + s(Site, bs=&quot;re&quot;), knots = list(Time=c(0,23)), family = &quot;binomial&quot;, data = occasions_cbind ) summary(cycl_rand_slope) ## ## Family: binomial ## Link function: logit ## ## Formula: ## cbind(success, failure) ~ s(Time, bs = &quot;cc&quot;, k = 12) + s(Time, ## bs = &quot;cc&quot;, k = 12, by = Site, m = 1) + s(Site, bs = &quot;re&quot;) ## ## Parametric coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.05918 0.09874 -30.98 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df Chi.sq p-value ## s(Time) 9.061e+00 10 2583.131 &lt;2e-16 *** ## s(Time):SiteA1 4.371e-01 10 0.562 0.3830 ## s(Time):SiteA10 2.003e-06 10 0.000 0.7101 ## s(Time):SiteA100 1.420e+00 10 3.306 0.4797 ## s(Time):SiteA11 1.797e-06 10 0.000 0.5936 ## s(Time):SiteA12 1.832e-06 10 0.000 0.7132 ## s(Time):SiteA13 1.639e+00 10 9.885 0.2819 ## s(Time):SiteA14 1.527e-06 10 0.000 0.5203 ## s(Time):SiteA15 2.331e+00 10 42.828 0.0452 * ## s(Time):SiteA16 2.205e-06 10 0.000 0.5133 ## s(Time):SiteA17 8.433e-01 10 3.353 0.1922 ## s(Time):SiteA18 2.039e-06 10 0.000 0.7737 ## s(Time):SiteA19 3.553e+00 10 62.047 0.0412 * ## s(Time):SiteA2 3.703e-06 10 0.000 0.5109 ## s(Time):SiteA20 3.998e+00 10 31.694 0.2002 ## s(Time):SiteA21 4.114e+00 10 46.728 0.0393 * ## s(Time):SiteA22 1.198e-06 10 0.000 0.7510 ## s(Time):SiteA23 7.953e-06 10 0.000 0.4636 ## s(Time):SiteA24 9.795e-01 10 1.770 0.4597 ## s(Time):SiteA25 3.840e-06 10 0.000 0.4801 ## s(Time):SiteA26 2.197e+00 10 111.557 0.0144 * ## s(Time):SiteA27 9.035e-07 10 0.000 0.9599 ## s(Time):SiteA28 2.784e-06 10 0.000 0.5393 ## s(Time):SiteA29 3.745e+00 10 33.511 0.1217 ## s(Time):SiteA3 5.584e-06 10 0.000 0.3965 ## s(Time):SiteA30 9.305e-07 10 0.000 0.9744 ## s(Time):SiteA31 2.444e-06 10 0.000 0.5635 ## s(Time):SiteA32 4.038e-02 10 0.045 0.3373 ## s(Time):SiteA33 2.085e-06 10 0.000 0.7380 ## s(Time):SiteA34 2.818e-06 10 0.000 0.5247 ## s(Time):SiteA35 1.054e-06 10 0.000 0.9977 ## s(Time):SiteA36 2.531e-01 10 0.357 0.3264 ## s(Time):SiteA37 4.911e-06 10 0.000 0.4435 ## s(Time):SiteA38 1.429e+00 10 5.344 0.3602 ## s(Time):SiteA39 1.353e-06 10 0.000 0.8886 ## s(Time):SiteA4 1.213e-06 10 0.000 0.8443 ## s(Time):SiteA40 3.242e-01 10 0.513 0.2930 ## s(Time):SiteA41 2.556e-01 10 0.415 0.2784 ## s(Time):SiteA42 1.115e+00 10 2.887 0.3039 ## s(Time):SiteA43 2.661e-06 10 0.000 0.5941 ## s(Time):SiteA44 1.061e-06 10 0.000 0.5674 ## s(Time):SiteA45 9.063e-03 10 0.009 0.3550 ## s(Time):SiteA46 9.732e-01 10 4.958 0.1412 ## s(Time):SiteA47 2.472e-02 10 0.027 0.3410 ## s(Time):SiteA48 1.970e-06 10 0.000 0.6506 ## s(Time):SiteA49 1.070e-06 10 0.000 0.9658 ## s(Time):SiteA5 8.967e-07 10 0.000 0.7955 ## s(Time):SiteA50 1.559e+00 10 5.293 0.0700 . ## s(Time):SiteA51 3.052e-06 10 0.000 0.5682 ## s(Time):SiteA52 7.708e-07 10 0.000 0.8218 ## s(Time):SiteA53 1.784e+00 10 19.804 0.1444 ## s(Time):SiteA54 8.030e-06 10 0.000 0.4270 ## s(Time):SiteA55 1.913e-06 10 0.000 0.5856 ## s(Time):SiteA56 9.627e-01 10 12.752 0.0569 . ## s(Time):SiteA57 9.381e-01 10 6.027 0.1786 ## s(Time):SiteA58 1.025e-06 10 0.000 0.9562 ## s(Time):SiteA59 7.248e-01 10 1.168 0.5292 ## s(Time):SiteA6 4.144e+00 10 49.389 0.1140 ## s(Time):SiteA60 1.565e-06 10 0.000 0.8538 ## s(Time):SiteA61 1.103e-06 10 0.000 0.9148 ## s(Time):SiteA62 2.243e-06 10 0.000 0.5242 ## s(Time):SiteA63 1.178e-05 10 0.000 0.4258 ## s(Time):SiteA64 2.117e+00 10 62.976 0.0260 * ## s(Time):SiteA65 2.724e+00 10 37.437 0.0981 . ## s(Time):SiteA66 1.809e+00 10 15.902 0.1948 ## s(Time):SiteA67 1.534e-06 10 0.000 0.7535 ## s(Time):SiteA68 2.834e-06 10 0.000 0.5173 ## s(Time):SiteA69 2.151e-01 10 0.257 0.3358 ## s(Time):SiteA7 4.642e-06 10 0.000 0.4472 ## s(Time):SiteA70 4.617e-01 10 2.428 0.1073 ## s(Time):SiteA71 1.383e-06 10 0.000 0.5298 ## s(Time):SiteA72 9.419e-01 10 2.310 0.2210 ## s(Time):SiteA73 4.981e-06 10 0.000 0.5110 ## s(Time):SiteA74 6.739e-02 10 0.072 0.3742 ## s(Time):SiteA75 1.080e-01 10 0.156 0.3083 ## s(Time):SiteA76 5.645e-01 10 0.855 0.4055 ## s(Time):SiteA77 9.215e-07 10 0.000 0.6584 ## s(Time):SiteA78 1.210e+00 10 3.184 0.3880 ## s(Time):SiteA79 1.114e+00 10 4.934 0.2169 ## s(Time):SiteA8 7.637e-07 10 0.000 0.6840 ## s(Time):SiteA80 2.614e+00 10 11.273 0.3388 ## s(Time):SiteA81 2.436e-03 10 0.002 0.3616 ## s(Time):SiteA82 7.592e-01 10 1.556 0.3970 ## s(Time):SiteA83 1.134e+00 10 13.496 0.0943 . ## s(Time):SiteA84 9.353e-06 10 0.000 0.4141 ## s(Time):SiteA85 2.802e-06 10 0.000 0.4922 ## s(Time):SiteA86 2.003e+00 10 11.026 0.2890 ## s(Time):SiteA87 2.833e-06 10 0.000 0.6427 ## s(Time):SiteA88 2.636e-06 10 0.000 0.8619 ## s(Time):SiteA89 1.315e+00 10 5.537 0.2723 ## s(Time):SiteA9 1.040e+00 10 2.502 0.4632 ## s(Time):SiteA90 1.151e+00 10 20.161 0.0376 * ## s(Time):SiteA91 1.381e-06 10 0.000 0.8268 ## s(Time):SiteA92 1.346e-06 10 0.000 0.9748 ## s(Time):SiteA93 9.053e-07 10 0.000 0.8641 ## s(Time):SiteA94 3.807e-02 10 0.039 0.3685 ## s(Time):SiteA95 6.342e-01 10 2.482 0.1206 ## s(Time):SiteA96 3.957e-06 10 0.000 0.5196 ## s(Time):SiteA97 5.507e-06 10 0.000 0.4437 ## s(Time):SiteA98 1.013e-06 10 0.000 0.7219 ## s(Time):SiteA99 2.401e+00 10 23.757 0.0874 . ## s(Site) 9.506e+01 99 3527.045 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.794 Deviance explained = 73.8% ## fREML = 3656.1 Scale est. = 1 n = 2400 # build the estimated activity patterns newdat &lt;- with(occasions_cbind, expand.grid(Time = seq(min(Time), max(Time), 1), Site = &quot;A1&quot; #Site doesn&#39;t matter ) ) temp &lt;- predict.bam(cycl_rand_slope, newdata = newdat, exclude = &quot;s(Site)&quot;, se.fit = TRUE, type = &quot;response&quot; ) cycl_pred &lt;- newdat %&gt;% mutate(pred = temp$fit, low = pred - 1.96*temp$se.fit, upp = pred + 1.96*temp$se.fit, Mod = &quot;Estimated: Random Slope&quot;) %&gt;% select(-Site) # combine the true and estimated curves for visualization purposes cond_eff &lt;- rbind(cond_true, cycl_pred) # plot (pl_cycl2 &lt;- ggplot(cond_eff, aes(Time, pred)) + geom_ribbon(aes(ymin = low, ymax = upp, color = Mod, fill = Mod), alpha = 0.3, linewidth = 0.25) + # geom_line(aes(color = Mod), linewidth = 1) + # scale_color_manual(values = c(&quot;blue&quot;, &quot;red&quot;)) + scale_fill_manual(values = c(&quot;blue&quot;, &quot;red&quot;)) + labs(x = &quot;Time of Day (Hour)&quot;, y = &quot;Predicted Activity Pattern \\n (probability)&quot;, title = &quot;Estimated vs Simulated Activity Patterns&quot; )+ theme_minimal()+ theme(legend.position = &quot;bottom&quot;, legend.title = element_blank(), legend.text = element_text(size=10,face=&quot;bold&quot;), legend.margin=margin(0,0,0,0), legend.box.margin=margin(-5,-10,-10,-10), plot.title = element_text(size=10,face=&quot;bold&quot;), axis.line = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.ticks = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.title = element_text(size=9,face=&quot;bold&quot;), panel.grid.minor.y = element_blank(), panel.grid.major.y = element_blank(), panel.grid.major.x = element_line(colour = &#39;lightgrey&#39;, linetype = &#39;dashed&#39;, linewidth=0.5), panel.grid.minor.x = element_blank(), strip.text = element_text(size = 9, colour = &quot;black&quot;, face = &quot;bold&quot;, hjust = 0), plot.margin = margin(0.1,0.1,0.5,0.1, &quot;cm&quot;))+ scale_x_continuous(breaks=seq(0,23,length.out=7), labels=seq(0,24,4))) ## Warning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning -Inf Figure 3.5: Comparison of simulated and HGAM estimated probability of activity conditional mean curves with variability in the frequency of site-use and timing of activity; shading corresponds to 95% confidence intervals. Adding a component that allows for variability in timing of activity seems to improve the model fit and provides an estimated activity pattern that more closely resembles the one used to simulated the data. Going forward. Throughout the tutorial, we fit trigonometric GLMMs using a binomial distribution (family = \"binomial\" in the code); however, we can run these models also using a Poisson distribution with a few adjustments to data and model structures. We report an example in section 10.2. When working with large datasets, using a Poisson distribution might result in lower computational times. By allowing these terms to vary independently, we eliminate the need to estimate several covariance terms, which may be difficult with small numbers of sites.↩︎ "],["dataprep.html", "Tutorial 4 Data preparation", " Tutorial 4 Data preparation In the previous Tutorials, we use simulated data to introduce the concept of variability in activity patterns (Tutorial 2) and to illustrate the use of hierarchical models for estimating activity patterns (Tutorial 3). We now switch gears. In this and the following three Tutorials, we fit trigonometric and cyclic cubic spline hierarchical models to empirical camera-trap data to address common ecological questions related to the study of activity patterns. In this Tutorial, we explain how to prepare data to analyze activity patterns from camera-trap datasets using hierarchical models. This procedure is applied throughout the tutorial every time we use a new set of real data. To minimize redundancy, we illustrate the procedure in detail here only once, and then refer to the code (and possibly extend it), when necessary. We start by loading the data and showing a preview of what it looks like. Here, we select camera-trap records of coyotes (Canis latrans). We also load some libraries to wrangle the dataset and get it in the desired format. # Load libraries rm(list = ls()) set.seed(129) library(dplyr) library(lubridate) ## Warning: package &#39;lubridate&#39; was built under R version 4.2.3 # Load data coy &lt;- read.csv(&quot;data_input/species_records.csv&quot;) %&gt;% filter(Species == &quot;Coyote&quot;) %&gt;% droplevels %&gt;% select(-X) %&gt;% mutate(DateTimeOriginal = ymd_hm(DateTimeOriginal)) head(coy) ## Station Species DateTimeOriginal Date Time Session ## 1 10E Coyote 2016-05-20 20:30:00 2016-05-20 20:30:51 Spring2016 ## 2 10E Coyote 2016-05-20 20:30:00 2016-05-20 20:30:51 Spring2016 ## 3 10E Coyote 2016-05-20 20:30:00 2016-05-20 20:30:52 Spring2016 ## 4 10E Coyote 2016-05-20 20:30:00 2016-05-20 20:30:58 Spring2016 ## 5 10E Coyote 2016-05-20 20:30:00 2016-05-20 20:30:59 Spring2016 ## 6 10E Coyote 2016-05-20 20:30:00 2016-05-20 20:30:59 Spring2016 To estimate activity patterns while accounting for site-to-site variability, at a minimum, we need information on the location id (or geographic coordinates) of the spatial sampling site (e.g., camera-trap site) and the time of day in which the different encounter events for a certain species occurred. In this dataset, each row corresponds to one image. We are interested in the location id and the time of day in which each event has been recorded. These values are stored in the columns named Station and Time, respectively. Because we set up our cameras to collect a burst of three pictures every time a camera was triggered, some images have the exact same time and location (see for example the first and second rows above). Individual or groups of animals often spend several minutes in front of a camera, triggering the sensor several times in a short period of time. This leads to highly autocorrelated camera-trap encounter events (once a camera has been triggered, it is likely that it will be triggered again in the following few minutes). When using KDEs, the resulting short-term autocorrelation will impact the choice of smoothing parameter. To reduce the level of autocorrelation, camera-trap data are often aggregated based on some time threshold (often between 1 and 60 minutes, commonly 30; Burton et al. (2015) and Iannarilli et al. (2021)); images collected at the same site and within this time threshold are combined into a single unique encounter event. This process, often referred to as data aggregation, is often highly recommended (although not strictly required, see discussion in Peral, Landman, and Kerley (2022)) as a preparatory step before applying KDEs. We compare KDEs with and without data aggregation in Tutorial 9.3. Data aggregation is not necessary when using trigonometric and cyclic cubic spline hierarchical models. Thus, we use the raw (non-aggregated) dataset in all the empirical case studies presented in these tutorials, except Tutorial 9.4, where we apply data aggregation to facilitate comparison of estimated activity patterns using hierarchical models and KDEs. Unlike KDEs, trigonometric and cyclic hierarchical models allow us to leverage not only the times an encounter was recorded but also the times in which cameras were active at a location and no encounter occurred. This, in turn, allows us to estimate activity while accounting for variable sampling effort and makes comparisons of relative probabilities of activity across sites (or e.g., environmental and anthropogenic conditions) possible. To account for sampling effort, we have to build a matrix that stores information about when a camera was active at each site, and in how many of these days (or shorter occasions, see later) we recorded the target species. We load a new dataframe that contains information on the start and the end of the sampling period at each site. The start of this period usually corresponds to the deployment date, while the end of the period is the date of retrieval or the last day the camera was working at each location, if failure or malfunctioning occurred (e.g., empty batteries, SD card full, or cameras displaced by an animal). cov &lt;- read.csv(&quot;data_input/CameraTrapProject_CT_data_for_analysis_MASTER.csv&quot;, as.is = TRUE) %&gt;% select(Session, Site, Date_setup, Date_retr, Problem1_from, Problem1_to) %&gt;% mutate(Date_setup = mdy(Date_setup), Date_retr = mdy(Date_retr), Problem1_from = mdy(Problem1_from), Problem1_to = mdy(Problem1_to)) head(cov) ## Session Site Date_setup Date_retr Problem1_from Problem1_to ## 1 Spring2016 1A 2016-05-12 2016-06-22 &lt;NA&gt; &lt;NA&gt; ## 2 Spring2016 1B 2016-05-03 2016-06-22 &lt;NA&gt; &lt;NA&gt; ## 3 Spring2016 1C 2016-05-03 2016-07-08 &lt;NA&gt; &lt;NA&gt; ## 4 Spring2016 1D 2016-05-03 2016-06-24 &lt;NA&gt; &lt;NA&gt; ## 5 Spring2016 1E 2016-05-03 2016-06-22 &lt;NA&gt; &lt;NA&gt; ## 6 Spring2016 2A 2016-05-12 2016-06-23 &lt;NA&gt; &lt;NA&gt; This dataframe contains identifiers for the sampling session (Session), the camera-trap site (Site), the date a camera was deployed (Date_setup) and retrieved (Date_retr) from the corresponding site. For cameras that were not functioning at the time of retrieval, we also have information regarding the period the camera was inactive (Problem1_from and Problem1_to). This structure follows the one accepted in the camtrapR package (Niedballa et al. 2016). In this format, the information about the last day a camera was active is spread across two columns, Date_retr (for cameras that did not fail) and Problem1_from (for cameras that did fail). We bring this information together in one column called end. # Merge time of deployment and retrieval + problems site &lt;- cov site$end &lt;- ymd(&quot;2000-01-01&quot;) for(i in 1:nrow(site)){ site$end[i] &lt;- min(site$Date_retr[i], site$Problem1_from[i], na.rm = TRUE ) } Then, we create a dataframe to store the information about when the target species was and was not encountered at the different locations within each temporal observation window. Here, we use 60 minutes to define the temporal sampling units, but different duration (from seconds to several hours) can be used. Shorter lengths will increase model fitting computation time and could potentially lead to parameter convergence problems if data are too sparse. In the code below, the argument by = '60 min' can be changed to accommodate other observation intervals (e.g., by = '30 min' leads to 30-min intervals). For each site, we create a list of all the hourly occasions from the start to the end of the deployment of the active camera at the site. We then pre-populate the column capt (which will track the encounter/non-encounter information) with zeros. # Create dataframe to store captures # (model does not converge if using 30 minutes) occasions &lt;- vector(&quot;list&quot;, length = nrow(site)) for(i in 1:nrow(site)){ occasions[[i]] &lt;- data.frame(Session = site$Session[i], Site = site$Site[i], start = seq(from = ymd_hms(paste(site$Date_setup[i], &quot;00:00:00&quot;, sep = &quot; &quot;)), to = ymd_hms(paste(site$end[i], &quot;23:59:59&quot;, sep = &quot; &quot;)), by = &#39;60 min&#39; ) ) %&gt;% mutate(end = c(start[2:length(start)], start[length(start)]+minutes(60) ) ) } occasions &lt;- do.call(rbind.data.frame, occasions) occasions$capt &lt;- 0 head(occasions) ## Session Site start end capt ## 1 Spring2016 1A 2016-05-12 00:00:00 2016-05-12 01:00:00 0 ## 2 Spring2016 1A 2016-05-12 01:00:00 2016-05-12 02:00:00 0 ## 3 Spring2016 1A 2016-05-12 02:00:00 2016-05-12 03:00:00 0 ## 4 Spring2016 1A 2016-05-12 03:00:00 2016-05-12 04:00:00 0 ## 5 Spring2016 1A 2016-05-12 04:00:00 2016-05-12 05:00:00 0 ## 6 Spring2016 1A 2016-05-12 05:00:00 2016-05-12 06:00:00 0 For each coyote observation collected during the study, we assign the value 1 to the column capt for the row corresponding to the site and time interval of the encounter event. #&#39; Store captures for(i in 1:nrow(coy)){ occasions[occasions$Session == as.character(coy$Session[i]) &amp; occasions$Site == as.character(coy$Station[i]) &amp; occasions$start &lt;= coy$DateTimeOriginal[i] &amp; occasions$end &gt; coy$DateTimeOriginal[i], &quot;capt&quot;] &lt;- 1 } table(occasions$capt) ## ## 0 1 ## 570925 251 We have 251 and 570 925 1-hour long occasions with and without encounters of coyotes, respectively. Because we chose to use hourly occasions, we can extract the information about the hour of each interval and use it as the Time variable in the models that we will run in the following Tutorials. If a different time length is used (e.g., 1-minute long), this step needs to be adjusted accordingly. We also code Site as a factor variable. # Format data occasions$Time &lt;- hour(occasions$start) occasions$Site &lt;- as.factor(occasions$Site) nrow(occasions) ## [1] 571176 Our dataset, occasions, now contains more than 570,000 rows. Running an hierarchical model on such a large dataset might take a long time or require using a computing cluster. We can speed up model fitting by first summarizing the encounter/non-encounter records across the sampling period, counting the number of successes (i.e. intervals with encounters) and failures (i.e. intervals without encounters) at each site and hourly temporal occasion. We can then run the hierarchical models using the cbind(success, failure) approach (see next Tutorial for examples). However, this approach will not work in cases where we want to model variation in activity as a function of additional temporally-varying covariates (e.g., Julian day). In those situations, we may be forced to work with the occassions data set and its binary encounter records. Below, we provide code for creating a dataset containing the number of successes and failures for each observation window and site. # format data for cbind(success, failure) occasions_cbind &lt;- occasions %&gt;% group_by(Site, Time) %&gt;% summarize(success = sum(capt), failure = n() - success ) ## `summarise()` has grouped output by &#39;Site&#39;. You can override using the `.groups` argument. head(occasions_cbind) ## # A tibble: 6 × 4 ## # Groups: Site [1] ## Site Time success failure ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10A 0 0 259 ## 2 10A 1 0 259 ## 3 10A 2 0 259 ## 4 10A 3 0 259 ## 5 10A 4 0 259 ## 6 10A 5 0 259 The first row indicates that for Site 10A and Time interval 0 (i.e. from 00:00:00 to 00:59:59) we had 259 occasions (i.e. days in this case) without detecting a coyote and 0 occasions where we encountered a coyote. In the next Tutorial (Tutorial 5), we see how to use this dataset to test hypotheses about coyotes’ diel activity patterns. "],["unibim.html", "Tutorial 5 Evaluating hypotheses regarding the shape of activity curves", " Tutorial 5 Evaluating hypotheses regarding the shape of activity curves Diel activity patterns are often described or classified as either: unimodal: the species is mostly active during a restricted portion of the 24-hour cycle, and not (or almost not) active in the remaining time. The activity curve is characterized by only one peak, and it is typical of species that are either diurnal or nocturnal. bimodal: the species is mostly active during two specific times of the 24-hour cycle, often corresponding to sunrise and sunset (i.e. crepuscular species) and inactive (or mostly inactive) during the rest of the time. The activity curve is characterized by having two peaks. cathemeral (non-modal): the species is equally active throughout the 24-hour cycle and the activity curve approximates a flat line. Here, we build and compare trigonometric hierarchical models representing these three different diel activity patterns. To reiterate, we use independent camera-trap records of coyotes (Canis latrans) collected between 2016 and 2018 at 100 locations in Northern Minnesota, USA, (Iannarilli et al. 2021), with the data organized as described in the previous Tutorial 4. # Load libraries set.seed(129) library(dplyr) library(lubridate) library(GLMMadaptive) library(lmtest) ## Warning: package &#39;lmtest&#39; was built under R version 4.2.3 library(ggpubr) head(occasions_cbind) ## # A tibble: 6 × 4 ## # Groups: Site [1] ## Site Time success failure ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10A 0 0 259 ## 2 10A 1 0 259 ## 3 10A 2 0 259 ## 4 10A 3 0 259 ## 5 10A 4 0 259 ## 6 10A 5 0 259 We start by fitting a model that describes a unimodal activity pattern. This structure includes only the first of the two cosine terms in equation 1 (which translates in the first and second terms of equation 2), and a random intercept and a random slope as described in Tutorial 3. # Unimodal unimodal &lt;- mixed_model(fixed = cbind(success, failure) ~ cos(2*pi*Time/24) + sin(2*pi*Time/24), random = ~ cos(2*pi*Time/24) + sin(2*pi*Time/24) || Site, family = binomial(), data = occasions_cbind ) summary(unimodal) ## ## Call: ## mixed_model(fixed = cbind(success, failure) ~ cos(2 * pi * Time/24) + ## sin(2 * pi * Time/24), random = ~cos(2 * pi * Time/24) + ## sin(2 * pi * Time/24) || Site, data = occasions_cbind, family = binomial()) ## ## Data Descriptives: ## Number of Observations: 2400 ## Number of Groups: 100 ## ## Model: ## family: binomial ## link: logit ## ## Fit statistics: ## log.Lik AIC BIC ## -660.5908 1333.182 1348.813 ## ## Random effects covariance matrix: ## StdDev ## (Intercept) 1.4853 ## cos(2 * pi * Time/24) 0.5347 ## sin(2 * pi * Time/24) 0.5315 ## ## Fixed effects: ## Estimate Std.Err z-value p-value ## (Intercept) -9.0125 0.2182 -41.3085 &lt; 1e-04 ## cos(2 * pi * Time/24) 0.5754 0.1334 4.3133 &lt; 1e-04 ## sin(2 * pi * Time/24) 0.2232 0.1306 1.7091 0.08743 ## ## Integration: ## method: adaptive Gauss-Hermite quadrature rule ## quadrature points: 7 ## ## Optimization: ## method: hybrid EM and quasi-Newton ## converged: TRUE To code for a bimodal activity pattern, we extend the model structure above by also including the second cosine term in equation 1 (i.e. third and forth terms in equation 2). # Bimodal bimodal &lt;- mixed_model(fixed = cbind(success, failure) ~ cos(2*pi*Time/24) + sin(2*pi*Time/24)+ cos(2*pi*Time/12) + sin(2*pi*Time/12), random = ~ cos(2*pi*Time/24) + sin(2*pi*Time/24)+ cos(2*pi*Time/12) + sin(2*pi*Time/12) || Site, family = binomial(), data = occasions_cbind ) summary(bimodal) ## ## Call: ## mixed_model(fixed = cbind(success, failure) ~ cos(2 * pi * Time/24) + ## sin(2 * pi * Time/24) + cos(2 * pi * Time/12) + sin(2 * pi * ## Time/12), random = ~cos(2 * pi * Time/24) + sin(2 * pi * ## Time/24) + cos(2 * pi * Time/12) + sin(2 * pi * Time/12) || ## Site, data = occasions_cbind, family = binomial()) ## ## Data Descriptives: ## Number of Observations: 2400 ## Number of Groups: 100 ## ## Model: ## family: binomial ## link: logit ## ## Fit statistics: ## log.Lik AIC BIC ## -654.8336 1329.667 1355.719 ## ## Random effects covariance matrix: ## StdDev ## (Intercept) 1.4885 ## cos(2 * pi * Time/24) 0.6659 ## sin(2 * pi * Time/24) 0.4075 ## cos(2 * pi * Time/12) 0.1727 ## sin(2 * pi * Time/12) 0.1757 ## ## Fixed effects: ## Estimate Std.Err z-value p-value ## (Intercept) -9.0816 0.2228 -40.7584 &lt; 1e-04 ## cos(2 * pi * Time/24) 0.6865 0.1567 4.3824 &lt; 1e-04 ## sin(2 * pi * Time/24) 0.2897 0.1221 2.3722 0.017683 ## cos(2 * pi * Time/12) -0.2782 0.1195 -2.3277 0.019927 ## sin(2 * pi * Time/12) -0.2451 0.1055 -2.3225 0.020205 ## ## Integration: ## method: adaptive Gauss-Hermite quadrature rule ## quadrature points: 7 ## ## Optimization: ## method: hybrid EM and quasi-Newton ## converged: TRUE Finally, we use a model without any cosine term to describe a cathemeral activity patterns. We include a random intercept but cannot include a random slope due the structure of model itself. null_mod &lt;- mixed_model(fixed = cbind(success, failure) ~ 1, random = ~ 1 | Site, family = binomial(), data = occasions_cbind ) summary(null_mod) ## ## Call: ## mixed_model(fixed = cbind(success, failure) ~ 1, random = ~1 | ## Site, data = occasions_cbind, family = binomial()) ## ## Data Descriptives: ## Number of Observations: 2400 ## Number of Groups: 100 ## ## Model: ## family: binomial ## link: logit ## ## Fit statistics: ## log.Lik AIC BIC ## -697.7788 1399.558 1404.768 ## ## Random effects covariance matrix: ## StdDev ## (Intercept) 1.508908 ## ## Fixed effects: ## Estimate Std.Err z-value p-value ## (Intercept) -8.7938 0.2179 -40.361 &lt; 1e-04 ## ## Integration: ## method: adaptive Gauss-Hermite quadrature rule ## quadrature points: 11 ## ## Optimization: ## method: EM ## converged: TRUE Model selection. We can now proceed to compare the different model structures and associated hypotheses. We can do this using the Akaike Information Criterion (AIC; Burnham and Anderson (2002)). # AIC comparison AIC(null_mod, unimodal, bimodal) ## df AIC ## null_mod 2 1399.558 ## unimodal 6 1333.182 ## bimodal 10 1329.667 We can also compare the models using a Likelihood Ratio Test (LRT). lrtest(null_mod, unimodal, bimodal) ## Likelihood ratio test ## ## Model 1: cbind(success, failure) ~ 1 ## Model 2: cbind(success, failure) ~ cos(2 * pi * Time/24) + sin(2 * pi * ## Time/24) ## Model 3: cbind(success, failure) ~ cos(2 * pi * Time/24) + sin(2 * pi * ## Time/24) + cos(2 * pi * Time/12) + sin(2 * pi * Time/12) ## #Df LogLik Df Chisq Pr(&gt;Chisq) ## 1 2 -697.78 ## 2 6 -660.59 4 74.376 2.7e-15 *** ## 3 10 -654.83 4 11.514 0.02135 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Both comparisons suggest that the bimodal pattern is the most supported model among those compared. We can, thus, conclude that coyotes have a bimodal activity pattern. We can also visually compare predicted activity curves based on these models. As in other examples, we first use the function GLMMadaptive::effectPlotData to predict the activity curves throughout the 24-hour cycle and then use ggplot to plot the results. # build a new dataset newdat &lt;- with(occasions_cbind, expand.grid(Time = seq(min(Time), max(Time), length.out = 24) ) ) # obtain the estimated activity curves predict_unimodal &lt;- effectPlotData(unimodal, newdat, marginal = FALSE) %&gt;% mutate(Model = &quot;Unimodal&quot;) predict_bimodal &lt;- effectPlotData(bimodal, newdat, marginal = FALSE) %&gt;% mutate(Model = &quot;Bimodal&quot;) predict_cathemeral &lt;- effectPlotData(null_mod, newdat, marginal = FALSE) %&gt;% mutate(Model = &quot;Cathemeral&quot;) # join and plot results pl_shapes &lt;- rbind(predict_unimodal, predict_bimodal, predict_cathemeral) %&gt;% ggplot(., aes(x = Time, y = plogis(pred), group = Model, fill = Model)) + geom_line(aes(colour = Model)) + geom_ribbon(aes(ymin = plogis(low), ymax = plogis(upp), colour = NULL), alpha = 0.3) + labs(x = &quot;Time of Day (Hour)&quot;, y = &quot;Predicted Activity Pattern \\n (probability)&quot;)+ theme_minimal()+ theme(legend.position = &quot;top&quot;, legend.title = element_blank(), legend.text = element_text(size=10,face=&quot;bold&quot;), legend.margin=margin(0,0,0,0), legend.box.margin=margin(-5,-10,-10,-10), plot.title = element_blank(), axis.line = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.ticks = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.title = element_text(size=9,face=&quot;bold&quot;), panel.grid.minor.y = element_blank(), panel.grid.major.y = element_blank(), panel.grid.major.x = element_line(colour = &#39;lightgrey&#39;, linetype = &#39;dashed&#39;, linewidth=0.5), panel.grid.minor.x = element_blank(), strip.text = element_text(size = 9, colour = &quot;black&quot;, face = &quot;bold&quot;, hjust = 0) ) + #panel.grid.minor.x = element_line(colour=&#39;deepskyblue4&#39;, linetype = &#39;dotted&#39;, size=0.35))+ scale_x_continuous(breaks=seq(0,23,length.out=7), labels=seq(0,24,4)) pl_shapes Figure 5.1: Predicted probability of activity by three hypotheses: unimodal, bimodal, and cathemeral; shading represented 95% confidence intervals. # save plot ggsave(plot = pl_shapes, filename = &quot;ms_figures/Fig2_Uni_vs_Bim_comparison.jpg&quot;, device = &quot;jpeg&quot;, units = &quot;cm&quot;, width = 12, height = 10, dpi = 600) We can test other hypotheses related to the shape of the activity curve (e.g., three or more peaks) for our target species by simply adding additional cosine terms to the model structure and then comparing the different structures using AIC or LRT. However, going beyond two peaks might be computationally challenging, especially when a random slope is included. In these cases, one might need to consider reducing model complexity by only including a random intercept. "],["cov.html", "Tutorial 6 Modeling activity patterns with covariates 6.1 Categorical covariates 6.2 Continuous covariates", " Tutorial 6 Modeling activity patterns with covariates Many researchers exploring activity patterns from camera-trap data sooner or later run into the challenge of estimating how activity patterns vary in response to some conditions, often related to the environment in which the data have been collected. We address this common set of ecological questions in this Tutorial, focusing first on categorical and then on continuous variables. 6.1 Categorical covariates The task of comparing activity patterns among different levels of a categorical variable can be achieved using KDE approaches. However, the process can be particularly cumbersome, especially when comparing more than two levels. It requires splitting the dataset of the independent records depending on the levels of the variable of interest, run a KDE for each group of records, and then comparing the results using an ad-hoc test. Conversely, the same task can be accomplished by fitting a hierarchical model to the whole dataset, as we describe here. In Tutorial 9.4, we repeat this same analyses using the KDE approach, and compare results to those obtained using the trigonometric and cyclic cubic spline hierarchical models. Again, we use camera-trap records collected between 2016 and 2018 at 100 locations in Northern Minnesota, USA (Iannarilli et al. 2021). This time, we use records of American black bear (Ursus americanus) and explore differences in activity patterns of this species between spring and fall. # Load Libraries rm(list = ls()) set.seed(129) library(dplyr) library(lubridate) library(GLMMadaptive) library(mgcv) library(ggpubr) library(forcats) # Load data dat &lt;- read.csv(&quot;data_input/species_records.csv&quot;) %&gt;% filter(Species == &quot;BlackBear&quot;) %&gt;% droplevels() %&gt;% mutate(DateTimeOriginal = ymd_hm(DateTimeOriginal)) cov &lt;- read.csv(&quot;data_input/CameraTrapProject_CT_data_for_analysis_MASTER.csv&quot;, as.is = TRUE) %&gt;% mutate(Date_setup = mdy(Date_setup), Date_retr = mdy(Date_retr), Problem1_from = mdy(Problem1_from), Problem1_to = mdy(Problem1_to) ) # Extract Season information from Session column dat$Season &lt;- as.factor(substr(dat$Session, 1, 1)) # Merge time of deployment and retrieval + problems site &lt;- cov site$end &lt;- ymd(&quot;2000-01-01&quot;) for(i in 1:nrow(site)){ site$end[i] &lt;- min(site$Date_retr[i], site$Problem1_from[i], na.rm = TRUE ) } # Create dataframe to store captures occasions &lt;- vector(&quot;list&quot;, length = nrow(site)) for(i in 1:nrow(site)){ occasions[[i]] &lt;- data.frame(Session = site$Session[i], Site = site$Site[i], start = seq(from = ymd_hms(paste(site$Date_setup[i], &quot;00:00:00&quot;, sep = &quot; &quot;)), to = ymd_hms(paste(site$end[i], &quot;23:59:59&quot;, sep = &quot; &quot;)), by = &#39;60 min&#39; ) ) %&gt;% mutate(end = c(start[2:length(start)], start[length(start)]+minutes(60) ) ) } occasions &lt;- do.call(rbind.data.frame, occasions) occasions$capt &lt;- 0 # Store captures for(i in 1:nrow(dat)){ occasions[occasions$Session == as.character(dat$Session[i]) &amp; occasions$Site == as.character(dat$Station[i]) &amp; occasions$start &lt;= dat$DateTimeOriginal[i] &amp; occasions$end &gt; dat$DateTimeOriginal[i], &quot;capt&quot;] &lt;- 1 } # Format data occasions$Time &lt;- hour(occasions$start) occasions$Season &lt;- as.factor(substr(occasions$Session, 1, 1)) table(occasions$Season, occasions$capt) ## ## 0 1 ## F 222236 244 ## S 347923 773 We obtained 773 (out of 348 696) and 244 (out of 222 480) hourly occasions with at least 1 positive record of a bear in the Spring and Fall sessions, respectively. We again format the data using a cbind(success, failure) structure as described in Tutorial 4, but this time we also create an additional column (Season) that contains the information about the season (i.e. Spring or Fall) in which the records were collected and use it as an additional grouping criterion. occasions$Site &lt;- as.factor(occasions$Site) # format data for cbind(success, failure) occasions_cbind &lt;- occasions %&gt;% group_by(Site, Time, Season) %&gt;% summarize(success = sum(capt), failure = n() - success) ## `summarise()` has grouped output by &#39;Site&#39;, &#39;Time&#39;. You can override using the `.groups` argument. head(occasions_cbind) ## # A tibble: 6 × 5 ## # Groups: Site, Time [3] ## Site Time Season success failure ## &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10A 0 F 0 100 ## 2 10A 0 S 0 159 ## 3 10A 1 F 0 100 ## 4 10A 1 S 0 159 ## 5 10A 2 F 0 100 ## 6 10A 2 S 0 159 We save this dataset for applications in other parts of the tutorial. write.csv(occasions_cbind, file = &quot;data_output/occasions_cbind_Ursus_americanus_seasons.csv&quot;) 6.1.1 Trigonometric GLMMs Extending the trigonometric hierarchical models to explore the effect of a covariate on activity patterns only requires the inclusion of interactions between the trigonometric terms and the covariate. The interactions allow the coefficients of the trigonometric terms to vary by treatment group, in this case, the two levels of the covariate Season. We start by fitting and visualizing estimated activity curves for the two seasons based on a random intercept-only model in which the random intercept for Site accounts for repeated measures and varying levels of activity at each site: # run model trig_rand_int &lt;- mixed_model(fixed = cbind(success, failure) ~ cos(2 * pi * Time/24) * Season + sin(2 * pi * Time/24) * Season + sin(2 * pi * Time/12) * Season + cos(2 * pi * Time/12) * Season, random = ~ 1 | Site, data = occasions_cbind, family = binomial(), iter_EM = 0 ) summary(trig_rand_int) ## ## Call: ## mixed_model(fixed = cbind(success, failure) ~ cos(2 * pi * Time/24) * ## Season + sin(2 * pi * Time/24) * Season + sin(2 * pi * Time/12) * ## Season + cos(2 * pi * Time/12) * Season, random = ~1 | Site, ## data = occasions_cbind, family = binomial(), iter_EM = 0) ## ## Data Descriptives: ## Number of Observations: 4800 ## Number of Groups: 100 ## ## Model: ## family: binomial ## link: logit ## ## Fit statistics: ## log.Lik AIC BIC ## -2284.197 4590.394 4619.051 ## ## Random effects covariance matrix: ## StdDev ## (Intercept) 0.9022139 ## ## Fixed effects: ## Estimate Std.Err z-value p-value ## (Intercept) -7.3343 0.1216 -60.3133 &lt; 1e-04 ## cos(2 * pi * Time/24) 0.4808 0.1102 4.3629 &lt; 1e-04 ## SeasonS 0.7450 0.0869 8.5698 &lt; 1e-04 ## sin(2 * pi * Time/24) -0.5554 0.0989 -5.6164 &lt; 1e-04 ## sin(2 * pi * Time/12) -0.1697 0.0980 -1.7311 0.0834365 ## cos(2 * pi * Time/12) -0.2358 0.0986 -2.3904 0.0168278 ## cos(2 * pi * Time/24):SeasonS -0.7353 0.1275 -5.7657 &lt; 1e-04 ## SeasonS:sin(2 * pi * Time/24) 0.3460 0.1100 3.1453 0.0016592 ## SeasonS:sin(2 * pi * Time/12) -0.4507 0.1131 -3.9863 &lt; 1e-04 ## SeasonS:cos(2 * pi * Time/12) -0.3739 0.1135 -3.2945 0.0009859 ## ## Integration: ## method: adaptive Gauss-Hermite quadrature rule ## quadrature points: 11 ## ## Optimization: ## method: quasi-Newton ## converged: TRUE # build estimate of activity newdat &lt;- with(occasions_cbind, expand.grid(Time = seq(min(Time), 24, length.out = 48), Season = levels(Season) ) ) pred_rand_int &lt;- effectPlotData(trig_rand_int, newdat, marginal = FALSE) # plot (pl_trig &lt;- ggplot(pred_rand_int, aes(Time, plogis(pred))) + geom_ribbon(aes(ymin = plogis(low), ymax = plogis(upp), color = Season, fill = Season), alpha = 0.3, linewidth = 0.25) + geom_line(aes(color = Season), linewidth = 1) + scale_color_manual(values = c(&quot;orange&quot;, &quot;darkgreen&quot;)) + scale_fill_manual(values = c(&quot;orange&quot;, &quot;darkgreen&quot;)) + coord_cartesian(ylim = c(0, 0.005)) + labs(x = &quot;Time of Day (Hour)&quot;, y = &quot;Predicted Activity Pattern \\n (probability)&quot;, title = &quot;A: Hierarchical model, Trigonometric GLMM \\n(random intercept-only)&quot;)+ theme_minimal()+ theme(legend.position = &quot;bottom&quot;, legend.title = element_blank(), legend.text = element_text(size=10,face=&quot;bold&quot;), legend.margin=margin(0,0,0,0), legend.box.margin=margin(-5,-10,-10,-10), plot.title = element_text(size=10,face=&quot;bold&quot;), axis.line = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.ticks = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.title = element_text(size=9,face=&quot;bold&quot;), panel.grid.minor.y = element_blank(), panel.grid.major.y = element_blank(), panel.grid.major.x = element_line(colour = &#39;lightgrey&#39;, linetype = &#39;dashed&#39;, linewidth=0.5), panel.grid.minor.x = element_blank(), strip.text = element_text(size = 9, colour = &quot;black&quot;, face = &quot;bold&quot;, hjust = 0) ) + scale_x_continuous(breaks=seq(0,23,length.out=7), labels=seq(0,24,4)) ) Figure 6.1: Predicted probability of activity of black bears in the spring (S) and fall (F) using a random intercept-only trigonometric hierarchical model; shading corresponds to 95% confidence intervals. The estimated activity patterns show a striking difference between the two seasons. In Spring, black bears had a bimodal activity pattern, with the species active primarily around around 8:00 and 20:00. During the Fall, the species was mostly inactive during the morning. Importantly, the two seasons were sampled for a different number of days since the study included 3 springs but only 2 falls. This difference in sampling effort makes the direct comparison between the intensity of activity in the two seasons difficult when using KDEs. However, both the trigonometric GLMMs and the cyclic cubic HGAMs (see later) directly account for sampling effort and make this comparison possible. Thus, we can also say that the probability of a black bear being active around 20:00 was overall higher in the Spring than in the Fall. Model selection: To test the importance of the seasonal effect on activity patterns of black bears, we can also compare the model above with a model that does not include the covariate using AIC: # run model trig_rand_int_no_cov &lt;- mixed_model(fixed = cbind(success, failure) ~ cos(2 * pi * Time/24) + sin(2 * pi * Time/24) + sin(2 * pi * Time/12) + cos(2 * pi * Time/12), random = ~ 1 | Site, data = occasions_cbind, family = binomial(), iter_EM = 0 ) AIC(trig_rand_int, trig_rand_int_no_cov) ## df AIC ## trig_rand_int 11 4590.394 ## trig_rand_int_no_cov 6 4774.190 The AIC comparison supports the model that includes a seasonal effect on bears’ activity patterns. We could consider further extending the interaction model by including random slopes. trig_rand_slope &lt;- mixed_model(fixed = cbind(success, failure) ~ cos(2 * pi * Time/24) * Season + sin(2 * pi * Time/24) * Season + sin(2 * pi * Time/12) * Season + cos(2 * pi * Time/12) * Season, random = ~ cos(2 * pi * Time/24) * Season + sin(2 * pi * Time/24) * Season + sin(2 * pi * Time/12) * Season + cos(2 * pi * Time/12) * Season || Site, data = occasions_cbind, family = binomial(), iter_EM = 0 ) summary(trig_rand_slope) However, this model is likely to be too computational demanding for most laptops. In such cases, we can either opt for the simpler model - keeping in mind that we are not allowing the estimates to vary in their timing of activity - or we can run this model on a computing cluster. 6.1.2 Cyclic cubic spline HGAMs Similar to trigonometric GLMMs, cyclic cubic spline HGAMs can also be used to model activity patterns as a function of a categorical covariate. As we explained in Tutorial 3, this class of models is highly flexible and offers a plethora of options when it comes to choosing a model structure. Here we focus on two model specifications. The first resembles a trigonometric random intercept-only GLMM and contains Season as a fixed term, a cubic cyclic smoother for Time which varies by Season, and a smoother for Site as a random effect. This structure accommodates variability in the frequency of site-use, but not in the timing of activity. # &#39;Random intercept-only&#39; mod_cycl1 &lt;- bam(cbind(success, failure) ~ Season + s(Time, bs = &quot;cc&quot;, k = 12, by = Season, m = 1) + s(Site, bs=&quot;re&quot;), knots = list(Time=c(0,23)), family = &quot;binomial&quot;, data = occasions_cbind ) # Predict activity patterns newdat &lt;- with(occasions_cbind, expand.grid(Time = seq(min(Time), max(Time), 1), Season = levels(Season), Site = &quot;7B&quot; #Station doesn&#39;t matter ) ) cycl_pred1 &lt;- predict.bam(mod_cycl1, newdata = newdat, exclude = &quot;s(Site)&quot;, se.fit = TRUE, type = &quot;response&quot; ) cycl_pred1 &lt;- cbind(newdat, fit=cycl_pred1$fit, se.fit=cycl_pred1$se.fit, Model = &quot;Random intercept-only&quot; ) # Plot pl_cycl1 &lt;- ggplot(cycl_pred1, aes(Time, fit)) + geom_ribbon(aes(ymin = fit-1.96*se.fit, ymax = fit+1.96*se.fit, color = Season, fill = Season), alpha = 0.3, linewidth = 0.25) + geom_line(aes(color = Season), linewidth = 1) + scale_color_manual(values = c(&quot;orange&quot;, &quot;darkgreen&quot;)) + scale_fill_manual(values = c(&quot;orange&quot;, &quot;darkgreen&quot;)) + labs(x = &quot;Time of Day (Hour)&quot;, y = &quot;Predicted Activity Pattern \\n (probability)&quot;, title = &quot;B: Hierarchical model, \\nCyclic cubic spline HGAM&quot;)+ coord_cartesian(ylim = c(0, 0.005)) + theme_minimal()+ theme(legend.position = &quot;bottom&quot;, legend.title = element_blank(), legend.text = element_text(size=10,face=&quot;bold&quot;), legend.margin=margin(0,0,0,0), legend.box.margin=margin(-5,-10,-10,-10), plot.title = element_text(size=10,face=&quot;bold&quot;), axis.line = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.ticks = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.title = element_text(size=9,face=&quot;bold&quot;), panel.grid.minor.y = element_blank(), panel.grid.major.y = element_blank(), panel.grid.major.x = element_line(colour = &#39;lightgrey&#39;, linetype = &#39;dashed&#39;, linewidth=0.5), panel.grid.minor.x = element_blank(), strip.text = element_text(size = 9, colour = &quot;black&quot;, face = &quot;bold&quot;, hjust = 0) ) + scale_x_continuous(breaks=seq(0,max(cycl_pred1$Time),length.out=7), labels=seq(0,24,4)) # bringing hierarchical models plot together (ggarrange(pl_trig, pl_cycl1, ncol = 2, common.legend = TRUE, legend = &quot;bottom&quot;, widths = c(0.5, 0.5)) + theme(plot.margin = margin(0.1,0.1,0.5,0.1, &quot;cm&quot;))) Figure 6.2: Predicted probability of activity of black bears in the spring (S) and fall (F) using a random intercept-only trigonometric hierarchical model (A) and a cyclic cubic spline hierarhical model (B); shading corresponds to 95% confidence intervals. The patterns estimated by the cyclic cubic spline model closely matches that returned by the random intercept-only trigonometric model, though the curve is slightly more irregular. Model selection: As done for the trigonometric models, we can use AIC to test the importance of the variable Season by comparing models with and without terms involving the covariate of interest. # Without general smoother for Time, no covariate effect mod_cycl1_no_cov &lt;- bam(cbind(success, failure) ~ s(Site, bs=&quot;re&quot;), knots = list(Time=c(0,23)), family = &quot;binomial&quot;, data = occasions_cbind ) AIC(mod_cycl1, mod_cycl1_no_cov) ## df AIC ## mod_cycl1 98.46857 4441.474 ## mod_cycl1_no_cov 83.49616 4936.451 AIC(mod_cycl1_no_cov) - AIC(mod_cycl1) ## [1] 494.977 Importantly, (generalized) LRTs should be viewed with some caution. For example, Pedersen et al. (2019) suggest there is insufficient theory to support their use, and N. (2017) note that p-values tend to be too small (usually half of what they should be). Thus, whenever possible, we recommend careful a priori choice of modeling structures based on the desired inference and the characteristics of the data. We can extend the previous model structure to accommodate site-to-site variability in timing of activity by adding a smoother for Time that depends on Site. We also add a global smoother for Time that shrinks the site-specific estimates towards a common curve. This structure mimics a trigonometric random intercept and random slope model and allows the site-specific activity curves to vary not only in their intercept but also in their shape. # &#39;Random intercept and slope&#39; mod_cycl2 &lt;- bam(cbind(success, failure) ~ Season + s(Time, bs = &quot;cc&quot;, k = 12) + # global smoother s(Time, bs = &quot;cc&quot;, k = 12, by = Site, m = 1) + s(Time, bs = &quot;cc&quot;, k = 12, by = Season, m = 1) + s(Site, bs=&quot;re&quot;), knots = list(Time=c(0,23)), family = &quot;binomial&quot;, data = occasions_cbind ) #summary(mod_cycl2) AIC(mod_cycl1) - AIC(mod_cycl2) ## [1] 8.390475 This model is more supported than the previous one. Plotting some of the estimated site-specific curves shows how the activity patterns can vary greatly among sites. # Predict site-specific activity patterns newdat &lt;- with(occasions_cbind, expand.grid(Time = seq(min(Time), max(Time), 1), Season = levels(Season), Site = unique(occasions_cbind$Site) ) ) cycl_pred1 &lt;- predict.bam(mod_cycl1, newdata = newdat, exclude = NULL, se.fit = TRUE, type = &quot;response&quot; ) cycl_pred1 &lt;- cbind(newdat, fit=cycl_pred1$fit, se.fit=cycl_pred1$se.fit, Model = &quot;Random intercept-only&quot; ) cycl_pred2 &lt;- predict.bam(mod_cycl2, newdata = newdat, exclude = NULL, se.fit = TRUE, type = &quot;response&quot; ) cycl_pred2 &lt;- cbind(newdat, fit=cycl_pred2$fit, se.fit=cycl_pred2$se.fit, Model = &quot;Random intercept and slope&quot; ) cycl_pred &lt;- rbind(cycl_pred1, cycl_pred2) selected_sites &lt;- c(&quot;2A&quot;, &quot;2B&quot;, &quot;3B&quot;, &quot;4D&quot;,&quot;7B&quot;, &quot;14B&quot;, &quot;14C&quot;, &quot;15A&quot;, &quot;15E&quot;, &quot;18C&quot;) pl_cycl &lt;- cycl_pred %&gt;% filter(Site %in% selected_sites) %&gt;% mutate(Season2 = ifelse(Season == &quot;F&quot;, &quot;Fall&quot;, &quot;Spring&quot;)) %&gt;% ggplot(., aes(Time, fit, group = Site)) + geom_line(aes(color = Site, group = Site), size = 1) + labs(x = &quot;Time of Day (Hour)&quot;, y = &quot;Predicted Activity Pattern \\n (probability)&quot;, title = &quot;Cyclic cubic spline HGAMs&quot;)+ theme_minimal()+ theme(legend.position = &quot;bottom&quot;, legend.title = element_blank(), legend.text = element_text(size=10,face=&quot;bold&quot;), legend.margin=margin(0,0,0,0), legend.box.margin=margin(-5,-10,-10,-10), plot.title = element_text(size=10,face=&quot;bold&quot;), axis.line = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.ticks = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.title = element_text(size=9,face=&quot;bold&quot;), panel.grid.minor.y = element_blank(), panel.grid.major.y = element_blank(), panel.grid.major.x = element_line(colour = &#39;lightgrey&#39;, linetype = &#39;dashed&#39;, linewidth=0.5), panel.grid.minor.x = element_blank(), strip.text = element_text(size = 9, colour = &quot;black&quot;, face = &quot;bold&quot;, hjust = 0), plot.margin = margin(0.1,0.1,0.5,0.1, &quot;cm&quot;) ) + scale_x_continuous(breaks=seq(0,max(cycl_pred$Time),length.out=7), labels=seq(0,24,4)) + facet_grid(Season2~Model) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. pl_cycl Figure 6.3: Predicted probability of activity of black bears across a select group of sites using a cyclic cubic spline model with a structured resembling a trigonometric random intercept-only (left column) and random intercept and slope (right column) model. # save plot ggsave(plot = pl_cycl, filename = &quot;ms_figures/Fig4_Site_specific_estimates.jpg&quot;, device = &quot;jpeg&quot;, units = &quot;cm&quot;, width = 24, height = 20, dpi = 600) 6.2 Continuous covariates We use the same data to illustrate how hierarchical models can be used to quantify effects of continuous covariates. Specifically, we explore how bear diel activity changes in response to anthropogenic modifications of the landscape quantified through the Global Human Modification index (GHM; Kennedy and Kiesecker (2020); Kennedy and Kiesecker (2019)), a continuous measure of human modification of terrestrial landscapes that ranges between 0 and 1. A histogram of the values of GHM at the different sites indicates that the level of human-driven landscape modification is relatively low in the study area with most sites having a GHM value lower than 0.10. hist(cov$ghm) Figure 6.4: Frequency of observed global human modification index (GHM) values across camera sites. We link the GHM values available in the cov object to the occasion dataframe created at the beginning of this tutorial, and then proceed with arranging the data in the format required to run the hierarchical model as we have done in other examples. This time though, we also add GHM as an additional grouping level. # Add ghm info occasions &lt;- left_join(occasions, cov %&gt;% select(Session, Site, ghm)) %&gt;% mutate(ghm = round(ghm, digits = 3)) ## Joining with `by = join_by(Session, Site)` head(table(occasions$ghm, occasions$capt)) ## ## 0 1 ## 0.015 9397 35 ## 0.017 15332 28 ## 0.018 7849 47 ## 0.02 5972 4 ## 0.021 4264 8 ## 0.022 13961 31 occasions$Site &lt;- as.factor(occasions$Site) # format data for cbind(success, failure) occasions_cbind &lt;- occasions %&gt;% group_by(Site, Time, ghm) %&gt;% summarize(success = sum(capt), failure = n() - success) ## `summarise()` has grouped output by &#39;Site&#39;, &#39;Time&#39;. You can override using the `.groups` argument. We can now apply either trigonometric GLMMs or HGAMs to the data. As an example, we report a trigonometric random intercept-only model and a cyclic cubic spline HGAM without a site-specific smoother. # random intercept-only trigonometric GLMM mod_trig_cont &lt;- mixed_model(fixed = cbind(success, failure) ~ cos(2 * pi * Time/24) * ghm + sin(2 * pi * Time/24) * ghm + sin(2 * pi * Time/12) * ghm + cos(2 * pi * Time/12) * ghm, random = ~ 1 | Site, data = occasions_cbind, family = binomial(), iter_EM = 0 ) summary(mod_trig_cont) ## ## Call: ## mixed_model(fixed = cbind(success, failure) ~ cos(2 * pi * Time/24) * ## ghm + sin(2 * pi * Time/24) * ghm + sin(2 * pi * Time/12) * ## ghm + cos(2 * pi * Time/12) * ghm, random = ~1 | Site, data = occasions_cbind, ## family = binomial(), iter_EM = 0) ## ## Data Descriptives: ## Number of Observations: 2784 ## Number of Groups: 100 ## ## Model: ## family: binomial ## link: logit ## ## Fit statistics: ## log.Lik AIC BIC ## -1907.898 3837.795 3866.452 ## ## Random effects covariance matrix: ## StdDev ## (Intercept) 0.8716829 ## ## Fixed effects: ## Estimate Std.Err z-value p-value ## (Intercept) -6.5825 0.1437 -45.8218 &lt; 1e-04 ## cos(2 * pi * Time/24) -0.0752 0.0789 -0.9535 0.34036 ## ghm -2.8516 1.5657 -1.8214 0.06855 ## sin(2 * pi * Time/24) -0.3062 0.0625 -4.8980 &lt; 1e-04 ## sin(2 * pi * Time/12) -0.4642 0.0710 -6.5403 &lt; 1e-04 ## cos(2 * pi * Time/12) -0.4561 0.0705 -6.4672 &lt; 1e-04 ## cos(2 * pi * Time/24):ghm 0.1742 0.9703 0.1796 0.85750 ## ghm:sin(2 * pi * Time/24) 0.8375 0.7415 1.1294 0.25872 ## ghm:sin(2 * pi * Time/12) -0.9825 0.8686 -1.1311 0.25802 ## ghm:cos(2 * pi * Time/12) -0.6938 0.8599 -0.8068 0.41979 ## ## Integration: ## method: adaptive Gauss-Hermite quadrature rule ## quadrature points: 11 ## ## Optimization: ## method: quasi-Newton ## converged: TRUE # cyclic cubic spline HGAM without common smoother for Time mod_cycl1_cont &lt;- bam(cbind(success, failure) ~ ghm + s(Time, bs = &quot;cc&quot;, k = 12, by = ghm, m = 1) + s(Site, bs=&quot;re&quot;), knots = list(Time=c(0,23)), family = &quot;binomial&quot;, data = occasions_cbind ) summary(mod_cycl1_cont) ## ## Family: binomial ## Link function: logit ## ## Formula: ## cbind(success, failure) ~ ghm + s(Time, bs = &quot;cc&quot;, k = 12, by = ghm, ## m = 1) + s(Site, bs = &quot;re&quot;) ## ## Parametric coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -6.2804 0.1353 -46.427 &lt; 2e-16 *** ## ghm -5.8321 1.5420 -3.782 0.000155 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df Chi.sq p-value ## s(Time):ghm 7.908 9.079 143.1 &lt;2e-16 *** ## s(Site) 80.847 99.000 603.9 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Rank: 112/113 ## R-sq.(adj) = 0.243 Deviance explained = 30.7% ## fREML = 4135.1 Scale est. = 1 n = 2784 To facilitate understanding, we compare the model-based estimates of activity visually for different quantiles (0.025, 0.5 and 0.975) of the GHM covariate. # create a new dataset for the estimates newdat &lt;- with(occasions_cbind, expand.grid(Time = seq(min(Time), 24, length.out = 48), ghm = quantile(ghm, probs = c(0.025, 0.5, 0.975)), Site = &quot;7B&quot; # Site does not matter ) ) %&gt;% mutate(ghm_ft = as.factor(ghm), ghm_ft = fct_recode(ghm_ft, &quot;Low GHM&quot; = levels(ghm_ft)[1], &quot;Intermediate GHM&quot; = levels(ghm_ft)[2], &quot;High GHM&quot; = levels(ghm_ft)[3]) ) # trigonometric estimates cond_eff_cont &lt;- effectPlotData(mod_trig_cont, newdat, marginal = FALSE) %&gt;% mutate(pred = plogis(pred), low = plogis(low), upp = plogis(upp), Model = &quot;Trig GLMM&quot; ) # cyclic cubic spline estimates cycl_pred1_cont &lt;- predict.bam(mod_cycl1_cont, newdata = newdat, exclude = &quot;s(Site)&quot;, se.fit = TRUE, type = &quot;response&quot; ) cycl_pred1_cont &lt;- cbind(newdat, pred = cycl_pred1_cont$fit, low = cycl_pred1_cont$fit - 1.95*cycl_pred1_cont$se.fit, upp = cycl_pred1_cont$fit + 1.95*cycl_pred1_cont$se.fit, Model = &quot;CC spline HGAM&quot; ) # bring together the estimates based on the two approaches pred_cont &lt;- rbind(cond_eff_cont, cycl_pred1_cont) # plot results ggplot(pred_cont, aes(Time, pred)) + geom_ribbon(aes(ymin = low, ymax = upp, color = ghm_ft, fill = ghm_ft), alpha = 0.3, linewidth = 0.25) + geom_line(aes(color = ghm_ft), linewidth = 1) + scale_color_viridis_d() + scale_fill_viridis_d() + labs(x = &quot;Time of Day (Hour)&quot;, y = &quot;Predicted Activity Pattern \\n (probability)&quot;)+ #coord_cartesian(ylim = c(0, 0.005)) + theme_minimal()+ theme(legend.position = &quot;none&quot;, legend.title = element_blank(), legend.text = element_text(size=10,face=&quot;bold&quot;), legend.margin=margin(0,0,0,0), legend.box.margin=margin(-5,-10,-10,-10), plot.title = element_text(size=10,face=&quot;bold&quot;), axis.line = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.ticks = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.title = element_text(size=9,face=&quot;bold&quot;), panel.grid.minor.y = element_blank(), panel.grid.major.y = element_blank(), panel.grid.major.x = element_line(colour = &#39;lightgrey&#39;, linetype = &#39;dashed&#39;, linewidth=0.5), panel.grid.minor.x = element_blank(), strip.text = element_text(size = 9, colour = &quot;black&quot;, face = &quot;bold&quot;, hjust = 0) ) + scale_x_continuous(breaks=seq(0,23,length.out=7), labels=seq(0,24,4)) + facet_grid(Model~ghm_ft) Figure 6.5: Predicted probability of activity by black bears varying by levels of the continuous covariate global human modification index (GHM) using a cyclic cubic spline hierarhical model (first row) and trigonometric hierarhical model (second row). The estimates based on the cyclic cubic spline HGAM show a clear progression in the shape of black bears’ activity pattern, from having an almost cathemeral pattern at low level of GHM to a well-defined bimodal pattern at the high level of GHM. This trend is less evident in the estimates based on trigonometric GLMM. We can further evaluate evidence for the effect of GHM on activity by comparing these models with their relative null model (i.e. a version of the model without the covariate of interest) using AIC (or LRT for trigonometric GLMMs). # null random intercept-only trigonometric GLMM mod_trig_cont_null &lt;- mixed_model(fixed = cbind(success, failure) ~ cos(2 * pi * Time/24) + sin(2 * pi * Time/24) + sin(2 * pi * Time/12) + cos(2 * pi * Time/12), random = ~ 1 | Site, data = occasions_cbind, family = binomial(), iter_EM = 0 ) # null cyclic cubic spline HGAM without common smoother for Time mod_cycl1_cont_null &lt;- bam(cbind(success, failure) ~ s(Time, bs = &quot;cc&quot;, k = 12, m = 1) + s(Site, bs=&quot;re&quot;), knots = list(Time=c(0,23)), family = &quot;binomial&quot;, data = occasions_cbind ) # Comparing trig GLMMs AIC(mod_trig_cont, mod_trig_cont_null) ## df AIC ## mod_trig_cont 11 3837.795 ## mod_trig_cont_null 6 3833.623 AIC(mod_trig_cont_null) - AIC(mod_trig_cont) ## [1] -4.17234 # Comparing cyclic cubic spline HGAMs AIC(mod_cycl1_cont, mod_cycl1_cont_null) ## df AIC ## mod_cycl1_cont 91.44899 3825.111 ## mod_cycl1_cont_null 91.92963 3709.086 AIC(mod_cycl1_cont_null) - AIC(mod_cycl1_cont) ## [1] -116.0255 We find that there is little support for an effect of GHM on the activity patterns of American black bear, which is perhaps not surprising given the low level of human disturbance at these sites. "],["temppart.html", "Tutorial 7 Temporal partitioning", " Tutorial 7 Temporal partitioning Another common question related to activity patterns often asked by ecologists is whether a species changes its activity due to the co-occurrence of another species. In this Tutorial, we illustrate how hierarchical models can be used to also address this set of questions. As an example, we analyze temporal partitioning between coyotes and wolves (Canis lupus). We format the data as usual. # Load libraries rm(list = ls()) set.seed(129) library(dplyr) library(lubridate) library(GLMMadaptive) library(ggpubr) library(mgcv) library(forcats) # Load data: Coyote/Wolf Species interactions dat &lt;- read.csv(&quot;data_input/species_records.csv&quot;, as.is = TRUE) %&gt;% filter(c(Species == &quot;Coyote&quot; | Species == &quot;Wolf&quot;)) %&gt;% droplevels() %&gt;% mutate(DateTimeOriginal = ymd_hm(DateTimeOriginal)) cov &lt;- read.csv(&quot;data_input/CameraTrapProject_CT_data_for_analysis_MASTER.csv&quot;, as.is = TRUE) %&gt;% select(c(Session, Site, Date_setup, Date_retr, Problem1_from, Problem1_to)) %&gt;% mutate(Date_setup = mdy(Date_setup), Date_retr = mdy(Date_retr), Problem1_from = mdy(Problem1_from), Problem1_to = mdy(Problem1_to) ) However, before organizing the data using the cbind(success, failure) approach we need to include the additional information on whether the other species was also detected at least once at a certain site during a sampling session. We thus create two lists of site X session (hereafter, site-session) in which either wolves or coyotes have been encountered and then look at the overlap between these two lists. # Where both species were detected at the same site during a session? # List sites with at least a detection on a given session site_Co &lt;- dat %&gt;% filter(Species == &quot;Coyote&quot;) %&gt;% group_by(Session, Station) %&gt;% summarize(n = n()) ## `summarise()` has grouped output by &#39;Session&#39;. You can override using the `.groups` argument. site_Wo &lt;- dat %&gt;% filter(Species == &quot;Wolf&quot;) %&gt;% group_by(Session, Station) %&gt;% summarize(n = n()) ## `summarise()` has grouped output by &#39;Session&#39;. You can override using the `.groups` argument. site &lt;- semi_join(site_Co, site_Wo, by = c(&quot;Session&quot;, &quot;Station&quot;)) %&gt;% mutate(Sess_site = paste(Session, Station, sep = &quot;_&quot;)) Wolves and coyotes have been detected at 191 and 104 site-sessions, respectively, and 47 times both species occurred at the same site-session. # Add binary variable for presence of other species dat$oth_sp &lt;- NA for(i in 1:nrow(dat)){ dat$oth_sp[i] &lt;- ifelse(paste(dat$Session[i], dat$Station[i], sep = &quot;_&quot;) %in% site$Sess_site, 1,0) } table(dat$Species, dat$oth_sp) ## ## 0 1 ## Coyote 3209 1747 ## Wolf 8148 2248 For each species (e.g., coyote), we used a dummy variable called oth_sp to describe whether the other species (e.g., wolf) was photographed at least once (oth_sp = 1; 0 otherwise) at the same site during a specific site-session. This resulted in 1747 and 3209 records of coyotes at site-sessions where wolves were and were not recorded, and 2248 and 8148 records of wolves at site-sessions where coyotes were and were not recorded. With this additional piece of information, we can proceed with formatting the data (i.e. counting the number of successes and failures for each grouping). We build a dataframe to store the encounter/nonencounter information for each species. # Prepare data for model-based methods # Merge time of deployment and retrieval + problems site_Co2 &lt;- cov site_Co2$end &lt;- ymd(&quot;2000-01-01&quot;) for(i in 1:nrow(site_Co2)){ site_Co2$end[i] &lt;- min(site_Co2$Date_retr[i], site_Co2$Problem1_from[i], na.rm = TRUE ) } # Create dataframe to store captures occasions_Co &lt;- vector(&quot;list&quot;, length = nrow(site_Co2)) for(i in 1:nrow(site_Co2)){ occasions_Co[[i]] &lt;- data.frame(Session = site_Co2$Session[i], site = site_Co2$Site[i], start = seq(from = ymd_hms(paste(site_Co2$Date_setup[i], &quot;00:00:00&quot;, sep = &quot; &quot;)), to = ymd_hms(paste(site_Co2$end[i], &quot;23:59:59&quot;, sep = &quot; &quot;)), by = &#39;60 min&#39;) ) %&gt;% mutate(end = c(start[2:length(start)], start[length(start)]+minutes(60) ) ) } occasions_Co &lt;- do.call(rbind.data.frame, occasions_Co) occasions_Co$capt &lt;- 0 occasions_Wo &lt;- occasions_Co Then, we populate these dataframes with the information recorded for coyotes and wolves, respectively. We start with coyotes. # Coyote data: # Store captures dat_Co &lt;- dat %&gt;% filter(Species == &quot;Coyote&quot;) for(i in 1:nrow(dat_Co)){ occasions_Co[occasions_Co$Session == as.character(dat_Co$Session[i]) &amp; occasions_Co$site == as.character(dat_Co$Station[i]) &amp; occasions_Co$start &lt;= dat_Co$DateTimeOriginal[i] &amp; occasions_Co$end &gt; dat_Co$DateTimeOriginal[i], &quot;capt&quot;] &lt;- 1 } # Format data occasions_Co$Time &lt;- hour(occasions_Co$start) occasions_Co$oth_sp &lt;- NA for(i in 1:nrow(occasions_Co)){ occasions_Co$oth_sp[i] &lt;- ifelse(paste(occasions_Co$Session[i], occasions_Co$site[i], sep = &quot;_&quot;) %in% site$Sess_site, 1,0) } table(occasions_Co$oth_sp, occasions_Co$capt) ## ## 0 1 ## 0 514127 121 ## 1 56798 130 occasions_Co$site &lt;- as.factor(occasions_Co$site) occasions_Co$oth_sp &lt;- as.factor(occasions_Co$oth_sp) # format data for cbind(success, failure) occasions_Co_cbind &lt;- occasions_Co %&gt;% group_by(site, Time, oth_sp) %&gt;% summarize(success = sum(capt), failure = n() - success) ## `summarise()` has grouped output by &#39;site&#39;, &#39;Time&#39;. You can override using the `.groups` argument. We had 130 hourly intervals (out of 56 928) and 121 hourly intervals (out of 514 248) in which coyotes were recorded at a site-session where wolves were and were not photographed. We repeat the same steps for wolves. # Store captures dat_Wo &lt;- dat %&gt;% filter(Species == &quot;Wolf&quot;) for(i in 1:nrow(dat_Wo)){ occasions_Wo[occasions_Wo$Session == as.character(dat_Wo$Session[i]) &amp; occasions_Wo$site == as.character(dat_Wo$Station[i]) &amp; occasions_Wo$start &lt;= dat_Wo$DateTimeOriginal[i] &amp; occasions_Wo$end &gt; dat_Wo$DateTimeOriginal[i], &quot;capt&quot;] &lt;- 1 } # Format data occasions_Wo$Time &lt;- hour(occasions_Wo$start) occasions_Wo$oth_sp &lt;- NA for(i in 1:nrow(occasions_Wo)){ occasions_Wo$oth_sp[i] &lt;- ifelse(paste(occasions_Wo$Session[i], occasions_Wo$site[i], sep = &quot;_&quot;) %in% site$Sess_site, 1,0) } table(occasions_Wo$oth_sp, occasions_Wo$capt) ## ## 0 1 ## 0 513894 354 ## 1 56747 181 occasions_Wo$site &lt;- as.factor(occasions_Wo$site) occasions_Wo$oth_sp &lt;- as.factor(occasions_Wo$oth_sp) # format data for cbind(success, failure) occasions_Wo_cbind &lt;- occasions_Wo %&gt;% group_by(site, Time, oth_sp) %&gt;% summarize(success = sum(capt), failure = n() - success) ## `summarise()` has grouped output by &#39;site&#39;, &#39;Time&#39;. You can override using the `.groups` argument. For this species, we had 181 (out of 56 928) and 354 (out of 514 248) hourly intervals with at least 1 record of wolves at site-sessions in which coyotes were and were not recorded. We now proceed to fit a model to the data for each species. For convenience, we run a trigonometric random intercept-only model, but more complex trigonometric GLMMs or cyclic cubic spline HGAMs could be considered. # run model: coyote mod_trig_Co &lt;- mixed_model(fixed = cbind(success, failure) ~ cos(2*pi*Time/24)*oth_sp + sin(2*pi*Time/24)*oth_sp + cos(2*pi*Time/12)*oth_sp + sin(2*pi*Time/12)*oth_sp, random = ~ 1 | site, family = binomial(), data = occasions_Co_cbind, iter_EM = 0 ) summary(mod_trig_Co) ## ## Call: ## mixed_model(fixed = cbind(success, failure) ~ cos(2 * pi * Time/24) * ## oth_sp + sin(2 * pi * Time/24) * oth_sp + cos(2 * pi * Time/12) * ## oth_sp + sin(2 * pi * Time/12) * oth_sp, random = ~1 | site, ## data = occasions_Co_cbind, family = binomial(), iter_EM = 0) ## ## Data Descriptives: ## Number of Observations: 3168 ## Number of Groups: 100 ## ## Model: ## family: binomial ## link: logit ## ## Fit statistics: ## log.Lik AIC BIC ## -743.6756 1509.351 1538.008 ## ## Random effects covariance matrix: ## StdDev ## (Intercept) 1.164095 ## ## Fixed effects: ## Estimate Std.Err z-value p-value ## (Intercept) -9.0237 0.1935 -46.6262 &lt; 1e-04 ## cos(2 * pi * Time/24) 0.6432 0.1617 3.9778 &lt; 1e-04 ## oth_sp1 1.4042 0.1932 7.2670 &lt; 1e-04 ## sin(2 * pi * Time/24) 0.3302 0.1322 2.4974 0.012512 ## cos(2 * pi * Time/12) -0.3449 0.1389 -2.4830 0.013030 ## sin(2 * pi * Time/12) -0.3411 0.1392 -2.4502 0.014279 ## cos(2 * pi * Time/24):oth_sp1 0.1993 0.2248 0.8866 0.375297 ## oth_sp1:sin(2 * pi * Time/24) 0.0174 0.1953 0.0892 0.928941 ## oth_sp1:cos(2 * pi * Time/12) 0.3083 0.1946 1.5840 0.113191 ## oth_sp1:sin(2 * pi * Time/12) 0.1930 0.1954 0.9876 0.323334 ## ## Integration: ## method: adaptive Gauss-Hermite quadrature rule ## quadrature points: 11 ## ## Optimization: ## method: quasi-Newton ## converged: TRUE # run model: wolf mod_trig_Wo &lt;- mixed_model(fixed = cbind(success, failure) ~ cos(2*pi*Time/24)*oth_sp + sin(2*pi*Time/24)*oth_sp + cos(2*pi*Time/12)*oth_sp + sin(2*pi*Time/12)*oth_sp, random = ~ 1 | site, family = binomial(), data = occasions_Wo_cbind, iter_EM = 0 ) summary(mod_trig_Wo) ## ## Call: ## mixed_model(fixed = cbind(success, failure) ~ cos(2 * pi * Time/24) * ## oth_sp + sin(2 * pi * Time/24) * oth_sp + cos(2 * pi * Time/12) * ## oth_sp + sin(2 * pi * Time/12) * oth_sp, random = ~1 | site, ## data = occasions_Wo_cbind, family = binomial(), iter_EM = 0) ## ## Data Descriptives: ## Number of Observations: 3168 ## Number of Groups: 100 ## ## Model: ## family: binomial ## link: logit ## ## Fit statistics: ## log.Lik AIC BIC ## -1275.273 2572.546 2601.203 ## ## Random effects covariance matrix: ## StdDev ## (Intercept) 1.134112 ## ## Fixed effects: ## Estimate Std.Err z-value p-value ## (Intercept) -7.8886 0.1439 -54.8261 &lt; 1e-04 ## cos(2 * pi * Time/24) 0.5082 0.0880 5.7740 &lt; 1e-04 ## oth_sp1 0.4711 0.1457 3.2344 0.00121891 ## sin(2 * pi * Time/24) 0.3586 0.0783 4.5796 &lt; 1e-04 ## cos(2 * pi * Time/12) -0.2094 0.0791 -2.6452 0.00816464 ## sin(2 * pi * Time/12) -0.3083 0.0800 -3.8520 0.00011716 ## cos(2 * pi * Time/24):oth_sp1 0.1295 0.1457 0.8893 0.37385895 ## oth_sp1:sin(2 * pi * Time/24) -0.1112 0.1473 -0.7547 0.45045328 ## oth_sp1:cos(2 * pi * Time/12) 0.4407 0.1379 3.1956 0.00139527 ## oth_sp1:sin(2 * pi * Time/12) -0.0862 0.1405 -0.6133 0.53966172 ## ## Integration: ## method: adaptive Gauss-Hermite quadrature rule ## quadrature points: 11 ## ## Optimization: ## method: quasi-Newton ## converged: TRUE We now plot the results for both coyotes and wolves in the presence and absence of the other species. We focus on estimating marginal mean activity patterns because our aim is to compare between two groups that differ in their characteristics (i.e. site-sessions with and without the other species). The only change to the code required to obtain the marginal means is to set the argument marginal in the effectPlotData function equal to TRUE (see Tutorial 8 for more on conditional and marginal mean activity patterns). # coyote newdat &lt;- with(occasions_Co_cbind, expand.grid(Time = seq(min(Time), 24, length.out = 48), oth_sp = as.factor(c(0,1)) ) ) cond_eff_Co &lt;- effectPlotData(mod_trig_Co, newdat, marginal = TRUE) cond_eff_Co$Species &lt;- &quot;Coyote&quot; # wolf newdat &lt;- with(occasions_Wo_cbind, expand.grid(Time = seq(min(Time), 24, length.out = 48), oth_sp = as.factor(c(0,1)) ) ) cond_eff_Wo &lt;- effectPlotData(mod_trig_Wo, newdat, marginal = TRUE) cond_eff_Wo$Species &lt;- &quot;Wolf&quot; cond_eff &lt;- rbind(cond_eff_Co, cond_eff_Wo) %&gt;% mutate(oth_sp_labels = fct_recode(oth_sp, &quot;Without the other species&quot; = &quot;0&quot;, &quot;With the other species&quot; = &quot;1&quot;)) (pl_trig &lt;- ggplot(cond_eff, aes(Time, plogis(pred), color=Species, group=Species)) + geom_ribbon(aes(ymin=plogis(low), ymax=plogis(upp), fill=Species), alpha=0.5, linewidth = 0.25) + geom_line(size=1) + labs(x = &quot;Time of Day (Hour)&quot;, y = &quot;Predicted Activity Pattern \\n (probability)&quot;, title = &quot;A: trigonometric GLMMs&quot;)+ scale_color_manual(values = c(&quot;#E69F00&quot;, &quot;#0072B2&quot;)) + scale_fill_manual(values = c(&quot;#E69F00&quot;, &quot;#0072B2&quot;)) + theme_minimal()+ theme(legend.position = &quot;right&quot;, legend.title = element_text(size=10,face=&quot;bold&quot;), plot.title = element_blank(), axis.line = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.ticks = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.title = element_text(size=9,face=&quot;bold&quot;), panel.grid.minor.y = element_blank(), panel.grid.major.y = element_blank(), panel.grid.major.x = element_line(colour = &#39;lightgrey&#39;, linetype = &#39;dashed&#39;, linewidth=0.5), panel.grid.minor.x = element_blank(), strip.text = element_text(face = &quot;bold&quot;) ) + scale_x_continuous(breaks=seq(0,24,4), labels=seq(0,24,4)) + facet_wrap(~oth_sp_labels, scales = &quot;fixed&quot;) ) Figure 7.1: Predicted probability of activity by coyote and wolf and depending on each others presence or absence; shading corresponds to 95% confidence intervals. ggsave(plot = pl_trig, filename = &quot;ms_figures/Fig5_Coyote_Wolf_patterns.jpg&quot;, device = &quot;jpeg&quot;, units = &quot;cm&quot;, width = 24, height = 10, dpi = 600) This figure is presented in Iannarilli et al. (2024) as Figure 5; there, we offer additional discussion regarding the estimated activity curves. Next, we consider a cyclic cubic spline HGAM (without a site-specific smoother) for each species, which we compare to the trigonometric GLMM below. # Cyclic cubic splines (&#39;random intercept-only&#39;) # Coyote mod_cycl_Co &lt;- bam(cbind(success, failure) ~ s(Time, bs = &quot;cc&quot;, k = 12, by = oth_sp, m = 1) + oth_sp + s(site, bs=&quot;re&quot;), knots = list(Time=c(0,23)), family = &quot;binomial&quot;, data = occasions_Co_cbind ) newdat &lt;- with(occasions_Co_cbind, expand.grid(Time = seq(0, 24, 1), oth_sp = as.factor(c(0,1)), site = &quot;10E&quot; #Site doesn&#39;t matter ) ) cycl_pred_Co &lt;- predict.bam(mod_cycl_Co, newdata = newdat, exclude = &quot;s(site)&quot;, se.fit = TRUE, type = &quot;response&quot; ) cycl_pred_Co &lt;- cbind(newdat, fit=cycl_pred_Co$fit, se.fit=cycl_pred_Co$se.fit, Species = &quot;Coyote&quot; ) # Wolf mod_cycl_Wo &lt;- bam(cbind(success, failure) ~ s(Time, bs = &quot;cc&quot;, k = 12, by = oth_sp, m = 1) + oth_sp + s(site, bs=&quot;re&quot;), knots = list(Time=c(0,23)), family = &quot;binomial&quot;, data = occasions_Wo_cbind ) newdat &lt;- with(occasions_Wo_cbind, expand.grid(Time = seq(0, 24, 1), oth_sp = as.factor(c(0,1)), site = &quot;10E&quot; #Site doesn&#39;t matter ) ) cycl_pred_Wo &lt;- predict.bam(mod_cycl_Wo, newdata = newdat, exclude = &quot;s(site)&quot;, se.fit = TRUE, type = &quot;response&quot; ) cycl_pred_Wo &lt;- cbind(newdat, fit=cycl_pred_Wo$fit, se.fit=cycl_pred_Wo$se.fit, Species = &quot;Wolf&quot; ) cycl_pred &lt;- rbind(cycl_pred_Co, cycl_pred_Wo) %&gt;% mutate(oth_sp_labels = fct_recode(oth_sp, &quot;Without the other species&quot; = &quot;0&quot;, &quot;With the other species&quot; = &quot;1&quot;)) pl_cycl &lt;- ggplot(cycl_pred, aes(Time, fit, color=Species, group=Species)) + geom_ribbon(aes(ymin=fit-1.96*se.fit, ymax=fit+1.96*se.fit, fill=Species), alpha=0.5, size = 0.25) + geom_line(linewidth = 1) + labs(x = &quot;Time of Day (Hour)&quot;, y = &quot;Predicted Activity Pattern \\n (probability)&quot;, title = &quot;B: Conditional mean (cyclic cubic spline HGAMs)&quot;)+ scale_color_manual(values = c(&quot;#E69F00&quot;, &quot;#0072B2&quot;)) + scale_fill_manual(values = c(&quot;#E69F00&quot;, &quot;#0072B2&quot;)) + theme_minimal()+ theme(legend.position = &quot;none&quot;, plot.title = element_text(size=10,face=&quot;bold&quot;), axis.line = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.ticks = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.title = element_text(size=9,face=&quot;bold&quot;), panel.grid.minor.y = element_blank(), panel.grid.major.y = element_blank(), panel.grid.major.x = element_line(colour = &#39;lightgrey&#39;, linetype = &#39;dashed&#39;, linewidth = 0.5), panel.grid.minor.x = element_blank(), strip.text = element_text(face = &quot;bold&quot;) ) + scale_x_continuous(breaks=seq(0,24,4), labels=seq(0,24,4)) + facet_wrap(~oth_sp_labels, scales = &quot;fixed&quot;) pl_trig2 &lt;- pl_trig + labs(title = &quot;A: Marginal mean (trigonometric GLMM)&quot;) + theme(legend.position = &quot;none&quot;, plot.title = element_text(size=10,face=&quot;bold&quot;, hjust = 0)) ggarrange(pl_trig2, pl_cycl, nrow = 2, ncol = 1, common.legend = TRUE, legend = &quot;right&quot;) Figure 7.2: Predicted probability of activity by coyote and wolf and depending on each others presence or absence from a trigonometric (A) and cyclic cubic spline (B) hierarhical model; shading corresponds to 95% confidence intervals. We see that the trigonometric and the cyclic cubic spline hierarchical models return the same general pattern. Model selection. As with the other examples, we can assess the importance of the presence of the competitor species by comparing models with and without terms involving this covariate (i.e. oth_sp). We fit the reduced model below. # Trigonometric GLMM (random intercept-only) # run model: coyote mod_trig_Co_null &lt;- mixed_model(fixed = cbind(success, failure) ~ cos(2*pi*Time/24) + sin(2*pi*Time/24) + cos(2*pi*Time/12) + sin(2*pi*Time/12), random = ~ 1 | site, family = binomial(), data = occasions_Co_cbind, iter_EM = 0 ) # run model: wolf mod_trig_Wo_null &lt;- mixed_model(fixed = cbind(success, failure) ~ cos(2*pi*Time/24) + sin(2*pi*Time/24) + cos(2*pi*Time/12) + sin(2*pi*Time/12), random = ~ 1 | site, family = binomial(), data = occasions_Wo_cbind, iter_EM = 0 ) For each of the two species, we can now compare the models that include and do not include the presence of the other species. For coyotes, AIC(mod_trig_Co_null, mod_trig_Co) ## df AIC ## mod_trig_Co_null 6 1576.861 ## mod_trig_Co 11 1509.351 AIC(mod_trig_Co_null) - AIC(mod_trig_Co) ## [1] 67.5101 Based on AIC, there is a strong support for the model that assumes an effect of the presence of wolves as a driver of coyotes’ activity patterns. AIC(mod_trig_Wo_null, mod_trig_Wo) ## df AIC ## mod_trig_Wo_null 6 2592.905 ## mod_trig_Wo 11 2572.546 AIC(mod_trig_Wo_null) - AIC(mod_trig_Wo) ## [1] 20.35866 A similar conclusion is reached for the wolf. These results do not imply a casual effect, but rather define an association: in site-sessions where the species co-occur (i.e. in space and time), they tend to have activity patterns that differ compared to site-sessions when they do not co-occur. These differences might be driven by other factors (e.g., level of human disturbance) not considered in the analysis. "],["condvsmarg.html", "Tutorial 8 Conditional and marginal mean activity patterns 8.1 Estimating conditional and marginal mean activity patterns 8.2 The frequentist framework", " Tutorial 8 Conditional and marginal mean activity patterns GLMMs allow users to estimate both site-specific diel activity patterns and population-level activity patterns formed by averaging activity levels across sites in the population (see Box 1 in Iannarilli et al. (2024)). Site-specific estimates are frequently referred to as conditional means since they are formed by conditioning on a set of site-specific parameters; population-level means are often referred to as marginal means. Although one can estimate the conditional mean associated with any of the sampled sites, it is also common to plot the conditional mean for a ‘typical site’ (i.e. a site with average characteristics), formed by setting all of the random coefficients to their mean values (Fieberg et al. 2009). Conditional and marginal mean activity patterns can be estimated in both the frequentist and Bayesian frameworks. The frequentist framework is generally accessible by more users than the Bayesian framework thanks to the availability of many ready-to-use R packages that implement GLMMs, so we first focus on this framework. An example of the same approach in the Bayesian framework is provided in section 10.3. We use simulated data to illustrate how random intercept-only and random intercept and slope models are able to return differences in conditional and marginal mean activity patterns. Data simulation. Using the approach introduced in Tutorials 2 and 3, we simulate encounter events for a species with a bimodal activity pattern using camera traps set at 100 sites with 30 days of sampling at each site. In addition to simulating the data, the sim_activity function also returns the ‘true’ conditional activity pattern for a typical site (one with random effects \\(\\tau_i = 0\\) and \\(\\gamma_i = 0\\)) at each of 513 equally spaced points \\(j\\) between 0 and 24 hours (i.e. covering the diel activity pattern) using: \\[E[Y_{it} |\\tau_i=0,\\gamma_i=0] = \\frac{exp^{\\beta_0 + \\beta_1*\\text{cos}(\\frac{2\\pi t}{24} + \\theta_0) + \\beta_2*\\text{cos}(\\frac{2\\pi t}{12} + \\theta_1)}}{1+exp^{\\beta_0 + \\beta_1*\\text{cos}(\\frac{2\\pi t}{24} + \\theta_0) + \\beta_2*\\text{cos}(\\frac{2\\pi t}{12} + \\theta_1)}}\\] where \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), \\(\\theta_0\\), and \\(\\theta_1\\) correspond to the parameter values used to simulate the data (see Tutorial 3). In addition, the sim_activity function returns the ‘true’ marginal mean activity pattern, which is formed by averaging activity patterns across sites. To determine marginal means, we have to integrate over the distribution of the random effects: \\[E[Y_{t}] = \\left[\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty} \\frac{\\exp^{\\beta_0 + \\beta_1\\text{cos}(\\frac{2\\pi t}{24} + (\\theta_0 + \\gamma_i)) + \\beta_2\\text{cos}(\\frac{2\\pi t}{12} + (\\theta_1 + \\gamma_i))+ \\tau_i}}{1+\\exp^{\\beta_0 + \\beta_1\\text{cos}(\\frac{2\\pi t}{24} + (\\theta_0+\\gamma_i)) + \\beta_2\\text{cos}(\\frac{2\\pi t}{12} + (\\theta_1+\\gamma_i))+ \\tau_i}} \\frac{\\exp^{\\frac{-\\tau^{2}}{2\\sigma_{\\tau}^2}}}{\\sqrt{2\\pi}\\sigma_{\\tau}} \\frac{\\exp^{\\frac{-\\gamma^{2}}{2\\sigma_{\\gamma}^2}}}{\\sqrt{2\\pi}\\sigma_{\\gamma}} \\; d\\tau d\\gamma\\right]\\] Because this set of integrals has no closed-form solution, we must use approximation methods to solve for \\(E[Y_t]\\). In Tutorial 2, we used the integrate function to approximate \\(E[Y_t]\\). Alternatively, the sim_activity function estimates the marginal mean curve using a simulation approach in which it randomly selects 100,000 sites and their associated values of \\(\\tau_i\\) and \\(\\gamma_i\\), determines site-specific activity patterns for these sites on the probability scale, and then averages these site-specific activity patterns at each of the 513 reference points \\(j\\) between 0 and 23 hours. If we plot the estimated conditional and marginal mean activity curves (Figure 8.1), we see that for this set of parameter values, the two means have a similar shape. However, marginal mean is higher than the conditional mean, suggesting a higher level of overall activity when measured at the population level compared to the activity level at a typical site. # Load libraries and function to simulate activity patterns library(dplyr) library(GLMMadaptive) library(mgcv) library(tidyr) source(&quot;source_functions/sim_activity.R&quot;) source(&quot;source_functions/sim_to_minute.R&quot;) set.seed(129) # Set equations&#39; parameters M = 100 J = 30 wavelength = 24 n_peak = 2 b0 = -3 b1 = 1 b2 = 0.7 theta0 = 3 theta1 = 2 sd_tau = 1 sd_gamma = 0.3 time &lt;- seq(0, wavelength, wavelength/512) dat &lt;- sim_activity(M = M, J = J, wavelength = wavelength, n_peak = n_peak, n_sim = 1, b0 = -3, b0_constant = TRUE, # common intercept tau_constant = FALSE, sdtau = sd_tau, # ~site-specific intercept b1 = b1, b2 = b2, # amplitude of the cosine terms theta0 = theta0, theta1 = theta1, # common phaseshifts for the cosine terms phaseshift_constant = FALSE, sd_phaseshift = sd_gamma, # site-specific phaseshift (equal for both cosine terms) plot_true_act = TRUE ) Figure 8.1: Conditional activity pattern for a typical site (one with random effects equal to zero) and marginal mean activity pattern formed by averaging site-specific activity patterns across a population of sites. 8.1 Estimating conditional and marginal mean activity patterns Before fitting models, we need to prepare the data. We use the code from Tutorial 3 to aggregate the data by site, resulting in a data set with the following variables: Site = ID for each surveyed site Time = hour of day success = number of encounters at the particular Site and Time n_events = number of observation intervals associated with the particular Site and Time failure = number of observation intervals without an encounters at the particular Site and Time # The data y &lt;- as.data.frame(dat$sim_data) # summarize aggregated data y &lt;- data.frame(id=paste(&quot;A&quot;, seq(1,M,1), sep=&quot;&quot;), y=as.data.frame(y) ) colnames(y) &lt;- c(&quot;id&quot;, paste0(&quot;S&quot;, seq(1, ncol(y)-1, 1), sep=&quot;&quot;) ) # Format data in long format y_long &lt;- pivot_longer(data = y, cols = -id, names_to = &quot;time&quot;, values_to = &quot;y&quot;) %&gt;% mutate(time=as.numeric(substr(time,2,10)), id = factor(id) ) # create variable to describe time as hour (between 0 and 23) y_long$hour &lt;- sim_to_minute(y_long$time, group = wavelength) # count successes and failures at each site-occasion occasions_cbind &lt;- y_long %&gt;% group_by(id, hour) %&gt;% summarise(success = sum(y), n_events = n(), failure = n_events - success ) %&gt;% dplyr::rename(Site = id, Time = hour ) ## `summarise()` has grouped output by &#39;id&#39;. You can override using the `.groups` argument. We are now ready to estimate conditional and marginal mean activity in the frequentist framework. 8.2 The frequentist framework When using trigonometric GLMMs, we can quantify the conditional mean by setting all the random effects in the GLMM as equal to 0 and the marginal mean by integrating over the distribution of the random effects (Fieberg et al. 2009). Among the many packages available in R to model GLMMs, GLMMadaptive (Rizopoulos 2022) has the advantage of providing estimates of parameters describing both conditional and marginal mean response curves. We start by fitting the model, similar to how we have done it in previous sections of these tutorials. # Modelling activity pattern using trigonometric GLMMs in the parametric modelling framework trig_rand_slope &lt;- mixed_model(fixed = cbind(success, failure) ~ cos(2 * pi * Time/24) + sin(2 * pi * Time/24) + cos(2 * pi * Time/12) + sin(2 * pi * Time/12), random = ~ cos(2 * pi * Time/24) + sin(2 * pi * Time/24) + cos(2 * pi * Time/12) + sin(2 * pi * Time/12) || Site, data = occasions_cbind, family = binomial(), iter_EM = 0 ) summary(trig_rand_slope) ## ## Call: ## mixed_model(fixed = cbind(success, failure) ~ cos(2 * pi * Time/24) + ## sin(2 * pi * Time/24) + cos(2 * pi * Time/12) + sin(2 * pi * ## Time/12), random = ~cos(2 * pi * Time/24) + sin(2 * pi * ## Time/24) + cos(2 * pi * Time/12) + sin(2 * pi * Time/12) || ## Site, data = occasions_cbind, family = binomial(), iter_EM = 0) ## ## Data Descriptives: ## Number of Observations: 2400 ## Number of Groups: 100 ## ## Model: ## family: binomial ## link: logit ## ## Fit statistics: ## log.Lik AIC BIC ## -3704.187 7428.374 7454.425 ## ## Random effects covariance matrix: ## StdDev ## (Intercept) 0.9742 ## cos(2 * pi * Time/24) 0.0307 ## sin(2 * pi * Time/24) 0.2343 ## cos(2 * pi * Time/12) 0.1913 ## sin(2 * pi * Time/12) 0.0430 ## ## Fixed effects: ## Estimate Std.Err z-value p-value ## (Intercept) -3.0888 0.1000 -30.8945 &lt; 1e-04 ## cos(2 * pi * Time/24) -0.9410 0.0268 -35.1529 &lt; 1e-04 ## sin(2 * pi * Time/24) -0.1584 0.0345 -4.5927 &lt; 1e-04 ## cos(2 * pi * Time/12) -0.2659 0.0315 -8.4323 &lt; 1e-04 ## sin(2 * pi * Time/12) -0.5988 0.0237 -25.2223 &lt; 1e-04 ## ## Integration: ## method: adaptive Gauss-Hermite quadrature rule ## quadrature points: 7 ## ## Optimization: ## method: quasi-Newton ## converged: TRUE In most of the applications so far (except Tutorial 7), we explored the model results using the predicted conditional mean activity patterns. We calculated these patterns using the function effectPlotData, which is included in the GLMMadaptive package, by setting the argument marginal as equal FALSE. We do the same here. The conditional mean can be easily estimated using other packages available in R to fit GLMMs by setting the random effect equal to 0 and then predict the activity pattern over the 24-hour cycle. For example, we can use the predict function with the argument re.form = NA to obtain conditional means when models are fit using the glmer function in the lme4 package (Bates et al. 2015). We can estimate marginal means using numerical integration (as done in Tutorial 2), simulation, or using various approximation methods (see e.g., section 19.2 of Fieberg 2024). The primary reason we explore mixed_model in the GLMMadaptive package is because it does the hard work for us. To calculate the predicted marginal mean activity pattern, we only need to change the argument marginal from FALSE to TRUE in the effectPlotData function. newdat &lt;- with(occasions_cbind, expand.grid(Time = seq(min(Time), 24, length.out = 48)) ) cond_eff &lt;- effectPlotData(trig_rand_slope, newdat, marginal = FALSE) %&gt;% mutate(pred = plogis(pred), low = plogis(low), upp = plogis(upp), Method = &quot;effectPlotData&quot;, Mean = &quot;Conditional&quot; ) marg_eff &lt;- effectPlotData(trig_rand_slope, newdat, marginal = TRUE) %&gt;% mutate(pred = plogis(pred), low = plogis(low), upp = plogis(upp), Method = &quot;effectPlotData&quot;, Mean = &quot;Marginal&quot; ) It is also possible to calculate both means using the estimated coefficients extracted directly from the fitted model. In this case, we can obtain estimates of the marginal coefficients that approximate the curve formed by integrating over the random effects using the marginal_coefs function in the GLMMadaptive package, which uses a numerical approximation based on an adaptive Gauss-Hermite quadrature rule (Hedeker et al. 2018) to calculate these values. From the parameter estimates, we can calculate the predicted conditional and marginal activity curves using the appropriate coefficient values along with the following equation: \\[ \\text{logit}(p_t) = \\hat{\\beta_0} + \\hat{\\alpha_1}*\\text{cos}(\\frac{2\\pi t}{24}) + \\hat{\\alpha_2}*\\text{sin}(\\frac{2\\pi t}{24}) + \\hat{\\alpha_3}*\\text{cos}(\\frac{2\\pi t}{12}) + \\hat{\\alpha_4}*\\text{sin}(\\frac{2\\pi t}{12}) \\] betas &lt;- matrix(c(t(GLMMadaptive::fixef(trig_rand_slope)), t(GLMMadaptive::marginal_coefs(trig_rand_slope, std_errors = FALSE)$betas) ), ncol=5, nrow = 2, byrow = TRUE ) colnames(betas) &lt;- colnames(t(GLMMadaptive::fixef(trig_rand_slope))) rownames(betas) &lt;- c(&quot;Conditional&quot;, &quot;Marginal&quot;) betas &lt;- as.data.frame(betas) %&gt;% dplyr::mutate(GLMM = &quot;Rand_Intercept_and_Slope&quot;) cond &lt;- plogis(betas[1,1] + betas[1,2] * cos(2*pi*time/24) + betas[1,3] * sin(2*pi*time/24) + betas[1,4] * cos(2*pi*time/12) + betas[1,5] * sin(2*pi*time/12) ) phat_cond &lt;- data.frame(Time = time, pred = cond, low = NA, upp = NA, Method = &quot;coeff. estimates&quot;, Mean = &quot;Conditional&quot; ) marg &lt;- plogis(betas[2,1] + betas[2,2] * cos(2*pi*time/24) + betas[2,3] * sin(2*pi*time/24)+ betas[2,4] * cos(2*pi*time/12) + betas[2,5] * sin(2*pi*time/12) ) phat_marg &lt;- data.frame(Time = time, pred = marg, low = NA, upp = NA, Method = &quot;coeff. estimates&quot;, Mean = &quot;Marginal&quot; ) We visually compare the estimates returned by both methods, and the two estimates to the true conditional and marginal activity patterns used to simulate the data. # extract true conditional and marginal patterns cond_true &lt;- dat$Conditional %&gt;% mutate(low = NA, upp = NA, Method = &quot;True pattern&quot;, Mean = &quot;Conditional&quot;) %&gt;% dplyr::rename(Time = time, pred = p) %&gt;% select(Time, pred, low, upp, Method, Mean) marg_true &lt;- dat$Marginal %&gt;% mutate(low = NA, upp = NA, Method = &quot;True pattern&quot;, Mean = &quot;Marginal&quot;) %&gt;% dplyr::rename(Time = time, pred = p) %&gt;% select(Time, pred, low, upp, Method, Mean) # bring all the information together in one dataset res &lt;- rbind(cond_true, marg_true, cond_eff, marg_eff, phat_cond, phat_marg) ggplot(res, aes(x = Time, y= pred, col = Method)) + geom_ribbon(aes(ymin = low, ymax = upp, fill = Method), alpha = 0.1) + geom_line(linewidth = 1) + labs(x = &quot;Time of Day (Hour)&quot;, y = &quot;Activity Pattern \\n (probability)&quot;)+ theme_minimal()+ theme(legend.position = &quot;bottom&quot;, legend.title = element_blank(), legend.text = element_text(size=10,face=&quot;bold&quot;), legend.margin=margin(0,0,0,0), legend.box.margin=margin(-5,-10,-10,-10), plot.title = element_text(size=10,face=&quot;bold&quot;), axis.line = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.ticks = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.title = element_text(size=9,face=&quot;bold&quot;), panel.grid.minor.y = element_blank(), panel.grid.major.y = element_blank(), panel.grid.major.x = element_line(colour = &#39;lightgrey&#39;, linetype = &#39;dashed&#39;, linewidth=0.5), panel.grid.minor.x = element_blank(), strip.text = element_text(size = 9, colour = &quot;black&quot;, face = &quot;bold&quot;, hjust = 0), plot.margin = margin(0.1,0.1,0.5,0.1, &quot;cm&quot;) ) + scale_x_continuous(breaks=seq(0,23,length.out=7), labels=seq(0,24,4)) + facet_wrap(~Mean) ## Warning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning -Inf ## Warning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning -Inf ## Warning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning -Inf ## Warning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning -Inf Figure 8.2: Comparison of marginal and conditional activity curves produced two ways along with true values used to simulate data. The mean activity patterns returned by the effectDataPlot function and by the manual calculation match perfectly and both closely track the true, simulated conditional and marginal activity curves. The choice of which of the two predictive methods to use depends on the complexity of the fitted model. The effectDataPlot function can require long times to return a prediction when a random slope is included. "],["kdes.html", "Tutorial 9 Kernel Density Estimators 9.1 Data preparation for KDEs 9.2 Comparison of KDE estimators 9.3 Aggregating versus non-aggregating data in KDEs 9.4 KDEs versus hierarchical models approaches", " Tutorial 9 Kernel Density Estimators Kernel Density Estimators (KDEs) are currently the main approach used to estimate activity patterns from time-stamped (e.g., camera-trap) data. At the time of writing, we are aware of three packages available in R to estimate activity patterns using KDEs: overlap (Ridout and Linkie 2009; Meredith and Ridout 2021); activity (Rowcliffe J Marcus 2014; M. 2022); and circular (Agostinelli and Lund 2022a; Oliveira-Santos, Zucco, and Agostinelli 2013). In this Tutorial, we illustrate how to estimate activity patterns using KDEs with these three packages and highlight the consequences of not aggregating data first when using KDEs. We also compare a KDE-based estimate with results obtained using hierarchical model-based approaches for the analysis of seasonal activity patterns of black bears presented in Tutorial 6.1. 9.1 Data preparation for KDEs In Tutorial 4, we introduced the concept of data aggregation, that is, grouping images of a species taken at a certain location within a short time frame into independent encounter events. This process reduces short-term correlation in the data and it is often recommended as a preparatory step before applying KDEs (but see Peral, Landman, and Kerley (2022) for a different point of view). Thus, we begin our KDE-based analysis by aggregating our data from a raw format to independent encounter events. We load the three R libraries listed above along with other packages necessary for wrangling the data. We also load and preview the data. In this first section of this Tutorial, we use the subset of the camera-trap observations of American black bears collected in northern Minnesota between mid-May and mid-July 2017 (Iannarilli et al. 2021). # Load libraries rm(list = ls()) set.seed(129) library(dplyr) library(tidyr) library(forcats) library(ggpubr) library(gridExtra) library(grid) library(lubridate) library(activity) ## Warning: package &#39;activity&#39; was built under R version 4.2.3 library(overlap) ## Warning: package &#39;overlap&#39; was built under R version 4.2.3 ## Warning: package &#39;suntools&#39; was built under R version 4.2.3 library(circular) # Load data dat &lt;- read.csv(&quot;data_input/species_records.csv&quot;) %&gt;% filter(Species == &quot;BlackBear&quot; &amp; Session == &quot;Spring2017&quot;) %&gt;% droplevels %&gt;% select(-X) head(dat) ## Station Species DateTimeOriginal Date Time Session ## 1 10A BlackBear 2017-07-08 18:34 2017-07-08 18:34:02 Spring2017 ## 2 10A BlackBear 2017-07-08 18:34 2017-07-08 18:34:02 Spring2017 ## 3 10A BlackBear 2017-07-08 18:34 2017-07-08 18:34:03 Spring2017 ## 4 10A BlackBear 2017-07-10 18:56 2017-07-10 18:56:24 Spring2017 ## 5 10A BlackBear 2017-07-10 18:56 2017-07-10 18:56:25 Spring2017 ## 6 10A BlackBear 2017-07-10 18:56 2017-07-10 18:56:26 Spring2017 Choosing a temporal threshold for determining whether two records close in time belong to the same encounter event is not trivial. Here, we use a threshold of 30 minutes, which we chose after inspecting lorelograms quantifying the level of dependence as a function of time between observations (Iannarilli et al. 2021, 2019). We set the correct date-time format for the column specifying when each picture was taken (DateTimeOriginal), group the records based on species and camera-trap site (Station), order the data within each group, and calculate the time difference (in minutes) between each record and the previous. When calculating these differences, NAs are assigned to the first record collected for each species at a location. We replace those NAs with zeros, then proceed to assign a unique numeric identifier to each event. We then extract the date-time information for the first record of each group (alternatively, it is possible, for example, to calculate the median time of the records belonging to a certain event) and retain only the columns of interest. # Set the threshold that define independence independence between subsequent records independence_interval &lt;- 30 #minutes # Aggregate data to reduce short-term temporal dependence dat_event &lt;- dat %&gt;% mutate(DateTimeOriginal = ymd_hm(DateTimeOriginal)) %&gt;% group_by(Species, Station) %&gt;% # group by species and location arrange(DateTimeOriginal, .by_group = TRUE) %&gt;% # arrange record temporally within each group mutate(timediff = as.numeric(difftime(DateTimeOriginal, lag(DateTimeOriginal), units=&quot;mins&quot;)), # calculate time difference (in minutes) between consecutive events timediff = replace_na(timediff, 0), grp = ifelse(timediff &gt;= independence_interval, 1, 0), # create events grp_ind = cumsum(grp) + 1) # assign a group number to each event # group by camera site and event group # summarize: consider time of first image as time of the independent event dat_event &lt;- dat_event %&gt;% group_by(Species, Station, grp_ind) %&gt;% slice(1) %&gt;% select(&quot;Station&quot;, &quot;Species&quot;, &quot;DateTimeOriginal&quot;, &quot;Date&quot;, &quot;Time&quot;, &quot;Session&quot;) ## Adding missing grouping variables: `grp_ind` head(dat_event) ## # A tibble: 6 × 7 ## # Groups: Species, Station, grp_ind [6] ## grp_ind Station Species DateTimeOriginal Date Time Session ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 10A BlackBear 2017-07-08 18:34:00 2017-07-08 18:34:02 Spring2017 ## 2 2 10A BlackBear 2017-07-10 18:56:00 2017-07-10 18:56:24 Spring2017 ## 3 1 10B BlackBear 2017-05-23 16:28:00 2017-05-23 16:28:52 Spring2017 ## 4 2 10B BlackBear 2017-06-22 14:29:00 2017-06-22 14:29:44 Spring2017 ## 5 3 10B BlackBear 2017-06-25 20:58:00 2017-06-25 20:58:10 Spring2017 ## 6 4 10B BlackBear 2017-07-02 07:03:00 2017-07-02 7:03:05 Spring2017 # alternative code using the median time associated with each encounter event # summarize: calculate median time for each event # dat_event2 &lt;- dat_event %&gt;% # group_by(Species, Station, grp_ind) %&gt;% # summarise(median_time = median(DateTimeOriginal), # photo_date = Date[1]) We can visually compare the data before and after aggregation. # Plot number of records by camera trap station before and after data aggregation dat_summary &lt;- dat %&gt;% group_by(Station) %&gt;% summarize(n_records = n()) %&gt;% mutate(Aggregation = &quot;Before&quot;) dat_event_summary &lt;- dat_event %&gt;% group_by(Station) %&gt;% summarize(n_records = n()) %&gt;% mutate(Aggregation = &quot;After&quot;) rbind(dat_summary, dat_event_summary) %&gt;% mutate(Aggregation = fct_relevel(Aggregation, &quot;Before&quot;)) %&gt;% ggplot(., aes(x = n_records, y = Station, group = Aggregation)) + geom_col() + labs(y = &quot;Camera trap station&quot;, x = &quot; Number of records&quot;) + theme_bw() + facet_wrap(~Aggregation, scales = &quot;free_x&quot;) Figure 9.1: Numbers of records at each camera site before and after aggregating the data to reduce short-term autocorrelation Comparing the resulting number of records per location before and after data aggregation highlights that many images were taken within less than 30 minutes from a previous image. In the left panel, each record corresponds to an image, whereas, in the right panel, each record corresponds to an independent encounter event. In our specific case, aggregating data also removes duplicates recorded within the same trigger event. We will use the aggregated version of the dataset in the next sections of this tutorial. All the three packages require time values provided as radians. Thus, the next step is to transform the time values from the HH:MM:SS format to radians. This requires some multiplications. Since there are \\(2 \\times \\pi\\) radians in a circle and \\(60 minutes \\times 24 hours\\) in a day, we can estimate minutes in terms of radians by dividing the minutes corresponding to a certain time of the day by the total number of minutes in a day (equal to \\(1440 = 60 \\times 24\\)) and multiplying the resulting value by \\(2 \\times \\pi\\). For simplicity, we do not consider seconds here. # Specify the format of the column Time dat_event$Time &lt;- hms(dat_event$Time) # Convert time values to radians dat_event$Time_Rad &lt;- (hour(dat_event$Time)*60 + minute(dat_event$Time))/(60*24)*2*pi plot((hour(dat_event$Time)*60 + minute(dat_event$Time)), dat_event$Time_Rad, xlab = &quot;Original Timestamp (in minute in a day)&quot;, ylab = &quot;Time in radians&quot; ) The plot above shows the match in time of the records expressed as minutes (x axis) and radians (y axis). 9.2 Comparison of KDE estimators Our data are now ready, and we can proceed with estimating the activity patterns using each of the three packages available in R for applying the KDE approach. We will first run the code for each package and then visually compare the results. 9.2.1 Using the overlap package We start with the overlap package (Ridout and Linkie 2009; Meredith and Ridout 2021). To facilitate comparison of estimates based on the other two packages, we explicitly specify the values for the argument n.grid, which controls the number of points along the 24 hours cycle (in radians) at which density values are estimated. The argument xscale = 24*60 returns the values of these points in minutes, instead of the default radians. # To avoid plotting pdf(file = NULL) # Estimate activity patterns and store values in an object res_over &lt;- densityPlot(dat_event$Time_Rad, rug = FALSE, adjust = 1, xscale = 24*60, n.grid = 513, extend = NULL) %&gt;% mutate(se = NA, Package = &quot;Overlap&quot;, Data = &quot;Aggregated&quot;) dev.off() ## png ## 2 # store plot in object pl_over &lt;- ggplot(res_over, aes(x=x, y=y, group=Data)) + geom_line(aes(linetype=Data, color=Data), linewidth=1) + scale_color_manual(values = c(&quot;#CC79A7&quot;)) + labs(x = &quot;&quot;, y = &quot;Density&quot;, title = &quot;A: Overlap pkg&quot;) + theme_minimal() + theme(legend.position = &quot;none&quot;, legend.background = element_rect(fill = &quot;white&quot;, colour = &quot;black&quot;), legend.title = element_text(size=8,face=&quot;bold&quot;), axis.line = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.ticks = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.text = element_text(size=8), axis.title.y = element_text(size=10,face=&quot;bold&quot;), axis.title.x = element_blank(), panel.grid.minor.y = element_blank(), panel.grid.major.y = element_blank(), panel.grid.major.x = element_line(colour = &#39;lightgrey&#39;, linetype = &#39;dashed&#39;, linewidth=0.5), panel.grid.minor.x = element_blank(), strip.text = element_text(size = 10, colour = &quot;black&quot;, face = &quot;bold&quot;) ) + scale_x_continuous(breaks=seq(0,24*60,60*4), labels=seq(0,24,4), expand = expansion(mult = c(0.02,0.02)))+ scale_y_continuous(expand = expansion(mult = c(0.02, 0.02))) # print first rows of the output head(res_over) ## x y se Package Data ## 1 0.0000 0.0002348518 NA Overlap Aggregated ## 2 2.8125 0.0002256965 NA Overlap Aggregated ## 3 5.6250 0.0002170437 NA Overlap Aggregated ## 4 8.4375 0.0002088781 NA Overlap Aggregated ## 5 11.2500 0.0002011836 NA Overlap Aggregated ## 6 14.0625 0.0001939432 NA Overlap Aggregated 9.2.2 Using the activity package The fitact function within the activity package (Rowcliffe J Marcus 2014; M. 2022) allows users to estimate uncertainty (i.e. confidence intervals) around the activity patterns using bootstrap approaches. Following the recommendations available in the function’s help-file and based on the number of events available in the dataset, 313, we resample from the data (argument sample = \"data\"). Some tricks are needed to store the estimates returned by the fitact function in a R object. # Estimate activity patterns and store values in an object res_acti &lt;- as.data.frame(fitact(dat_event$Time_Rad, reps = 999, sample = &quot;data&quot;, show = FALSE)@pdf[,1:3] ) %&gt;% mutate(x = x/(2*pi)*24*60, # to bring results back to the minutes scale, Package = &quot;Activity&quot;, Data = &quot;Aggregated&quot;) # store plot in an object pl_acti &lt;- ggplot(res_acti, aes(x=x, y=y, group=Data)) + geom_ribbon(aes(ymin=y-1.96*se, ymax=y+1.96*se, color=Data, fill=Data), alpha=0.3) + geom_line(aes(linetype=Data, color=Data), linewidth=1) + scale_color_manual(values = c(&quot;#CC79A7&quot;)) + scale_fill_manual(values = c(&quot;#CC79A7&quot;)) + labs(x = &quot;&quot;, y = &quot;Activity density&quot;, title = &quot;B: Activity pkg&quot;) + coord_cartesian(ylim = c(0,0.4)) + theme_minimal() + theme(legend.position = &quot;none&quot;, axis.line = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.ticks = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.text = element_text(size=8), axis.title.y = element_text(size=10,face=&quot;bold&quot;), axis.title.x = element_blank(), panel.grid.minor.y = element_blank(), panel.grid.major.y = element_blank(), panel.grid.major.x = element_line(colour = &#39;lightgrey&#39;, linetype = &#39;dashed&#39;, linewidth=0.5), panel.grid.minor.x = element_blank(), strip.text = element_text(size = 10, colour = &quot;black&quot;, face = &quot;bold&quot;) ) + scale_x_continuous(breaks=seq(0,24*60,60*4), labels=seq(0,24,4), expand = expansion(mult = c(0.02,0.02)))+ scale_y_continuous(expand = expansion(mult = c(0.02, 0.02))) # print first rows of the output head(res_acti) ## x y se Package Data ## 1 0.0000 0.05382417 0.01253495 Activity Aggregated ## 2 2.8125 0.05172594 0.01229652 Activity Aggregated ## 3 5.6250 0.04974286 0.01206740 Activity Aggregated ## 4 8.4375 0.04787144 0.01184767 Activity Aggregated ## 5 11.2500 0.04610799 0.01163750 Activity Aggregated ## 6 14.0625 0.04444861 0.01143712 Activity Aggregated 9.2.3 Using the circular package We now estimate activity using the circular package (Agostinelli and Lund 2022a; Oliveira-Santos, Zucco, and Agostinelli 2013). # prepare the specific format required by circular dat_event$Time_Rad_circ &lt;- as.circular(dat_event$Time_Rad, units = &quot;radians&quot;, template = &quot;none&quot;, zero=0, type=&quot;angles&quot;, modulo = &quot;2pi&quot;, rotation=&quot;counter&quot; ) # determine bandwidth bw2 &lt;- bw.cv.ml.circular(dat_event$Time_Rad_circ, kernel = &quot;vonmises&quot;) # estimate activity patterns temp2 &lt;- modal.region.circular(dat_event$Time_Rad_circ, kernel = &quot;vonmises&quot;, q = 0.5, adjust = 1, bw = bw2 ) # organize the output in a dataframe to match those of the other packages res_circ &lt;- data.frame(x=temp2$density$x, y=temp2$density$y ) %&gt;% mutate(x = x/(2*pi)*24*60, # to bring results back to the minute scale, se = NA, Package = &quot;Circular&quot;, Data = &quot;Aggregated&quot; ) # extra step specific to this package: to extract the information related to the core areas of activity res_area1 &lt;- res_circ %&gt;% filter(x&gt;temp2$zeros[1,1]/(2*pi)*24*60 &amp; x&lt;temp2$zeros[1,2]/(2*pi)*24*60) res_area2 &lt;- res_circ %&gt;% filter(x&gt;temp2$zeros[2,1]/(2*pi)*24*60 &amp; x&lt;temp2$zeros[2,2]/(2*pi)*24*60) # store plot in an object pl_circ &lt;- ggplot(res_circ, aes(x=x, y=y, group=Data)) + geom_line(aes(linetype=Data, color=Data), linewidth=1) + geom_area(data=res_area1, aes(x=x, y=y), col=&quot;#CC79A7&quot;, fill=&quot;#CC79A7&quot;, alpha=0.3) + geom_area(data=res_area2, aes(x=x, y=y), col=&quot;#CC79A7&quot;, fill=&quot;#CC79A7&quot;, alpha=0.3) + scale_color_manual(values = c(&quot;#CC79A7&quot;)) + labs(x = &quot;&quot;, y = &quot;Kernel Density Estimate&quot;, title = &quot;C: Circular pkg&quot;) + coord_cartesian(ylim = c(0,0.4)) + theme_minimal() + theme(legend.position = &quot;inside&quot;, legend.position.inside = c(0.25, 0.9), axis.line = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.ticks = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.text = element_text(size=8), axis.title.y = element_text(size=10,face=&quot;bold&quot;), axis.title.x = element_blank(), panel.grid.minor.y = element_blank(), panel.grid.major.y = element_blank(), panel.grid.major.x = element_line(colour = &#39;lightgrey&#39;, linetype = &#39;dashed&#39;, linewidth=0.5), panel.grid.minor.x = element_blank(), strip.text = element_text(size = 10, colour = &quot;black&quot;, face = &quot;bold&quot;) ) + scale_x_continuous(breaks=seq(0,24*60,60*4), labels=seq(0,24,4), expand = expansion(mult = c(0.02,0.02)))+ scale_y_continuous(expand = expansion(mult = c(0.02, 0.02))) # print the first few rows of the output head(res_circ) ## x y se Package Data ## 1 0.000000 0.05359030 NA Circular Aggregated ## 2 2.291831 0.05187664 NA Circular Aggregated ## 3 4.583662 0.05024021 NA Circular Aggregated ## 4 6.875494 0.04867913 NA Circular Aggregated ## 5 9.167325 0.04719142 NA Circular Aggregated ## 6 11.459156 0.04577500 NA Circular Aggregated 9.2.4 Comparing the results and plotting We now can visually compare the estimates of activity patterns returned by the three packages. # Combine results of the 3 methods (pl &lt;- grid.arrange(pl_over, pl_acti, pl_circ, nrow = 1, bottom=textGrob(&quot;Time of day (Hour)&quot;, gp=gpar(fontsize=10, fontface=&quot;bold&quot;)), top = textGrob(&quot;KDE-based estimates of activity patterns using:&quot;,gp=gpar(fontsize=20,font=3)) ) ) Figure 9.2: Activity patterns based on Kernel Density Estimates of American black bear camera-trap data collected in Northern-Minnesota between mid-May and mid-July 2017. Estimates were obtained using R-package: A) overlap (Ridout and Linkie 2009); B) activity (Rowcliffe et al. 2014); C) circular (Agostinelli and Lund 2017). We removed records less than 30 minutes apart to minimize short-term correlation in detection due to animals lingering in front of the cameras. Lines represent mean estimates. Shaded areas represent 95% confidence intervals in B, and activity range cores (50% isopleth) in C. ## TableGrob (3 x 3) &quot;arrange&quot;: 5 grobs ## z cells name grob ## 1 1 (2-2,1-1) arrange gtable[layout] ## 2 2 (2-2,2-2) arrange gtable[layout] ## 3 3 (2-2,3-3) arrange gtable[layout] ## 4 4 (1-1,1-3) arrange text[GRID.text.2461] ## 5 5 (3-3,1-3) arrange text[GRID.text.2462] The three packages return patterns in activity that match in the timing of activity peaks and overall shape of the curves. However, there are some differences. Notably, the y-axis associated to the estimates produced using overlap (Ridout and Linkie 2009; Meredith and Ridout 2021) differs from those returned by the other two packages. These differences are due to an additional step that takes place within the densityPlot function in overlap and converts estimates so that the area under the curve is equal to one. The activity package (Rowcliffe J Marcus 2014; M. 2022) has the additional option to estimate 95% confidence intervals using bootstrap approaches; the main function implemented in this package, named activity::fitact, also has an argument to correct for differences in detection probability (e.g., between day and night) while the function activity::solartime enable double-anchoring procedures to adjust the timestamp associated with the different records based on sunrise and sunset times (for details, see Vazquez et al. 2019). Finally, the circular package (Agostinelli and Lund 2022a; Oliveira-Santos, Zucco, and Agostinelli 2013) can identify periods of higher levels of activity using an isopleth approach similar to that applied in the analysis of telemetry data to identify core areas in estimates of home ranges. 9.3 Aggregating versus non-aggregating data in KDEs Building on the previous section 9.2, here, we illustrate the consequences of not aggregating data prior to implementing KDEs. We re-run the same code as in 9.2, but prior to data aggregation. # Specify the format of the column Time dat$Time &lt;- hms(dat$Time) # Convert time values to radians dat$Time_Rad &lt;- (hour(dat$Time)*60 + minute(dat$Time))/(60*24)*2*pi # OVERLAP # To avoid plotting pdf(file = NULL) # Estimate activity patterns and store values in an object res_over_NonAggr &lt;- densityPlot(dat$Time_Rad, rug = FALSE, adjust = 1, xscale = 24*60, n.grid = 513, extend = NULL ) %&gt;% mutate(se = NA, Package = &quot;Overlap&quot;, Data = &quot;Non-Aggregated&quot; ) dev.off() ## png ## 2 # ACTIVITY # Estimate activity patterns and store values in an object res_acti_NonAggr &lt;- as.data.frame(fitact(dat$Time_Rad, reps = 999, sample = &quot;data&quot;, show = FALSE )@pdf[,1:3] ) %&gt;% mutate(x = x/(2*pi)*24*60, # to bring results back to the minute scale, Package = &quot;Activity&quot;, Data = &quot;Non-Aggregated&quot; ) # CIRCULAR # prepare the specific format required by circular dat$Time_Rad_circ &lt;- as.circular(dat$Time_Rad, units = &quot;radians&quot;, template = &quot;none&quot;, zero=0, type=&quot;angles&quot;, modulo = &quot;2pi&quot;, rotation=&quot;counter&quot; ) # determine bandwidth bw1 &lt;- bw.cv.ml.circular(dat$Time_Rad_circ, kernel = &quot;vonmises&quot;) ## Warning in bw.cv.ml.circular(dat$Time_Rad_circ, kernel = &quot;vonmises&quot;): minimum occurred at one end of the range # estimate activity patterns temp1 &lt;- modal.region.circular(dat$Time_Rad_circ, kernel = &quot;vonmises&quot;, q = 0.5, adjust = 1, bw = bw1 ) # organize the output in a dataframe to match those of the other packages res_circ_NonAggr &lt;- data.frame(x=temp1$density$x, y=temp1$density$y ) %&gt;% mutate(x = x/(2*pi)*24*60, # to bring results back to the minute scale, se = NA, Package = &quot;Circular&quot;, Data = &quot;Non-Aggregated&quot; ) res_area3 &lt;- res_circ_NonAggr %&gt;% filter(x&gt;temp1$zeros[1,1]/(2*pi)*24*60 &amp; x&lt;temp1$zeros[1,2]/(2*pi)*24*60) res_area4 &lt;- res_circ_NonAggr %&gt;% filter(x&gt;temp1$zeros[2,1]/(2*pi)*24*60 &amp; x&lt;temp1$zeros[2,2]/(2*pi)*24*60) We plot the results based on the aggregated and non-aggregated data and visually compare them. # plot OVERLAP # Join results and plot res_over_both &lt;- rbind(res_over, res_over_NonAggr) pl_over &lt;- ggplot(res_over_both, aes(x=x, y=y, group=Data)) + geom_line(aes(linetype=Data, color=Data), size=1) + scale_color_manual(values = c(&quot;#CC79A7&quot;, &quot;#0072B2&quot;)) + labs(x = &quot;&quot;, y = &quot;Density&quot;, title = &quot;A: Overlap pkg&quot;) + theme_minimal() + theme(legend.position = &quot;none&quot;, legend.background = element_rect(fill = &quot;white&quot;, colour = &quot;black&quot;), legend.title = element_text(size=8,face=&quot;bold&quot;), axis.line = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.ticks = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), #axis.title.y = element_blank(), axis.text = element_text(size=8), axis.title.y = element_text(size=10,face=&quot;bold&quot;), axis.title.x = element_blank(), panel.grid.minor.y = element_blank(), panel.grid.major.y = element_blank(), panel.grid.major.x = element_line(colour = &#39;lightgrey&#39;, linetype = &#39;dashed&#39;, size=0.5),#element_line(colour = &#39;grey&#39;, linetype = &#39;solid&#39;, size=0.5), panel.grid.minor.x = element_blank(), strip.text = element_text(size = 10, colour = &quot;black&quot;, face = &quot;bold&quot;) ) + scale_x_continuous(breaks=seq(0,24*60,60*4), labels=seq(0,24,4), expand = expansion(mult = c(0.02,0.02)))+ scale_y_continuous(expand = expansion(mult = c(0.02, 0.02))) ## Warning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0. ## ℹ Please use the `linewidth` argument instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. # plot ACTIVITY res_acti_both &lt;- rbind(res_acti, res_acti_NonAggr) pl_acti &lt;- ggplot(res_acti_both, aes(x=x, y=y, group=Data)) + geom_ribbon(aes(ymin=y-1.96*se, ymax=y+1.96*se, color=Data, fill=Data), alpha=0.3) + geom_line(aes(linetype=Data, color=Data), size=1) + scale_color_manual(values = c(&quot;#CC79A7&quot;, &quot;#0072B2&quot;)) + scale_fill_manual(values = c(&quot;#CC79A7&quot;, &quot;#0072B2&quot;)) + labs(x = &quot;&quot;, y = &quot;Activity density&quot;, title = &quot;B: Activity pkg&quot;) + coord_cartesian(ylim = c(0,0.4)) + theme_minimal() + theme(legend.position = &quot;none&quot;, axis.line = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.ticks = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), #axis.title.y = element_blank(), axis.text = element_text(size=8), axis.title.y = element_text(size=10,face=&quot;bold&quot;), axis.title.x = element_blank(), panel.grid.minor.y = element_blank(), panel.grid.major.y = element_blank(), panel.grid.major.x = element_line(colour = &#39;lightgrey&#39;, linetype = &#39;dashed&#39;, size=0.5),#element_line(colour = &#39;grey&#39;, linetype = &#39;solid&#39;, size=0.5), panel.grid.minor.x = element_blank(), strip.text = element_text(size = 10, colour = &quot;black&quot;, face = &quot;bold&quot;) ) + scale_x_continuous(breaks=seq(0,24*60,60*4), labels=seq(0,24,4), expand = expansion(mult = c(0.02,0.02)))+ scale_y_continuous(expand = expansion(mult = c(0.02, 0.02))) # plot CIRCULAR res_circ_both &lt;- rbind(res_circ, res_circ_NonAggr) pl_circ &lt;- ggplot(res_circ_both, aes(x=x, y=y, group=Data)) + geom_line(aes(linetype=Data, color=Data), size=1) + geom_area(data=res_area1, aes(x=x, y=y), col=&quot;#CC79A7&quot;, fill=&quot;#CC79A7&quot;, alpha=0.3) + geom_area(data=res_area2, aes(x=x, y=y), col=&quot;#CC79A7&quot;, fill=&quot;#CC79A7&quot;, alpha=0.3) + geom_area(data=res_area3, aes(x=x, y=y), col=&quot;#0072B2&quot;, fill=&quot;#0072B2&quot;, alpha=0.3) + geom_area(data=res_area4, aes(x=x, y=y), col=&quot;#0072B2&quot;, fill=&quot;#0072B2&quot;, alpha=0.3) + scale_color_manual(values = c(&quot;#CC79A7&quot;, &quot;#0072B2&quot;)) + labs(x = &quot;&quot;, y = &quot;Kernel Density Estimate&quot;, title = &quot;C: Circular pkg&quot;) + coord_cartesian(ylim = c(0,0.4)) + theme_minimal() + theme(legend.position = &quot;inside&quot;, legend.position.inside = c(0.25, 0.9), axis.line = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.ticks = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.text = element_text(size=8), axis.title.y = element_text(size=10,face=&quot;bold&quot;), axis.title.x = element_blank(), panel.grid.minor.y = element_blank(), panel.grid.major.y = element_blank(), panel.grid.major.x = element_line(colour = &#39;lightgrey&#39;, linetype = &#39;dashed&#39;, size=0.5), panel.grid.minor.x = element_blank(), strip.text = element_text(size = 10, colour = &quot;black&quot;, face = &quot;bold&quot;) ) + scale_x_continuous(breaks=seq(0,24*60,60*4), labels=seq(0,24,4), expand = expansion(mult = c(0.02,0.02)))+ scale_y_continuous(expand = expansion(mult = c(0.02, 0.02))) # Combine results of the 3 methods (pl_both &lt;- grid.arrange(pl_over, pl_acti, pl_circ, nrow = 1, bottom=textGrob(&quot;Time of day (Hour)&quot;, gp=gpar(fontsize=10, fontface=&quot;bold&quot;)), top = textGrob(&quot;KDE-based estimates of activity patterns using:&quot;,gp=gpar(fontsize=20,font=3)) ) ) Figure 9.3: Activity patterns based on Kernel Density Estimates of American black bear camera-trap data collected in northern-Minnesota between mid-May and mid-July 2017. Estimates were obtained using R-package: A) overlap (Ridout and Linkie 2009); B) activity (Rowcliffe et al. 2014); C) circular (Agostinelli and Lund 2017). Non-aggregated data included all photographic records; in the aggregated data, we removed records less than 30 minutes apart to minimize short-term correlation in detection due to animals lingering in front of the cameras. Lines represent mean estimates. Shaded areas represent 95% confidence intervals in B, and activity range cores (50% isopleth) in C. ## TableGrob (3 x 3) &quot;arrange&quot;: 5 grobs ## z cells name grob ## 1 1 (2-2,1-1) arrange gtable[layout] ## 2 2 (2-2,2-2) arrange gtable[layout] ## 3 3 (2-2,3-3) arrange gtable[layout] ## 4 4 (1-1,1-3) arrange text[GRID.text.2606] ## 5 5 (3-3,1-3) arrange text[GRID.text.2607] We see that applying KDEs to non-aggregated data leads to estimates of activity curves that are extremely wiggly due to the choice of smoothing parameter which assumes the observations are independent. This could make the interpretation of the results particularly challenging and even lead to biased conclusions when results are driven by one or two individuals staying in front of a camera for an extensive amount of time (but see Peral, Landman, and Kerley (2022) for a different perspective on this topic). 9.4 KDEs versus hierarchical models approaches In section 6.1, we use the seasonal activity patterns of black bears as an example to demonstrate how hierarchical models can be used to explore the effect of a categorical covariate on activity patterns. We repeat the same analysis here using KDEs and compare the results with the two hierarchical model approaches. We again use the full set of black bear camera-trap records collected between 2016 and 2018 at 100 locations in Northern Minnesota, USA (Iannarilli et al. 2021). However, we re-run the hierarchical model-based analysis using the aggregated version of the dataset, to facilitate comparison with KDE-based results. # Load additional Libraries library(GLMMadaptive) library(mgcv) library(tidyr) # Load data dat &lt;- read.csv(&quot;data_input/species_records.csv&quot;) %&gt;% filter(Species == &quot;BlackBear&quot;) %&gt;% droplevels() %&gt;% mutate(DateTimeOriginal = ymd_hm(DateTimeOriginal)) cov &lt;- read.csv(&quot;data_input/CameraTrapProject_CT_data_for_analysis_MASTER.csv&quot;, as.is = TRUE) %&gt;% select(c(Session, Site, Date_setup, Date_retr, Problem1_from, Problem1_to)) %&gt;% mutate(Date_setup = mdy(Date_setup), Date_retr = mdy(Date_retr), Problem1_from = mdy(Problem1_from), Problem1_to = mdy(Problem1_to) ) # Extract Season information from Session column dat$Season &lt;- as.factor(substr(dat$Session, 1, 1)) # Aggregate data to reduce short-term temporal dependence independence_interval &lt;- 30 # minutes dat &lt;- dat %&gt;% group_by(Species, Station, Season) %&gt;% # group by species, location, and season (to retain information in the dataset) arrange(DateTimeOriginal, .by_group = TRUE) %&gt;% # arrange record temporally within each group mutate(timediff = as.numeric(difftime(DateTimeOriginal, lag(DateTimeOriginal), units=&quot;mins&quot;)), # calculate time difference (in minutes) between consecutive events timediff = replace_na(timediff, 0), grp = ifelse(timediff &gt;= independence_interval, 1, 0), # create events grp_ind = cumsum(grp) + 1) # assign a group number to each event # group by camera site and event group # summarize: consider time of first image as time of the independent event dat &lt;- dat %&gt;% group_by(Species, Station, Season, grp_ind) %&gt;% slice(1) %&gt;% select(&quot;Station&quot;, &quot;Species&quot;, &quot;DateTimeOriginal&quot;, &quot;Date&quot;, &quot;Time&quot;, &quot;Session&quot;) ## Adding missing grouping variables: `Season`, `grp_ind` table(dat$Season) ## ## F S ## 236 809 # Merge time of deployment and retrieval + problems site &lt;- cov site$end &lt;- ymd(&quot;2000-01-01&quot;) for(i in 1:nrow(site)){ site$end[i] &lt;- min(site$Date_retr[i], site$Problem1_from[i], na.rm = TRUE) } # Create dataframe to store captures occasions &lt;- vector(&quot;list&quot;, length = nrow(site)) for(i in 1:nrow(site)){ occasions[[i]] &lt;- data.frame(Session = site$Session[i], Site = site$Site[i], start = seq(from = ymd_hms(paste(site$Date_setup[i], &quot;00:00:00&quot;, sep = &quot; &quot;)), to = ymd_hms(paste(site$end[i], &quot;23:59:59&quot;, sep = &quot; &quot;)), by = &#39;60 min&#39;) ) %&gt;% mutate(end = c(start[2:length(start)], start[length(start)]+minutes(60))) } occasions &lt;- do.call(rbind.data.frame, occasions) occasions$capt &lt;- 0 # Store captures for(i in 1:nrow(dat)){ occasions[occasions$Session == as.character(dat$Session[i]) &amp; occasions$Site == as.character(dat$Station[i]) &amp; occasions$start &lt;= dat$DateTimeOriginal[i] &amp; occasions$end &gt; dat$DateTimeOriginal[i], &quot;capt&quot;] &lt;- 1 } # Format data occasions$Time &lt;- hour(occasions$start) occasions$Season &lt;- as.factor(substr(occasions$Session, 1, 1)) table(occasions$Season, occasions$capt) ## ## 0 1 ## F 222248 232 ## S 347954 742 occasions$Site &lt;- as.factor(occasions$Site) # format data for cbind(success, failure) occasions_cbind &lt;- occasions %&gt;% group_by(Site, Time, Season) %&gt;% summarize(success = sum(capt), failure = n() - success ) ## `summarise()` has grouped output by &#39;Site&#39;, &#39;Time&#39;. You can override using the `.groups` argument. There is a minor mismatch between the number of encounters in the occassions data set (742 in the Spring, 232 in the Fall), and the number of independent encounters (809 in the Spring, 236 in the Fall). This discrepancy is due to a few independent encounters that are more than 30 minutes apart (the time threshold used to define independence) but that still fall within the same hourly time interval (e.g., if their datetime values are ‘2021-04-15 13:06:08’ and ‘2021-04-15 13:46:09’). As before, for the KDE analysis, we convert the time stamp to radians. We also split the dataset by Season and run KDEs for the two groups using a loop. # Estimate activity using KDE dat$Time &lt;- hms(dat$Time) dat$Time_Rad &lt;- (hour(dat$Time)*60 + minute(dat$Time))/(60*24)*2*pi # Split by Season dat$Season &lt;- as.factor(substr(dat$Session, 1, 1)) dat_ls &lt;- dat %&gt;% group_by(Species, Season) %&gt;% group_split() KDE_ls &lt;- vector(&quot;list&quot;, length = length(dat_ls)) for(i in 1:length(dat_ls)){ KDE_ls[[i]] &lt;- as.data.frame(fitact(dat_ls[[i]]$Time_Rad, reps = 999, sample = &quot;model&quot;, show = FALSE)@pdf[,1:3]) KDE_ls[[i]]$Species &lt;- unique(dat_ls[[i]]$Species) KDE_ls[[i]]$Season &lt;- unique(dat_ls[[i]]$Season) } KDE &lt;- do.call(rbind.data.frame, KDE_ls) # store plot pl_KDE &lt;- ggplot(KDE, aes(x = x/(2*pi)*24, y = y)) + geom_ribbon(aes(ymin = y - 1.96*se, ymax = y +1.96*se, color = Season, fill = Season), alpha = 0.3, linewidth = 0.25) + geom_line(aes(color = Season), linewidth = 1) + theme_minimal() + scale_color_manual(values = c(&quot;orange&quot;, &quot;darkgreen&quot;)) + scale_fill_manual(values = c(&quot;orange&quot;, &quot;darkgreen&quot;)) + labs(x = &quot;Time of Day (Hour)&quot;, y = &quot;Predicted Activity Pattern \\n (density) \\n&quot;, title = &quot;C: KDEs, \\nsplitting data by Season&quot;)+ theme_minimal()+ theme(legend.position = &quot;none&quot;, legend.title = element_blank(), legend.text = element_text(size=10,face=&quot;bold&quot;), legend.margin=margin(0,0,0,0), legend.box.margin=margin(-5,-10,-10,-10), plot.title = element_text(size=10,face=&quot;bold&quot;), axis.line = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.ticks = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.title = element_text(size=9,face=&quot;bold&quot;), panel.grid.minor.y = element_blank(), panel.grid.major.y = element_blank(), panel.grid.major.x = element_line(colour = &#39;lightgrey&#39;, linetype = &#39;dashed&#39;, linewidth = 0.5), panel.grid.minor.x = element_blank() ) + scale_x_continuous(breaks=seq(0,24,4), labels=seq(0,24,4)) Next, we re-run the analysis using the hierarchical model approach. In particular, we fit a trigonometric random intercept-only model and cyclic cubic spline with a smoother for Time that depends on Season. Both models account for variability in site-use, but not for variability in timing of activity. # trig GLMM random intercept-only # run model trig_rand_int &lt;- mixed_model(fixed = cbind(success, failure) ~ cos(2 * pi * Time/24) * Season + sin(2 * pi * Time/24) * Season + sin(2 * pi * Time/12) * Season + cos(2 * pi * Time/12) * Season, random = ~ 1 | Site, data = occasions_cbind, family = binomial(), iter_EM = 0 ) summary(trig_rand_int) ## ## Call: ## mixed_model(fixed = cbind(success, failure) ~ cos(2 * pi * Time/24) * ## Season + sin(2 * pi * Time/24) * Season + sin(2 * pi * Time/12) * ## Season + cos(2 * pi * Time/12) * Season, random = ~1 | Site, ## data = occasions_cbind, family = binomial(), iter_EM = 0) ## ## Data Descriptives: ## Number of Observations: 4800 ## Number of Groups: 100 ## ## Model: ## family: binomial ## link: logit ## ## Fit statistics: ## log.Lik AIC BIC ## -2234.784 4491.567 4520.224 ## ## Random effects covariance matrix: ## StdDev ## (Intercept) 0.8768821 ## ## Fixed effects: ## Estimate Std.Err z-value p-value ## (Intercept) -7.3440 0.1200 -61.2180 &lt; 1e-04 ## cos(2 * pi * Time/24) 0.4455 0.1112 4.0064 &lt; 1e-04 ## SeasonS 0.7232 0.0880 8.2204 &lt; 1e-04 ## sin(2 * pi * Time/24) -0.5206 0.0994 -5.2347 &lt; 1e-04 ## sin(2 * pi * Time/12) -0.1465 0.0995 -1.4727 0.14082648 ## cos(2 * pi * Time/12) -0.2416 0.1002 -2.4127 0.01583330 ## cos(2 * pi * Time/24):SeasonS -0.6815 0.1292 -5.2762 &lt; 1e-04 ## SeasonS:sin(2 * pi * Time/24) 0.3082 0.1109 2.7798 0.00543997 ## SeasonS:sin(2 * pi * Time/12) -0.4753 0.1150 -4.1340 &lt; 1e-04 ## SeasonS:cos(2 * pi * Time/12) -0.3853 0.1155 -3.3371 0.00084643 ## ## Integration: ## method: adaptive Gauss-Hermite quadrature rule ## quadrature points: 11 ## ## Optimization: ## method: quasi-Newton ## converged: TRUE # build estimate of activity newdat &lt;- with(occasions_cbind, expand.grid(Time = seq(min(Time), 24, length.out = 48), Season = levels(Season) ) ) pred_rand_int &lt;- effectPlotData(trig_rand_int, newdat, marginal = FALSE) # store plot pl_trig &lt;- ggplot(pred_rand_int, aes(Time, plogis(pred))) + geom_ribbon(aes(ymin = plogis(low), ymax = plogis(upp), color = Season, fill = Season), alpha = 0.3, linewidth = 0.25) + geom_line(aes(color = Season), linewidth = 1) + scale_color_manual(values = c(&quot;orange&quot;, &quot;darkgreen&quot;)) + scale_fill_manual(values = c(&quot;orange&quot;, &quot;darkgreen&quot;)) + labs(x = &quot;Time of Day (Hour)&quot;, y = &quot;Predicted Activity Pattern \\n (probability)&quot;, title = &quot;A: Hierarchical model, \\nTrigonometric GLMM&quot;)+ theme_minimal()+ theme(legend.position = &quot;bottom&quot;, legend.title = element_blank(), legend.text = element_text(size=10,face=&quot;bold&quot;), legend.margin=margin(0,0,0,0), legend.box.margin=margin(-5,-10,-10,-10), plot.title = element_text(size=10,face=&quot;bold&quot;), axis.line = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.ticks = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.title = element_text(size=9,face=&quot;bold&quot;), panel.grid.minor.y = element_blank(), panel.grid.major.y = element_blank(), panel.grid.major.x = element_line(colour = &#39;lightgrey&#39;, linetype = &#39;dashed&#39;, linewidth=0.5), panel.grid.minor.x = element_blank(), strip.text = element_text(size = 9, colour = &quot;black&quot;, face = &quot;bold&quot;, hjust = 0) ) + scale_x_continuous(breaks=seq(0,23,length.out=7), labels=seq(0,24,4)) # Cyclic cubic spline HGAMs mod_cycl &lt;- bam(cbind(success, failure) ~ s(Time, bs = &quot;cc&quot;, k = 12, by = Season, m = 1) + Season + s(Site, bs=&quot;re&quot;), knots = list(Time=c(0,23)), family = &quot;binomial&quot;, data = occasions_cbind ) summary(mod_cycl) ## ## Family: binomial ## Link function: logit ## ## Formula: ## cbind(success, failure) ~ s(Time, bs = &quot;cc&quot;, k = 12, by = Season, ## m = 1) + Season + s(Site, bs = &quot;re&quot;) ## ## Parametric coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -7.26263 0.11444 -63.461 &lt; 2e-16 *** ## SeasonS 0.65472 0.08917 7.342 2.1e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df Chi.sq p-value ## s(Time):SeasonF 4.179 10 63.48 &lt;2e-16 *** ## s(Time):SeasonS 8.672 10 237.07 &lt;2e-16 *** ## s(Site) 81.592 99 591.31 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.238 Deviance explained = 31.8% ## fREML = 6776.2 Scale est. = 1 n = 4800 # Predict activity patterns newdat &lt;- with(occasions_cbind, expand.grid(Time = seq(min(Time), max(Time), 1), Season = levels(Season), Site = &quot;7B&quot; #Station doesn&#39;t matter ) ) cycl_pred &lt;- predict.bam(mod_cycl, newdata = newdat, exclude = &quot;s(Site)&quot;, se.fit = TRUE, type = &quot;response&quot; ) cycl_pred &lt;- cbind(newdat, fit=cycl_pred$fit, se.fit=cycl_pred$se.fit, Model = &quot;Random-intercept only&quot; ) pl_cycl &lt;- ggplot(cycl_pred, aes(Time, fit)) + geom_ribbon(aes(ymin = fit-1.96*se.fit, ymax = fit+1.96*se.fit, color = Season, fill = Season), alpha = 0.3, size = 0.25) + geom_line(aes(color = Season), size = 1) + scale_color_manual(values = c(&quot;orange&quot;, &quot;darkgreen&quot;)) + scale_fill_manual(values = c(&quot;orange&quot;, &quot;darkgreen&quot;)) + labs(x = &quot;Time of Day (Hour)&quot;, y = &quot;Predicted Activity Pattern \\n (probability)&quot;, title = &quot;B: Hierarchical model, \\nCyclic cubic spline HGAM&quot;)+ coord_cartesian(ylim = c(0, 0.005)) + theme_minimal()+ theme(legend.position = &quot;bottom&quot;, legend.title = element_blank(), legend.text = element_text(size=10,face=&quot;bold&quot;), legend.margin=margin(0,0,0,0), legend.box.margin=margin(-5,-10,-10,-10), plot.title = element_text(size=10,face=&quot;bold&quot;), axis.line = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.ticks = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.title = element_text(size=9,face=&quot;bold&quot;), panel.grid.minor.y = element_blank(), panel.grid.major.y = element_blank(), panel.grid.major.x = element_line(colour = &#39;lightgrey&#39;, linetype = &#39;dashed&#39;, linewidth=0.5), panel.grid.minor.x = element_blank(), strip.text = element_text(size = 9, colour = &quot;black&quot;, face = &quot;bold&quot;, hjust = 0) ) + scale_x_continuous(breaks=seq(0,max(cycl_pred$Time),length.out=7), labels=seq(0,24,4)) We now bring together the results based on the three methods and plot them. plotcompare &lt;- ggarrange(pl_trig, pl_cycl, pl_KDE, ncol = 3, common.legend = FALSE, legend = &quot;none&quot;) + theme(plot.margin = margin(0.1,0.1,0.5,0.1, &quot;cm&quot;)) plotcompare ggsave(plotcompare, filename = &quot;ms_figures/Fig3_KDEs_HMs_comparison.jpg&quot;, device = &quot;jpeg&quot;, units = &quot;cm&quot;, width = 24, height = 10, dpi = 600) Both the hierarchical model-based approaches and the KDEs were able to quantify differences in activity patterns across the two seasons. All of the methods also return similarly shaped activity curves for each season. However, by using the hierarchical model-based approaches, we are also able to quantify different levels of activity between Spring and Fall. Because these approaches also account for the differential sampling effort, we can interpret the results in terms of the probability that the species is active throughout the day and also compare the activity patterns across seasons. For example, based on the results, we can say that the probability of the species being active around 20:00 is higher in the Spring (green curves) than in the Fall (yellow curves). Likely owing to the assumption of independence, the KDEs were more wiggly than the hierarchical model-based estimates, and these differences might be especially prominent when there are only a few sites with records of the species. Model selection. We can compare the two hierarchical models to their relative null model to assess the importance of the variable Season on bear’s activity patterns. # trig GLMM random intercept-only # null model trig_rand_int_null &lt;- mixed_model(fixed = cbind(success, failure) ~ cos(2 * pi * Time/24) + sin(2 * pi * Time/24) + sin(2 * pi * Time/12) + cos(2 * pi * Time/12) , random = ~ 1 | Site, data = occasions_cbind, family = binomial(), iter_EM = 0 ) summary(trig_rand_int_null) ## ## Call: ## mixed_model(fixed = cbind(success, failure) ~ cos(2 * pi * Time/24) + ## sin(2 * pi * Time/24) + sin(2 * pi * Time/12) + cos(2 * pi * ## Time/12), random = ~1 | Site, data = occasions_cbind, family = binomial(), ## iter_EM = 0) ## ## Data Descriptives: ## Number of Observations: 4800 ## Number of Groups: 100 ## ## Model: ## family: binomial ## link: logit ## ## Fit statistics: ## log.Lik AIC BIC ## -2325.26 4662.52 4678.151 ## ## Random effects covariance matrix: ## StdDev ## (Intercept) 0.8604153 ## ## Fixed effects: ## Estimate Std.Err z-value p-value ## (Intercept) -6.8032 0.0984 -69.1381 &lt; 1e-04 ## cos(2 * pi * Time/24) -0.0605 0.0546 -1.1083 0.26775 ## sin(2 * pi * Time/24) -0.2503 0.0430 -5.8229 &lt; 1e-04 ## sin(2 * pi * Time/12) -0.5153 0.0489 -10.5430 &lt; 1e-04 ## cos(2 * pi * Time/12) -0.5147 0.0487 -10.5593 &lt; 1e-04 ## ## Integration: ## method: adaptive Gauss-Hermite quadrature rule ## quadrature points: 11 ## ## Optimization: ## method: quasi-Newton ## converged: TRUE AIC(trig_rand_int_null) - AIC(trig_rand_int) ## [1] 170.9523 # Cyclic cubic spline HGAMs # null model mod_cycl_null &lt;- bam(cbind(success, failure) ~ s(Site, bs=&quot;re&quot;), knots = list(Time=c(0,23)), family = &quot;binomial&quot;, data = occasions_cbind ) summary(mod_cycl_null) ## ## Family: binomial ## Link function: logit ## ## Formula: ## cbind(success, failure) ~ s(Site, bs = &quot;re&quot;) ## ## Parametric coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -6.59099 0.09096 -72.46 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df Chi.sq p-value ## s(Site) 81.13 99 565.8 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.105 Deviance explained = 18.1% ## fREML = 6842.7 Scale est. = 1 n = 4800 AIC(mod_cycl_null) - AIC(mod_cycl) ## [1] 475.2755 Both hierarchical models support an effect of Season on the activity patterns of black bear. "],["app.html", "Tutorial 10 Appendix 10.1 The sim_activity function 10.2 Binomial versus other distributions 10.3 Activity patterns in the Bayesian framework 10.4 Link to the Diel.Niche package 10.5 Info session", " Tutorial 10 Appendix 10.1 The sim_activity function Throughout the tutorial, we use the sim_activity function for simulating activity patterns. This function simulates realized activity patterns based on a ‘true’ activity pattern distribution density build as specified in equation 1 reported in Tutorial 2 and introduced in Iannarilli et al. (2024). The arguments of the function specify the following: M: Numeric. Number of sampling units (e.g. sites). J: Numeric. Number of cycles (e.g. days). wavelength: Numeric. Length of a period (i.e. distance between identical points in a trigonometric function) b0: Numeric. Intercept. b0_constant: Logic. If TRUE (default), b0 is held constant; otherwise time-varying. tau_constant: Logic. If TRUE (default), random intercept is equal to 0 and there is no variability in use among sampling units sdtau: Numeric. Only if tau_constant = FALSE. Random intercept describing variability in use among sampling units coded as a Normal distribution N(0, sdtau). b1: Numeric. Wave amplitude of first cosinusoidal curve. b2: Numeric. Wave amplitude of second cosinusoidal curve. theta0: Numeric. Curve-specific phaseshift for the first cosinusoidal curve. theta1: Numeric. Curve-specific phaseshift for the second cosinusoidal curve. n_peak: Numeric. Number of peak in activity during a period - a day - (e.g. unimodal = 1, bimodal = 2). Must be a positive integer. phaseshift_constant: Logic. Should all the sites have a peak at the same time? (Default = TRUE). sd_phaseshift: Numeric. Variability in timing of peak activity among sites (i.e. standard deviation of the phaseshift in the sinusoidal curve that determines an horizontal shift). n_sim: Numeric. Number of datasets of realized detections to create. plot_true_act: Logic. If TRUE (default), a plot of the conditional and marginal means probability of activity by occasion is returned. When the goal is simulating diel activity patterns, wavelength can be specified either as the number of hours (i.e. wavelength = 24) or number of minutes (i.e. wavelength = 1440) in the diel cycle. When b2 = 0, the function returns a curve governed only by the first cosine term, and thus a curve with a unimodal pattern (the frequency of the first cosine term is set to 24). # Load function source(&quot;source_functions/sim_activity.R&quot;) # Set equations&#39; parameters M = 100 J = 30 wavelength = 24 n_peak = 2 b0 = -3 b1 = 1 b2 = 0 theta0 = 3 theta1 = 2 sd_tau = 1 sd_gamma = 0.3 time &lt;- seq(0, 23, length.out = 100) # simulate data dat &lt;- sim_activity(M = M, J = J, wavelength = wavelength, n_peak = n_peak, n_sim = 1, b0 = b0, b0_constant = TRUE, # common intercept tau_constant = FALSE, sdtau = sd_tau, # ~site-specific intercept b1 = b1, b2 = b2, # amplitude of the cosine terms theta0 = theta0, theta1 = theta1, # common phaseshifts for the cosine terms phaseshift_constant = FALSE, sd_phaseshift = sd_gamma, # site-specific phaseshift (equal for both cosine terms) plot_true_act = TRUE) When b2 differs from zero, the function returns a bimodal pattern. The presence of two cosine terms allows us to simulate curves with the two peaks having different heights and, thus, resembling real activity patterns. # Set equations&#39; parameters (all others as used before) n_peak = 2 b1 = 0.2 b2 = 0.6 theta0 = 1 theta1 = 2 # simulate data dat &lt;- sim_activity(M = M, J = J, wavelength = wavelength, n_peak = n_peak, n_sim = 1, b0 = b0, b0_constant = TRUE, # common intercept tau_constant = FALSE, sdtau = sd_tau, # ~site-specific intercept b1 = b1, b2 = b2, # amplitude of the cosine terms theta0 = theta0, theta1 = theta1, # common phaseshifts for the cosine terms phaseshift_constant = FALSE, sd_phaseshift = sd_gamma, # site-specific phaseshift (equal for both cosine terms) plot_true_act = TRUE) Activity patterns having two peaks that are the same height can be simulated setting b1 = 0, n_peak = 2, and b2 to a value other than zero. These choices silence the first cosine term (with frequency of 24) and impose single cosine with a frequency of 12. # Set equations&#39; parameters (all others as used before) n_peak = 2 b1 = 0 b2 = 3 theta0 = 2 theta1 = 2 # simulate data dat &lt;- sim_activity(M = M, J = J, wavelength = wavelength, n_peak = n_peak, n_sim = 1, b0 = b0, b0_constant = TRUE, # common intercept tau_constant = FALSE, sdtau = sd_tau, # ~site-specific intercept b1 = b1, b2 = b2, # amplitude of the cosine terms theta0 = theta0, theta1 = theta1, # common phaseshifts for the cosine terms phaseshift_constant = FALSE, sd_phaseshift = sd_gamma, # site-specific phaseshift (equal for both cosine terms) plot_true_act = TRUE) 10.2 Binomial versus other distributions How we aggregate observations of activity determines how we model our data. Rees et al. (2024) modeled animal diel activity using a generalized additive modeling approach with a negative binomial distribution after determining there was overdispersion using a Poisson distribution. The Poisson probability density function (PDF) is a special case of the Negative Binomial. Throughout, we have used the binomial probability mass function to model diel activity in both trigonometric GLMMs and cyclic cubic splines HGAMs. The Poisson distribution is in fact a special case of the binomial when the number of ‘trials’ is very large. As such, we can gain similar inference using these different distributions, but we need to think about how sampling effort is accounted for. In the binomial application, the zeros indicate that a camera was active on a given day, but there was no detection of the species. When using the Poisson or negative binomial distribution to model the counts of detections, we need to include sampling effort (e.g., Rees et al. (2024)). We can do this as an offset within the model, which changes our inference from the expected number of counts to the expected number of counts per unit of effort (i.e. thus modeling a rate). If setup this way, results using these different probability distributions should be quite similar (depending on sample size). There however may be computational benefits modeling counts at very large sample sizes. Below, we demonstrate the equivalence between using the binomial and Poisson probability distributions when fitting animal activity data using trigonometric and cyclic spline hierarchical models. 10.2.1 Data preparation We use camera-trap records collected between 2016 and 2018 at 100 locations in Northern Minnesota, USA (Iannarilli et al. 2021). We use the same data records of American black bear (Ursus americanus) as done in the Tutorial 6.1. However, we ignore the season covariate for simplicity and just focus on modeling the Fall data. We first load the occasions_cbind object that we saved in the Tutorial 6.1, then subset the data to only include records related to the Fall season. # Load Libraries rm(list = ls()) set.seed(129) library(dplyr) library(lubridate) library(GLMMadaptive) library(mgcv) library(ggpubr) # Load data occasions_cbind &lt;- read.csv(&quot;data_output/occasions_cbind_Ursus_americanus_seasons.csv&quot;) %&gt;% mutate(Site = as.factor(Site)) # Subset the data to the Fall data only, thus removing Spring data. y_fall &lt;- occasions_cbind[which(occasions_cbind$Season == &quot;F&quot;),] head(y_fall) ## X Site Time Season success failure ## 1 1 10A 0 F 0 100 ## 3 3 10A 1 F 0 100 ## 5 5 10A 2 F 0 100 ## 7 7 10A 3 F 0 100 ## 9 9 10A 4 F 0 100 ## 11 11 10A 5 F 0 100 # Create a column of the total number of possible detections (i.e. trials) to be used # as the offset when controlling for sampling effort when using the Poisson PDF. y_fall$total &lt;- y_fall$success + y_fall$failure 10.2.2 Trigonometric PDF comparison Fit the model using a binomial distribution and then fit the equivalent count-based model with an offset to control for sampling effort. # Fit the Binomial model trig_rand_int &lt;- mixed_model(fixed = cbind(success, failure) ~ cos(2 * pi * Time/24) + sin(2 * pi * Time/24) + sin(2 * pi * Time/12) + cos(2 * pi * Time/12) , random = ~ 1 | Site, data = y_fall, family = binomial(), iter_EM = 0) summary(trig_rand_int) ## ## Call: ## mixed_model(fixed = cbind(success, failure) ~ cos(2 * pi * Time/24) + ## sin(2 * pi * Time/24) + sin(2 * pi * Time/12) + cos(2 * pi * ## Time/12), random = ~1 | Site, data = y_fall, family = binomial(), ## iter_EM = 0) ## ## Data Descriptives: ## Number of Observations: 2400 ## Number of Groups: 100 ## ## Model: ## family: binomial ## link: logit ## ## Fit statistics: ## log.Lik AIC BIC ## -727.3341 1466.668 1482.299 ## ## Random effects covariance matrix: ## StdDev ## (Intercept) 1.104286 ## ## Fixed effects: ## Estimate Std.Err z-value p-value ## (Intercept) -7.5021 0.1610 -46.6107 &lt; 1e-04 ## cos(2 * pi * Time/24) 0.4811 0.1102 4.3649 &lt; 1e-04 ## sin(2 * pi * Time/24) -0.5557 0.0989 -5.6179 &lt; 1e-04 ## sin(2 * pi * Time/12) -0.1698 0.0980 -1.7323 0.083226 ## cos(2 * pi * Time/12) -0.2360 0.0987 -2.3921 0.016751 ## ## Integration: ## method: adaptive Gauss-Hermite quadrature rule ## quadrature points: 11 ## ## Optimization: ## method: quasi-Newton ## converged: TRUE # Fit Poisson Model with log offset using the variable &#39;total&#39; trig_rand_int_P &lt;- mixed_model(fixed = success ~ cos(2 * pi * Time/24) + sin(2 * pi * Time/24) + sin(2 * pi * Time/12) + cos(2 * pi * Time/12) + offset(log(total)), random = ~ 1 | Site, data = y_fall, family = poisson(), iter_EM = 0) summary(trig_rand_int_P) ## ## Call: ## mixed_model(fixed = success ~ cos(2 * pi * Time/24) + sin(2 * ## pi * Time/24) + sin(2 * pi * Time/12) + cos(2 * pi * Time/12) + ## offset(log(total)), random = ~1 | Site, data = y_fall, family = poisson(), ## iter_EM = 0) ## ## Data Descriptives: ## Number of Observations: 2400 ## Number of Groups: 100 ## ## Model: ## family: poisson ## link: log ## ## Fit statistics: ## log.Lik AIC BIC ## -727.3675 1466.735 1482.366 ## ## Random effects covariance matrix: ## StdDev ## (Intercept) 1.101219 ## ## Fixed effects: ## Estimate Std.Err z-value p-value ## (Intercept) -7.5017 0.1606 -46.6996 &lt; 1e-04 ## cos(2 * pi * Time/24) 0.4799 0.1101 4.3582 &lt; 1e-04 ## sin(2 * pi * Time/24) -0.5540 0.0988 -5.6074 &lt; 1e-04 ## sin(2 * pi * Time/12) -0.1690 0.0979 -1.7259 0.084364 ## cos(2 * pi * Time/12) -0.2353 0.0985 -2.3876 0.016960 ## ## Integration: ## method: adaptive Gauss-Hermite quadrature rule ## quadrature points: 11 ## ## Optimization: ## method: quasi-Newton ## converged: TRUE First, note the similarity in the coefficient estimates. Next, we can predict the diel activity for each model. Note that to be equivalent, we need to consider the same predictions for 1 unit of sampling effort, thus we use ‘total = 1’. newdat &lt;- with(occasions_cbind, expand.grid(Time = seq(0, 24, length.out = 24), total=1)) pred_rand_int &lt;- effectPlotData(trig_rand_int, newdat, marginal = FALSE) pred_rand_int_P &lt;- effectPlotData(trig_rand_int_P, newdat, marginal = FALSE) Now, we can plot the activity predictions from both models. # plot 1 pl_trig1 &lt;- ggplot(pred_rand_int, aes(Time, plogis(pred))) + geom_ribbon(aes(ymin = plogis(low), ymax = plogis(upp)), alpha = 0.1, linewidth = 0.25) + geom_line(aes(), linewidth = 1) + labs(x = &quot;Time of Day (Hour)&quot;, y = &quot;Predicted Activity Pattern \\n (probability)&quot;, title = &quot;B: Trigonometric Binomial Model&quot;)+ theme_minimal()+ theme(legend.position = &quot;bottom&quot;, legend.title = element_blank(), legend.text = element_text(size=10,face=&quot;bold&quot;), legend.margin=margin(0,0,0,0), legend.box.margin=margin(-5,-10,-10,-10), plot.title = element_text(size=10,face=&quot;bold&quot;), axis.line = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.ticks = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.title = element_text(size=9,face=&quot;bold&quot;), panel.grid.minor.y = element_blank(), panel.grid.major.y = element_blank(), panel.grid.major.x = element_line(colour = &#39;lightgrey&#39;, linetype = &#39;dashed&#39;, linewidth=0.5), panel.grid.minor.x = element_blank(), strip.text = element_text(size = 9, colour = &quot;black&quot;, face = &quot;bold&quot;, hjust = 0))+ scale_x_continuous(breaks=seq(0,23,length.out=7), labels=seq(0,24,4)) # plot 2 pl_trig1_P &lt;- ggplot(pred_rand_int_P, aes(Time, plogis(pred))) + geom_ribbon(aes(ymin = plogis(low), ymax = plogis(upp)), alpha = 0.1, linewidth = 0.25) + geom_line(aes(), linewidth = 1) + labs(x = &quot;Time of Day (Hour)&quot;, y = &quot;Predicted Activity Pattern \\n (probability)&quot;, title = &quot;A: Trigonometric Poisson Model &quot;)+ theme_minimal()+ theme(legend.position = &quot;bottom&quot;, legend.title = element_blank(), legend.text = element_text(size=10,face=&quot;bold&quot;), legend.margin=margin(0,0,0,0), legend.box.margin=margin(-5,-10,-10,-10), plot.title = element_text(size=10,face=&quot;bold&quot;), axis.line = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.ticks = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.title = element_text(size=9,face=&quot;bold&quot;), panel.grid.minor.y = element_blank(), panel.grid.major.y = element_blank(), panel.grid.major.x = element_line(colour = &#39;lightgrey&#39;, linetype = &#39;dashed&#39;, linewidth=0.5), panel.grid.minor.x = element_blank(), strip.text = element_text(size = 9, colour = &quot;black&quot;, face = &quot;bold&quot;, hjust = 0))+ scale_x_continuous(breaks=seq(0,23,length.out=7), labels=seq(0,24,4)) ggarrange(pl_trig1_P, pl_trig1) We can see that the predictions are equivalent. 10.2.3 Cyclic cubic PDF comparison Now, let’s do the same comparison when fitting models in a generalized additive model framework. # Fit binomial model mod_cycl &lt;- bam(cbind(success, failure) ~ s(Time, bs = &quot;cc&quot;, k = 12) + s(Site, bs = &quot;re&quot;), knots = list(Time = c(0,23)), family = &quot;binomial&quot;, data = y_fall) # Predict activity patterns newdat &lt;- with(occasions_cbind, expand.grid(Time = seq(0, 24, length.out=24),total=1, Site = &quot;7B&quot;)) # Station doesn&#39;t matter # Create predictions and confidence intervals cycl_pred &lt;- predict.bam(mod_cycl, newdata = newdat, exclude = &quot;s(Site)&quot;, se.fit = TRUE, type = &quot;link&quot;) cycl_pred$low &lt;- cycl_pred$fit - (1.96 * cycl_pred$se.fit) cycl_pred$upp &lt;- cycl_pred$fit + (1.96 * cycl_pred$se.fit) cycl_pred$Time &lt;- newdat$Time cycl_pred &lt;- data.frame(cycl_pred) # Fit Poisson model mod_cycl_P &lt;- bam(success ~ s(Time, bs = &quot;cc&quot;, k = 12) + s(Site, bs = &quot;re&quot;) + offset(log(total)), knots = list(Time = c(0,23)), family = &quot;poisson&quot;, data = y_fall) # Create predictions and confidence intervals cycl_pred_P &lt;- predict.bam(mod_cycl_P, newdata = newdat, exclude = &quot;s(Site)&quot;, se.fit = TRUE, type = &quot;link&quot;) cycl_pred_P$low &lt;- cycl_pred_P$fit - (1.96 * cycl_pred_P$se.fit) cycl_pred_P$upp &lt;- cycl_pred_P$fit + (1.96 * cycl_pred_P$se.fit) cycl_pred_P$Time &lt;- newdat$Time cycl_pred_P &lt;- data.frame(cycl_pred_P) Now, we can plot the activity predictions from both models. # plot 1 pl_cycle1 &lt;- ggplot(cycl_pred, aes(Time, plogis(fit))) + geom_ribbon(aes(ymin = plogis(low), ymax = plogis(upp)), alpha = 0.1, linewidth = 0.25) + geom_line(aes(), linewidth = 1) + labs(x = &quot;Time of Day (Hour)&quot;, y = &quot;Predicted Activity Pattern \\n (probability)&quot;, title = &quot;A: Hierarhical GAM - Binomial&quot;)+ theme_minimal()+ theme(legend.position = &quot;bottom&quot;, legend.title = element_blank(), legend.text = element_text(size=10,face=&quot;bold&quot;), legend.margin=margin(0,0,0,0), legend.box.margin=margin(-5,-10,-10,-10), plot.title = element_text(size=10,face=&quot;bold&quot;), axis.line = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.ticks = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.title = element_text(size=9,face=&quot;bold&quot;), panel.grid.minor.y = element_blank(), panel.grid.major.y = element_blank(), panel.grid.major.x = element_line(colour = &#39;lightgrey&#39;, linetype = &#39;dashed&#39;, linewidth=0.5), panel.grid.minor.x = element_blank(), strip.text = element_text(size = 9, colour = &quot;black&quot;, face = &quot;bold&quot;, hjust = 0))+ scale_x_continuous(breaks=seq(0,23,length.out=7), labels=seq(0,24,4)) #plot 2 pl_cycle1_P &lt;- ggplot(cycl_pred, aes(Time, plogis(fit))) + geom_ribbon(aes(ymin = plogis(low), ymax = plogis(upp)), alpha = 0.1, linewidth = 0.25) + geom_line(aes(), linewidth = 1) + labs(x = &quot;Time of Day (Hour)&quot;, y = &quot;Predicted Activity Pattern \\n (probability)&quot;, title = &quot;B: Hierarhical GAM - Poisson&quot;)+ theme_minimal()+ theme(legend.position = &quot;bottom&quot;, legend.title = element_blank(), legend.text = element_text(size=10,face=&quot;bold&quot;), legend.margin=margin(0,0,0,0), legend.box.margin=margin(-5,-10,-10,-10), plot.title = element_text(size=10,face=&quot;bold&quot;), axis.line = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.ticks = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.title = element_text(size=9,face=&quot;bold&quot;), panel.grid.minor.y = element_blank(), panel.grid.major.y = element_blank(), panel.grid.major.x = element_line(colour = &#39;lightgrey&#39;, linetype = &#39;dashed&#39;, linewidth=0.5), panel.grid.minor.x = element_blank(), strip.text = element_text(size = 9, colour = &quot;black&quot;, face = &quot;bold&quot;, hjust = 0))+ scale_x_continuous(breaks=seq(0,23,length.out=7), labels=seq(0,24,4)) ggarrange(pl_cycle1_P, pl_cycle1) Again, we can see that the predictions are equivalent. 10.2.4 Not controlling for sampling effort Let’s see what happens when we don’t control for sampling effort using the Poisson PDF. # Fit Poisson model mod_cycl_P2 &lt;- bam(success ~ s(Time, bs = &quot;cc&quot;, k = 12) + s(Site, bs=&quot;re&quot;), knots = list(Time=c(0,23)), family = &quot;poisson&quot;, data = y_fall) # Create predictions and confidence intervals newdat &lt;- with(occasions_cbind, expand.grid(Time = seq(0, 24, length.out=24), Site = &quot;7B&quot;)) # Station doesn&#39;t matter cycl_pred_P2 &lt;- predict.bam(mod_cycl_P2, newdata = newdat, exclude = &quot;s(Site)&quot;, se.fit = TRUE, type = &quot;link&quot;) cycl_pred_P2$low &lt;- cycl_pred_P2$fit - (1.96 * cycl_pred_P2$se.fit) cycl_pred_P2$upp &lt;- cycl_pred_P2$fit + (1.96 * cycl_pred_P2$se.fit) cycl_pred_P2$Time &lt;- newdat$Time cycl_pred_P2 &lt;- data.frame(cycl_pred_P2) Let’s plot this model output compared to the above Poisson model where we do control for sampling effort. pl_cycle1_P2 &lt;- ggplot(cycl_pred_P2, aes(Time, plogis(fit))) + geom_ribbon(aes(ymin = plogis(low), ymax = plogis(upp)), alpha = 0.1, linewidth = 0.25) + geom_line(aes(), linewidth = 1) + labs(x = &quot;Time of Day (Hour)&quot;, y = &quot;Predicted Activity Pattern \\n (probability)&quot;, title = &quot;C: Hierarhical GAM - Poisson NO OFFSET&quot;)+ theme_minimal()+ theme(legend.position = &quot;bottom&quot;, legend.title = element_blank(), legend.text = element_text(size=10,face=&quot;bold&quot;), legend.margin=margin(0,0,0,0), legend.box.margin=margin(-5,-10,-10,-10), plot.title = element_text(size=10,face=&quot;bold&quot;), axis.line = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.ticks = element_line(colour = &#39;black&#39;, linetype = &#39;solid&#39;), axis.title = element_text(size=9,face=&quot;bold&quot;), panel.grid.minor.y = element_blank(), panel.grid.major.y = element_blank(), panel.grid.major.x = element_line(colour = &#39;lightgrey&#39;, linetype = &#39;dashed&#39;, linewidth=0.5), panel.grid.minor.x = element_blank(), strip.text = element_text(size = 9, colour = &quot;black&quot;, face = &quot;bold&quot;, hjust = 0))+ scale_x_continuous(breaks=seq(0,23,length.out=7), labels=seq(0,24,4)) ggarrange(pl_cycle1_P, pl_cycle1_P2) We can see that the activity curves have the same shape, but the magnitude on the y-axis is different. Using the offset allows us to better represent animal activity in terms of when they are active and not active. 10.3 Activity patterns in the Bayesian framework Here, we use simulated data to illustrate how to fit Bayesian versions of the cyclic cubic spline hierarchical models. In particular, we briefly demonstrate how the brms package (Bürkner 2021a) can be used to fit an HGAM with random-intercepts. 10.3.1 Data Preparation We simulate data using the same approach and the same parameters presented in Tutorial 3. As such, the simulated data are generated from the same ‘true’ activity patterns. # Load libraries and function to simulate activity patterns rm(list = ls()) set.seed(129) library(dplyr) library(nimble) library(tidyr) library(brms) ## Warning: package &#39;brms&#39; was built under R version 4.2.3 ## Warning: package &#39;Rcpp&#39; was built under R version 4.2.3 source(&quot;source_functions/sim_activity.R&quot;) source(&quot;source_functions/sim_to_minute.R&quot;) # Set equations&#39; parameters M = 100 # The number of sites J = 30 # the number of binomial trials wavelength = 24 # 24 hour period n_peak = 2 # number of activity peaks b0 = -3 b1 = 1 b2 = 0.7 theta0 = 3 theta1 = 2 sd_tau = 1 sd_gamma = 0.3 time &lt;- seq(0, wavelength, wavelength/512) dat &lt;- sim_activity(M = M, J = J, wavelength = wavelength, n_peak = n_peak, n_sim = 1, #number of simulated datasets b0 = -3, b0_constant = TRUE, # common intercept tau_constant = FALSE, sdtau = sd_tau, # ~site-specific intercept b1 = b1, b2 = b2, # amplitude of the cosine terms theta0 = theta0, theta1 = theta1, # common phaseshifts for the cosine terms phaseshift_constant = FALSE, sd_phaseshift = sd_gamma, # site-specific phaseshift (equal for both cosine terms) plot_true_act = TRUE ) # The data sim_data &lt;- as.data.frame(dat$sim_data) dim(sim_data) ## [1] 100 720 The first step is to rearrange the specific format of the data created by the sim_activity function (see also Tutorial 3). # The data y &lt;- as.data.frame(dat$sim_data) dim(y) ## [1] 100 720 # summarize aggregated data y &lt;- data.frame(id=paste(&quot;A&quot;, seq(1,M,1), sep=&quot;&quot;), y=as.data.frame(y) ) colnames(y) &lt;- c(&quot;id&quot;, paste0(&quot;S&quot;, seq(1, ncol(y)-1, 1), sep=&quot;&quot;) ) # Format data in long format y_long &lt;- pivot_longer(data = y, cols = -id, names_to = &quot;time&quot;, values_to = &quot;y&quot;) %&gt;% mutate(time=as.numeric(substr(time,2,10)), id = factor(id) ) # create variable to describe time as hour (between 0 and 23) y_long$hour &lt;- sim_to_minute(y_long$time, group = wavelength) # count successes and failures at each site-occasion occasions_cbind &lt;- y_long %&gt;% group_by(id, hour) %&gt;% summarise(success = sum(y), n_events = n(), failure = n_events - success ) %&gt;% dplyr::rename(Site = id, Time = hour ) # show first few rows of the dataset created knitr::kable(head(occasions_cbind)) Site Time success n_events failure A1 0 0 30 30 A1 1 1 30 29 A1 2 0 30 30 A1 3 1 30 29 A1 4 1 30 29 A1 5 0 30 30 10.3.2 Model Setup and Fitting We are now ready to setup the Bayesian cyclic cubic spline model using code similar to that shown in Tutorial 3. However, here we wrap our code in the brm function from the package brms (Bürkner 2021a). We also need to change our notation slightly to specify the number of successes and the number of trials using success| trials(n_events). Note that below, we only consider random intercepts, but we could also include random slopes by adding s(Time, Site, bs=\"re\"). The random intercept can be specified in one of two ways, s(Site, bs=\"re\") or (1 | Site). Note that brms functions do not allow excluding smooth terms, such as s(Site, bs=\"re\") when making predictions (see here), so the latter approach is necessary if we want to estimate the conditional mean for a typical site. We compare these two approaches, below. # Setup the model but do not fit it (i.e., chains = 0) cycl_rand_int_setup &lt;- brms::brm(bf(success| trials(n_events) ~ s(Time, bs = &quot;cc&quot;, k = wavelength) + s(Site, bs=&quot;re&quot;) ), family = &quot;binomial&quot;, data = occasions_cbind, knots = list(Time = c(0,23)), cores = 1, seed = 17, chains=0 ) ## Compiling Stan program... ## Start sampling ## the number of chains is less than 1; sampling not done cycl_rand_int_setup_alt &lt;- brms::brm(bf(success| trials(n_events) ~ s(Time, bs = &quot;cc&quot;, k = wavelength) + (1|Site) ), family = &quot;binomial&quot;, data = occasions_cbind, knots = list(Time = c(0,23)), cores = 1, seed = 17, chains=0 ) ## Compiling Stan program... ## Start sampling ## the number of chains is less than 1; sampling not done # Now fit the model cycl_rand_int &lt;- update(cycl_rand_int_setup, recompile = FALSE, newdata = occasions_cbind, chains = 3 ) ## Start sampling ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 0.002284 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 22.84 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 95.6 seconds (Warm-up) ## Chain 1: 68.467 seconds (Sampling) ## Chain 1: 164.067 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 0.00133 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 13.3 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 85.18 seconds (Warm-up) ## Chain 2: 78.689 seconds (Sampling) ## Chain 2: 163.869 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 0.001127 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 11.27 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 85.406 seconds (Warm-up) ## Chain 3: 77.766 seconds (Sampling) ## Chain 3: 163.172 seconds (Total) ## Chain 3: ## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable. ## Running the chains for more iterations may help. See ## https://mc-stan.org/misc/warnings.html#bulk-ess cycl_rand_int_alt &lt;- update(cycl_rand_int_setup_alt, recompile = FALSE, newdata = occasions_cbind, chains = 3 ) ## Start sampling ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 0.001459 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 14.59 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 87.589 seconds (Warm-up) ## Chain 1: 70.046 seconds (Sampling) ## Chain 1: 157.635 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 0.001179 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 11.79 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 84.638 seconds (Warm-up) ## Chain 2: 72.39 seconds (Sampling) ## Chain 2: 157.028 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 0.001337 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 13.37 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 76.929 seconds (Warm-up) ## Chain 3: 61.079 seconds (Sampling) ## Chain 3: 138.008 seconds (Total) ## Chain 3: ## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable. ## Running the chains for more iterations may help. See ## https://mc-stan.org/misc/warnings.html#bulk-ess 10.3.3 Model Assessment For Bayesian models, we need to investigate parameter posterior distributions to evaluate whether there is evidence they have converged and thus are reliable to be used for inference. There are plenty of ways to conduct MCMC diagnostics (e.g., Roy (2019)). A good first step, is to simply visualize the posterior distributions as ‘traceplots’ and ‘density plots’ for each chain (independent model fit) to evaluate whether 1) different chains for each parameter are largely the same (plotted on top of each other), and 2) there is good ‘mixing’ within chain for each parameter. Mixing refers to the process of how the MCMC explores the parameter space to ‘fill out’ the full posterior distribution; good mixing can generally be assessed visually where the traceplot looks like a ‘hairy caterpillar’ and density plots look smooth. For a simple introduction see Denis Valle’s How do I tell if my MCMC has converged? and for additional details, see MCMC diagnostics. #Traceplots to check convergence mcmc_plot(cycl_rand_int, type = &quot;trace&quot;) ## No divergences to plot. mcmc_plot(cycl_rand_int_alt, type = &quot;trace&quot;) ## No divergences to plot. # Look at the coefficients for the two models and note their equivalence (parameters displayed in different orders) mcmc_plot(cycl_rand_int, type = &quot;areas&quot;, prob = 0.95 ) mcmc_plot(cycl_rand_int_alt, type = &quot;areas&quot;, prob = 0.95 ) # Summary of outputs - coefficients are the same summary(cycl_rand_int) ## Family: binomial ## Links: mu = logit ## Formula: success | trials(n_events) ~ s(Time, bs = &quot;cc&quot;, k = wavelength) + s(Site, bs = &quot;re&quot;) ## Data: occasions_cbind (Number of observations: 2400) ## Draws: 3 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 3000 ## ## Smoothing Spline Hyperparameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sds(sTime_1) 0.04 0.01 0.02 0.05 1.00 559 925 ## sds(sSite_1) 0.99 0.08 0.86 1.15 1.01 370 636 ## ## Regression Coefficients: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -3.06 0.10 -3.25 -2.86 1.03 125 467 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). summary(cycl_rand_int_alt) ## Family: binomial ## Links: mu = logit ## Formula: success | trials(n_events) ~ s(Time, bs = &quot;cc&quot;, k = wavelength) + (1 | Site) ## Data: occasions_cbind (Number of observations: 2400) ## Draws: 3 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 3000 ## ## Smoothing Spline Hyperparameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sds(sTime_1) 0.03 0.01 0.02 0.05 1.00 631 1161 ## ## Multilevel Hyperparameters: ## ~Site (Number of levels: 100) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.99 0.07 0.85 1.14 1.01 382 625 ## ## Regression Coefficients: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -3.06 0.10 -3.25 -2.86 1.03 110 433 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 10.3.4 Model Predictions We use the first parameterization (with s(Site, bs=\"re\")) to explore site-level variability in the activity patterns: # Predict the mean site-level activity pattern newdata &lt;- data.frame(Time=0:23, Site = occasions_cbind$Site, n_events=1 ) preds &lt;- fitted(cycl_rand_int,scale=&quot;response&quot;,newdata=newdata) preds &lt;- data.frame(newdata,preds) head(preds) ## Time Site n_events Estimate Est.Error Q2.5 Q97.5 ## 1 0 A1 1 0.005892358 0.001505928 0.003362585 0.009176446 ## 2 1 A1 1 0.004425826 0.001169577 0.002492922 0.007027831 ## 3 2 A1 1 0.003774225 0.001005591 0.002106514 0.006022182 ## 4 3 A1 1 0.004325475 0.001148243 0.002442596 0.006882073 ## 5 4 A1 1 0.006203072 0.001630072 0.003541543 0.009727854 ## 6 5 A1 1 0.010500089 0.002700026 0.006058918 0.016462475 # Plot site level mean activity ggplot(aes(x=Time, y=Estimate, group=Site, color=Site), data=preds) + geom_line() + theme_minimal() + theme(legend.position = &quot;none&quot;) We can then approximate the marginal mean activity curve by averaging across the site-level variation. #Average across sites to get the marginal mean marginal.mean = aggregate(preds$Estimate, list(preds$Time), FUN=mean) marginal.lower = aggregate(preds$Q2.5, list(preds$Time), FUN=mean) marginal.upper = aggregate(preds$Q97.5, list(preds$Time), FUN=mean) # Plot the marginal mean and compare it to the simulated true mean plot(marginal.mean$Group.1,marginal.mean$x,type=&quot;l&quot;,lwd=3,col=1, ylim=c(0,0.25),xlab=&quot;Time&quot;,ylab=&quot;Probability of Activity&quot; ) lines(marginal.lower$Group.1,marginal.lower$x) lines(marginal.upper$Group.1,marginal.upper$x) lines(dat$Marginal[,2],dat$Marginal[,1],lwd=3,col=2) legend(&quot;topright&quot;,lwd=c(3,1,1,3),col=c(1,1,1,2), legend=c(&quot;Marginal Mean&quot;, &quot;Lower Marginal Mean&quot;,&quot;Upper Marginal Mean&quot;, &quot;True Marginal Mean&quot;) ) (#fig:fit.bayes.cc.pred2)Predicted probability of the marginal mean activity. Next, we predict the conditional mean activity using the second parameterization (with (1 | Site)). # Predict the conditional mean activity newdata2 = data.frame(Time=0:23, n_events=1 ) preds2=fitted(cycl_rand_int_alt,scale=&quot;response&quot;, newdata=newdata2, re_formula = NA ) preds2=data.frame(newdata2,preds2) head(preds2) ## Time n_events Estimate Est.Error Q2.5 Q97.5 ## 1 0 1 0.01675945 0.001930674 0.013285795 0.02077941 ## 2 1 1 0.01263057 0.001630243 0.009751392 0.01610666 ## 3 2 1 0.01076266 0.001473304 0.008214943 0.01396013 ## 4 3 1 0.01231741 0.001659573 0.009414299 0.01584803 ## 5 4 1 0.01760722 0.002216415 0.013695462 0.02226960 ## 6 5 1 0.02954183 0.003439949 0.023322471 0.03640814 # Plot the conditional mean and compare it to the simulated true mean plot(preds2$Time, preds2$Estimate,type=&quot;l&quot;,lwd=3,col=1,ylim=c(0,0.25), xlab=&quot;Time&quot;,ylab=&quot;Probability of Activity&quot; ) lines(preds2$Time,preds2$Q2.5) lines(preds2$Time,preds2$Q97.5) lines(dat$Conditional[,2],dat$Conditional[,1],lwd=3,col=2) legend(&quot;topright&quot;,lwd=c(3,1,1,3),col=c(1,1,1,2), legend=c(&quot;Conditional Mean&quot;, &quot;Lower Conditional Mean&quot;,&quot;Upper Conditional Mean&quot;, &quot;True Conditional Mean&quot;) ) (#fig:fit.bayes.cc.predict3)Predicted probability of the conditional mean activity. 10.4 Link to the Diel.Niche package In the ecology and evolution literature, animal activity is often discussed in terms of diel classifications or phenotypes, such as diurnal, nocturnal, crepuscular, or cathemeral. But, how much activity in what time period does it mean to be diurnal, nocturnal, etc.? To make comparable inference across studies, we should define these diel phenotypes mathematically so that we can make inference based on consistent definitions and thus make appropriate comparisons across species or projects. We can use the Diel.Niche R package to do exactly this. First, we use our data to estimate animal diel activity across the 24-hour period and then second, we can classify predicted activity patterns into the corresponding diel phenotype. Here, we demonstrate this approach using a hierarchical cyclic cubic spline model, but the process is the same for any model predicting 24-hour temporal activity. 10.4.1 Data We use camera-trap records collected between 2016 and 2018 at 100 locations in Northern Minnesota, USA (Iannarilli et al. 2021). We use the same data records of American black bear (Ursus americanus) as done in other tutorials and in section 10.2. However, we ignore examining a season covariate for simplicity and focus on modeling the Spring data. As before, we first load the occasions_cbind object that we saved in section 6.1, then subset the data to only include records related to the season of interest, which is now Spring. # load packages library(mgcv) library(brms) library(suncalc) ## Warning: package &#39;suncalc&#39; was built under R version 4.2.2 library(lubridate) library(Diel.Niche) library(ggplot2) # Load data occasions_cbind &lt;- read.csv(&quot;data_output/occasions_cbind_Ursus_americanus_seasons.csv&quot;)%&gt;% mutate(Site = as.factor(Site)) # Subset the data to the Spring data only, thus removing Fall data. y_spring &lt;- occasions_cbind[which(occasions_cbind$Season == &quot;S&quot;),] head(y_spring) ## X Site Time Season success failure ## 2 2 10A 0 S 0 159 ## 4 4 10A 1 S 0 159 ## 6 6 10A 2 S 0 159 ## 8 8 10A 3 S 0 159 ## 10 10 10A 4 S 0 159 ## 12 12 10A 5 S 0 159 10.4.2 Model Fitting Using the Spring data for black bears, we can now fit a hierarchical cyclic cubic spline model. cycl_rand_int_ML &lt;- bam(cbind(success, failure) ~ s(Time, bs = &quot;cc&quot;, k = 12) + s(Site, bs=&quot;re&quot;), knots = list(Time=c(0,23)), family = &quot;binomial&quot;, data = y_spring ) 10.4.3 Model Predictions Let’s now use our model to predict the conditional mean activity curve across all sites. We then use those predictions to define the diel phenotype. Below, we classify each predicted time period into twilight, daytime, and nighttime. To make sure we capture enough predictions to summarize each of these periods, we predict the probability of activity at a smaller interval than how we modeled the data (e.g., 100 intervals instead of 24). newdat &lt;- with(occasions_cbind, expand.grid(Time = seq(0, 24, length.out=100),total=1, Site = &quot;7B&quot;)) # Station doesn&#39;t matter, as &#39;site&#39; is excluded below # Get predictions and of the conditional mean cycl_pred &lt;- predict.bam(cycl_rand_int_ML, newdata = newdat, exclude = &quot;s(Site)&quot;, type = &quot;response&quot;) cycl_pred &lt;- data.frame(cycl_pred) cycl_pred$Time &lt;- newdat$Time head(cycl_pred) ## cycl_pred Time ## 1 0.0006813909 0.0000000 ## 2 0.0005470650 0.2424242 ## 3 0.0004427939 0.4848485 ## 4 0.0003636476 0.7272727 ## 5 0.0003049808 0.9696970 ## 6 0.0002628894 1.2121212 These predictions are absolute probabilities of activity, which incorporates sampling effort. We need to turn these into relative activity probabilities; therefore, assuming the species is active, these are the relative probabilities of activity during each time period. cycl_pred$cycl_pred.st = cycl_pred$cycl_pred/sum(cycl_pred$cycl_pred) 10.4.4 Twilight, daytime, and nighttime Next, we need to determine the time periods that correspond to twilight, daytime, and nighttime. To do so, we need to get the relevant astronomical periods that define these periods. # Extract sunrise and sunset times for the general study area and period. sun.times &lt;- getSunlightTimes( date = as.Date(&quot;2016-04-15&quot;), lat = 47.73 , lon = -94.55, data = NULL, keep = c(&quot;dawn&quot;, &quot;dusk&quot;, &quot;nightEnd&quot;, &quot;night&quot;), tz = &quot;America/Chicago&quot;) # Use the extracted sun times to define dawn and dusk dawn &lt;- c(hour(sun.times$nightEnd)+minute(sun.times$nightEnd)/60, hour(sun.times$dawn)+minute(sun.times$dawn)/60) dusk &lt;- c(hour(sun.times$dusk)+minute(sun.times$dusk)/60, hour(sun.times$night)+minute(sun.times$night)/60) # Create a vector that defines each time period of cycl_pred$Time into twilight (combination of dawn and dusk) # daytime, and nighttime diel.class &lt;- rep(NA,nrow(cycl_pred)) diel.class[cycl_pred$Time&gt;=dawn[1] &amp; cycl_pred$Time&lt;=dawn[2]] &lt;- &quot;Twilight&quot; diel.class[cycl_pred$Time&gt;=dusk[1] &amp; cycl_pred$Time&lt;=dusk[2]] &lt;- &quot;Twilight&quot; diel.class[cycl_pred$Time&gt;=dawn[2] &amp; cycl_pred$Time&lt;=dusk[1]] &lt;- &quot;Day&quot; # the undefined time periods are then the night diel.class[is.na(diel.class)] &lt;- &quot;Night&quot; cycl_pred$diel.class &lt;- diel.class # Sum the standardized predictions of activity by diel period # These are the relative probabilities of activity diel.summary &lt;- aggregate(cycl_pred$cycl_pred.st, by=list(diel.class=cycl_pred$diel.class), FUN=sum) diel.summary ## diel.class x ## 1 Day 0.7640512 ## 2 Night 0.0784899 ## 3 Twilight 0.1574589 10.4.5 Diel.Niche Next, we reorganize the probabilities of activity in the order used by the Diel.Niche package: twilight, daytime, nighttime. For details about the Diel.Niche package, see the Github repository and the associated manuscript. y = t(matrix(c(diel.summary$x[3], diel.summary$x[1], diel.summary$x[2] ) ) ) colnames(y)=c(&quot;P_twilight&quot;,&quot;P_daytime&quot;,&quot;P_nightime&quot;) # Use the diel niche package to extract the associated diel phenotype within the Traditional hypothesis set diel.out &lt;- Diel.Niche::posthoc.niche(y, hyp= hyp.sets(&quot;Traditional&quot;)) diel.out ## p.twi p.day p.night Hypothesis ## 1 0.1574589 0.7640512 0.0784899 Cathemeral Traditional The corresponding diel phenotype for the conditional mean activity is the Cathemeral phenotype. Lastly, lets plot the conditional mean activity with the supported diel phenotype for these results. plot(cycl_pred$Time,cycl_pred$cycl_pred,type=&quot;l&quot;,col=2,lwd=3, main=paste(&quot;Spring Black Bear Activity \\n&quot;,diel.out$Hypothesis), ylab=&quot;Probability of Activity&quot;,xlab=&quot;Time&quot;) abline(v=c(dawn[1],dawn[2]),lty=3) abline(v=c(dusk[1],dusk[2]),lty=3) The vertical dotted lines indicate the periods of dawn and dusk for this sampling period. As we can see, black bears are mostly active during the day with a relative probability activity of 0.76. They are also have a high relative activity during twilight with a probability of 0.16. Interestingly, if we use a set of diel phenotype hypotheses that are more specific in separating bimodal and trimodal activity (i.e. General hypothesis set; see definitions at Github repository) we would find the correspondent diel phenotype to be better described as Diurnal-Crepuscular. Diel.Niche::posthoc.niche(y, hyp= hyp.sets(&quot;General&quot;)) ## p.twi p.day p.night Hypothesis ## 1 0.1574589 0.7640512 0.0784899 Diurnal-Crepuscular This difference in classification is because of the low amount of activity during the nighttime. It is hard to describe this activity pattern as Cathemeral, given that all three time periods (twilight, daytime, and nighttime) are not used. More specifically, black bears are concentrating their activity during twilight and daytime, thus the more accurate description would be Diurnal-Crepuscular. 10.4.6 Conclusion Modeling animal activity patterns continuously throughout the 24-hour period provides important fine scale inference that is not achieved when aggregating data into blocks of time (e.g., daytime vs nighttime). However, it is important to then still be able to translate findings in relevant terminology based on diel phenotypes. The Diel.Niche R package provides an efficient way of summarizing findings from hierarchical trigonometric and cyclic cubic spline models. Here, we provided a simple example to demonstrate the process of using the Diel.Niche R package. However, there are a number of options we could also consider. For example, we could predict the diel phenotype for each site. This is most useful when we have site-to-site variation in terms of a random intercept and slope. We could also use covariates in our model and predict the supported diel phenotype across covariate values. Furthermore, we only predicted the corresponding diel phenotype using the estimated conditional mean activity. There is of course uncertainty associated with this mean activity level. To explore how supported the diel phenotype is when considering uncertainty from the conditional mean, we could predict activity curves at different levels of uncertainty or at certain confidence intervals to investigate uncertainty in the associated diel phenotype. 10.5 Info session To facilitate reproducibility, we report information on packages and package versions used to create these tutorials. library(grateful) ## Warning: package &#39;grateful&#39; was built under R version 4.2.3 pkgs &lt;- cite_packages(output = &quot;table&quot;, out.dir = &quot;.&quot;) knitr::kable(pkgs) Package Version Citation activity 1.3.4 Rowcliffe (2023) base 4.2.1 R Core Team (2022) bookdown 0.39 Xie (2016b); Xie (2024b) brms 2.21.0 Bürkner (2017); Bürkner (2018); Bürkner (2021b) circular 0.4.95 Agostinelli and Lund (2022b) Diel.Niche 0.0.1.0 Gerber et al. (2023) distill 1.6 Dervieux et al. (2023) ggpubr 0.6.0 Kassambara (2023) GLMMadaptive 0.9.1 Rizopoulos (2023) gridExtra 2.3 Auguie (2017) knitr 1.46 Xie (2014); Xie (2015); Xie (2024c) lmtest 0.9.40 Zeileis and Hothorn (2002) MESS 0.5.9 Ekstrøm (2022) mgcv 1.8.40 S. N. Wood (2003); S. N. Wood (2004); S. N. Wood (2011); S. N. Wood et al. (2016); S. N. Wood (2017) nimble 0.12.2 de Valpine et al. (2017); de Valpine et al. (2022b); de Valpine et al. (2022a) overlap 0.3.9 Meredith, Ridout, and Campbell (2024) rmarkdown 2.27 Xie, Allaire, and Grolemund (2018); Xie, Dervieux, and Riederer (2020); Allaire et al. (2024) suncalc 0.5.1 Thieurmel and Elmarhraoui (2022) tidyverse 1.3.1 Wickham et al. (2019) sessionInfo() ## R version 4.2.1 (2022-06-23 ucrt) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 19045) ## ## Matrix products: default ## ## locale: ## [1] LC_COLLATE=English_United States.utf8 LC_CTYPE=English_United States.utf8 ## [3] LC_MONETARY=English_United States.utf8 LC_NUMERIC=C ## [5] LC_TIME=English_United States.utf8 ## ## attached base packages: ## [1] grid stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] grateful_0.2.4 Diel.Niche_0.0.1.0 suncalc_0.5.1 brms_2.21.0 Rcpp_1.0.10 ## [6] nimble_0.12.2 circular_0.4-95 overlap_0.3.9 suntools_1.0.0 activity_1.3.4 ## [11] forcats_0.5.1 lmtest_0.9-40 zoo_1.8-10 lubridate_1.9.3 tidyr_1.3.1 ## [16] mgcv_1.8-40 nlme_3.1-157 ggpubr_0.6.0 GLMMadaptive_0.9-1 ggplot2_3.5.1 ## [21] gridExtra_2.3 dplyr_1.1.4 ## ## loaded via a namespace (and not attached): ## [1] backports_1.4.1 systemfonts_1.0.6 plyr_1.8.7 igraph_1.3.4 ## [5] lazyeval_0.2.2 splines_4.2.1 rstantools_2.3.1 inline_0.3.19 ## [9] digest_0.6.29 htmltools_0.5.8.1 fansi_1.0.3 magrittr_2.0.3 ## [13] checkmate_2.3.1 sfsmisc_1.1-18 mosaicCore_0.9.2.1 RcppParallel_5.1.5 ## [17] matrixStats_1.3.0 timechange_0.3.0 colorspace_2.0-3 textshaping_0.3.7 ## [21] haven_2.5.0 xfun_0.44 crayon_1.5.2 MESS_0.5.9 ## [25] jsonlite_1.8.8 Rglpk_0.6-5.1 glue_1.6.2 geepack_1.3.9 ## [29] polyclip_1.10-4 gtable_0.3.5 V8_4.2.1 distributional_0.4.0 ## [33] car_3.1-2 ggformula_0.10.2 pkgbuild_1.4.4 rstan_2.32.6 ## [37] multinomineq_0.2.6 abind_1.4-5 scales_1.3.0 mvtnorm_1.2-4 ## [41] DBI_1.2.2 geeM_0.10.1 rstatix_0.7.2 viridisLite_0.4.2 ## [45] units_0.8-5 ggstance_0.3.6 proxy_0.4-27 stats4_4.2.1 ## [49] StanHeaders_2.32.6 httr_1.4.7 htmlwidgets_1.6.4 posterior_1.5.0 ## [53] ellipsis_0.3.2 pkgconfig_2.0.3 loo_2.5.1 farver_2.1.2 ## [57] sass_0.4.9 utf8_1.2.2 tidyselect_1.2.1 labeling_0.4.3 ## [61] rlang_1.1.3 reshape2_1.4.4 munsell_0.5.1 tools_4.2.1 ## [65] cachem_1.0.6 cli_3.6.1 generics_0.1.3 broom_1.0.5 ## [69] ggridges_0.5.6 evaluate_0.23 stringr_1.5.1 fastmap_1.2.0 ## [73] yaml_2.3.8 ragg_1.3.2 knitr_1.46 purrr_1.0.2 ## [77] slam_0.1-50 compiler_4.2.1 bayesplot_1.11.1 rstudioapi_0.16.0 ## [81] plotly_4.10.4 curl_4.3.2 e1071_1.7-14 ggsignif_0.6.4 ## [85] tibble_3.2.1 tweenr_2.0.2 bslib_0.7.0 stringi_1.7.8 ## [89] highr_0.10 Brobdingnag_1.2-9 lattice_0.20-45 Matrix_1.6-5 ## [93] classInt_0.4-10 tensorA_0.36.2.1 vctrs_0.6.5 pillar_1.9.0 ## [97] lifecycle_1.0.4 jquerylib_0.1.4 bridgesampling_1.1-2 data.table_1.15.4 ## [101] cowplot_1.1.3 QuickJSR_1.1.3 R6_2.5.1 renv_1.0.3 ## [105] bookdown_0.39 KernSmooth_2.23-20 codetools_0.2-18 boot_1.3-28 ## [109] MASS_7.3-57 withr_3.0.0 parallel_4.2.1 hms_1.1.1 ## [113] terra_1.7-71 quadprog_1.5-8 labelled_2.10.0 coda_0.19-4.1 ## [117] class_7.3-20 rmarkdown_2.27 carData_3.0-5 sf_1.0-16 ## [121] ggforce_0.4.1 `r if (knitr:::is_html_output()) ’ "],["references.html", "References", " References Agostinelli, C., and U. Lund. 2022a. R Package circular: Circular Statistics (Version 0.4-95). CA: Department of Environmental Sciences, Informatics; Statistics, Ca’ Foscari University, Venice, Italy. UL: Department of Statistics, California Polytechnic State University, San Luis Obispo, California, USA. https://r-forge.r-project.org/projects/circular/. ———. 2022b. R Package circular: Circular Statistics (Version 0.4-95). CA: Department of Environmental Sciences, Informatics; Statistics, Ca’ Foscari University, Venice, Italy. UL: Department of Statistics, California Polytechnic State University, San Luis Obispo, California, USA. https://r-forge.r-project.org/projects/circular/. Allaire, JJ, Yihui Xie, Christophe Dervieux, Jonathan McPherson, Javier Luraschi, Kevin Ushey, Aron Atkins, et al. 2024. rmarkdown: Dynamic Documents for r. https://github.com/rstudio/rmarkdown. Auguie, Baptiste. 2017. gridExtra: Miscellaneous Functions for “Grid” Graphics. https://CRAN.R-project.org/package=gridExtra. Bates, Douglas, Martin Mächler, Ben Bolker, and Steve Walker. 2015. “Fitting Linear Mixed-Effects Models Using lme4.” Journal of Statistical Software 67 (1): 1–48. https://doi.org/10.18637/jss.v067.i01. Bürkner, Paul-Christian. 2017. “brms: An R Package for Bayesian Multilevel Models Using Stan.” Journal of Statistical Software 80 (1): 1–28. https://doi.org/10.18637/jss.v080.i01. ———. 2018. “Advanced Bayesian Multilevel Modeling with the R Package brms.” The R Journal 10 (1): 395–411. https://doi.org/10.32614/RJ-2018-017. ———. 2021a. “Bayesian Item Response Modeling in R with brms and Stan.” Journal of Statistical Software. ———. 2021b. “Bayesian Item Response Modeling in R with brms and Stan.” Journal of Statistical Software 100 (5): 1–54. https://doi.org/10.18637/jss.v100.i05. Burnham, Kenneth P, and David R Anderson. 2002. Model Selection and Multimodel Inference. 2nd ed. New York, NY: Springer. Burton, A Cole, Eric Neilson, Dario Moreira, Andrew Ladle, Robin Steenweg, Jason T Fisher, Erin Bayne, and Stan Boutin. 2015. “Wildlife Camera Trapping: A Review and Recommendations for Linking Surveys to Ecological Processes.” J. Appl. Ecol. 52 (3): 675–85. de Valpine, Perry, Christopher Paciorek, Daniel Turek, Nick Michaud, Cliff Anderson-Bergman, Fritz Obermeyer, Claudia Wehrhahn Cortes, Abel Rodrìguez, Duncan Temple Lang, and Sally Paganin. 2022a. NIMBLE User Manual (version 0.12.2). https://doi.org/10.5281/zenodo.1211190. ———. 2022b. NIMBLE: MCMC, Particle Filtering, and Programmable Hierarchical Modeling (version 0.12.2). https://doi.org/10.5281/zenodo.1211190. de Valpine, Perry, Daniel Turek, Christopher Paciorek, Cliff Anderson-Bergman, Duncan Temple Lang, and Ras Bodik. 2017. “Programming with Models: Writing Statistical Algorithms for General Model Structures with NIMBLE.” Journal of Computational and Graphical Statistics 26: 403–13. https://doi.org/10.1080/10618600.2016.1172487. Dervieux, Christophe, JJ Allaire, Rich Iannone, Alison Presmanes Hill, and Yihui Xie. 2023. distill: “R Markdown” Format for Scientific and Technical Writing. https://CRAN.R-project.org/package=distill. Ekstrøm, Claus Thorn. 2022. MESS: Miscellaneous Esoteric Statistical Scripts. https://CRAN.R-project.org/package=MESS. Fieberg, John. 2024. Statistics for Ecologists: A Frequentist and Bayesian Treatment of Modern Regression Models. University of Minnesota Libraries Publishing. https://hdl.handle.net/11299/260227. Fieberg, John, Randall H Rieger, Michael C Zicus, and Jonathan S Schildcrout. 2009. “Regression Modelling of Correlated Data in Ecology: Subject-Specific and Population Averaged Response Patterns.” J. Appl. Ecol. 46 (5): 1018–25. Gerber, Brian D., Kadambari Devarajan, Zach J. Farris, and Mason Fidino. 2023. “A Model-Based Hypothesis Framework to Define and Estimate the Diel Niche via the Diel.niche’ r Package.” Journal of Animal Ecology 00: 0–15. https://doi.org/https://doi.org/10.1111/1365-2656.14035. Hedeker, Donald, Stephen H C du Toit, Hakan Demirtas, and Robert D Gibbons. 2018. “A Note on Marginalization of Regression Parameters from Mixed Models of Binary Outcomes.” Biometrics 74 (1): 354–61. Iannarilli, Fabiola. 2020. “Addressing Challenges in Camera-Trap Studies: Survey Designs for Multiple Species, Serial Dependence, and Site-to-Site Variability When Estimating Activity Patterns.” PhD thesis, Retrieved from the University of Minnesota Digital Conservancy. https://hdl.handle.net/11299/216384. Iannarilli, Fabiola, Todd W Arnold, John Erb, and John R Fieberg. 2019. “Using Lorelograms to Measure and Model Correlation in Binary Data: Applications to Ecological Studies.” Methods Ecol. Evol. 10: 2153–62. Iannarilli, Fabiola, John Erb, Todd W. Arnold, and John R. Fieberg. 2021. “Evaluating Species-Specific Responses to Camera-Trap Survey Designs.” Wildlife Biology 2021 (1). https://doi.org/10.2981/wlb.00726. Iannarilli, Fabiola, Brian D. Gerber, John Erb, and John R. Fieberg. 2024. “A ‘How-to’ Guide for Estimating Animal Diel Activity Using Hierarchical Models.” Journal of Animal Ecology. Kassambara, Alboukadel. 2023. ggpubr: “ggplot2” Based Publication Ready Plots. https://CRAN.R-project.org/package=ggpubr. Kennedy, J. R. Oakleaf, C. M., and J. Kiesecker. 2019. “Managing the Middle: A Shift in Conservation Priorities Based on the Global Human Modification Gradient.” Global Change Biology 25 (3): 811–26. https://doi.org/10.1111/gcb.14549. ———. 2020. “Global Human Modification of Terrestrial Systems.” https://doi.org/10.7927/edbc-3z60. M., Rowcliffe. 2022. Activity: Animal Activity Statistics. https://CRAN.R-project.org/package=activity. Meredith, Mike, and Martin Ridout. 2021. Overlap: Estimates of Coefficient of Overlapping for Animal Activity Patterns. https://CRAN.R-project.org/package=overlap. Meredith, Mike, Martin Ridout, and Liz A. D. Campbell. 2024. overlap: Estimates of Coefficient of Overlapping for Animal Activity Patterns. https://CRAN.R-project.org/package=overlap. N., Wood S. 2017. Generalized Additive Models: An Introduction with r. 2nd edition. Chapman; Hall/CRC. Niedballa, Jürgen, Rahel Sollmann, Alexandre Courtiol, and Andreas Wilting. 2016. “camtrapR: An r Package for Efficient Camera Trap Data Management.” Methods in Ecology and Evolution 7 (12): 1457–62. https://doi.org/10.1111/2041-210X.12600. Oliveira-Santos, L G R, C A Zucco, and C Agostinelli. 2013. “Using Conditional Circular Kernel Density Functions to Test Hypotheses on Animal Circadian Activity.” Anim. Behav. 85: 269–80. Pedersen, Eric J, David L Miller, Gavin L Simpson, and Noam Ross. 2019. “Hierarchical Generalized Additive Models in Ecology: An Introduction with Mgcv.” PeerJ 2019 (5). Peral, Christopher, Marietjie Landman, and Graham I H Kerley. 2022. “The Inappropriate Use of Time‐to‐independence Biases Estimates of Activity Patterns of Free‐ranging Mammals Derived from Camera Traps.” Ecol. Evol. 12 (10). R Core Team. 2022. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/. ———. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/. Rees, Matthew W, Brendan A Wintle, Jack H Pascoe, Mark Le Pla, Emma K Birnbaum, and Bronwyn A Hradsky. 2024. “Dynamic Shifts in Predator Diel Activity Patterns Across Landscapes and Threat Levels.” Oikos 2024 (3). Ridout, Martin, and Matthew Linkie. 2009. “Estimating Overlap of Daily Activity Patterns from Camera Trap Data.” Journal of Agricultural, Biological, and Environmental Statistics 14 (3): 322–37. Rizopoulos, Dimitris. 2022. GLMMadaptive: Generalized Linear Mixed Models Using Adaptive Gaussian Quadrature. https://CRAN.R-project.org/package=GLMMadaptive. ———. 2023. GLMMadaptive: Generalized Linear Mixed Models Using Adaptive Gaussian Quadrature. https://CRAN.R-project.org/package=GLMMadaptive. Rowcliffe J Marcus, Kranstauber Bart, Kays Roland. 2014. “Quantifying Levels of Animal Activity Using Camera Trap Data.” Methods Ecol. Evol. 5: 1170–79. Rowcliffe, Marcus. 2023. activity: Animal Activity Statistics. https://CRAN.R-project.org/package=activity. Roy, Vivekananda. 2019. “Convergence Diagnostics for Markov Chain Monte Carlo,” September. https://arxiv.org/abs/1909.11827. Thieurmel, Benoit, and Achraf Elmarhraoui. 2022. suncalc: Compute Sun Position, Sunlight Phases, Moon Position and Lunar Phase. https://CRAN.R-project.org/package=suncalc. Vazquez, Carmen, J Marcus Rowcliffe, Kamiel Spoelstra, and Patrick A Jansen. 2019. “Comparing Diel Activity Patterns of Wildlife Across Latitudes and Seasons: Time Transformations Using Day Length.” Methods Ecol. Evol. 10 (12): 2057–66. Wickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686. Wood, S. N. 2017. Generalized Additive Models: An Introduction with r. 2nd ed. Chapman; Hall/CRC. Wood, S. N. 2003. “Thin-Plate Regression Splines.” Journal of the Royal Statistical Society (B) 65 (1): 95–114. ———. 2004. “Stable and Efficient Multiple Smoothing Parameter Estimation for Generalized Additive Models.” Journal of the American Statistical Association 99 (467): 673–86. ———. 2011. “Fast Stable Restricted Maximum Likelihood and Marginal Likelihood Estimation of Semiparametric Generalized Linear Models.” Journal of the Royal Statistical Society (B) 73 (1): 3–36. Wood, S. N., N., Pya, and B. S\"afken. 2016. “Smoothing Parameter and Model Selection for General Smooth Models (with Discussion).” Journal of the American Statistical Association 111: 1548–75. Xie, Yihui. 2014. “knitr: A Comprehensive Tool for Reproducible Research in R.” In Implementing Reproducible Computational Research, edited by Victoria Stodden, Friedrich Leisch, and Roger D. Peng. Chapman; Hall/CRC. ———. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. https://yihui.org/knitr/. ———. 2016a. Bookdown: Authoring Books and Technical Documents with R Markdown. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/bookdown. ———. 2016b. bookdown: Authoring Books and Technical Documents with R Markdown. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/bookdown. ———. 2024a. Bookdown: Authoring Books and Technical Documents with r Markdown. https://github.com/rstudio/bookdown. ———. 2024b. bookdown: Authoring Books and Technical Documents with r Markdown. https://github.com/rstudio/bookdown. ———. 2024c. knitr: A General-Purpose Package for Dynamic Report Generation in r. https://yihui.org/knitr/. Xie, Yihui, J. J. Allaire, and Garrett Grolemund. 2018. R Markdown: The Definitive Guide. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/rmarkdown. Xie, Yihui, Christophe Dervieux, and Emily Riederer. 2020. R Markdown Cookbook. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/rmarkdown-cookbook. Zeileis, Achim, and Torsten Hothorn. 2002. “Diagnostic Checking in Regression Relationships.” R News 2 (3): 7–10. https://CRAN.R-project.org/doc/Rnews/. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
